{
  "summary": "",
  "qa_pairs": [
    {
      "question": "What are the five critical features of the CRAG benchmark, and how does it ensure realism in its questions by incorporating smart assistant use cases?",
      "answer": "The five critical features of CRAG are realism, richness, insightfulness, reliability, and longevity. Realism is ensured by generating questions that reflect real-world QA scenarios, such as using smart assistant use cases, paraphrasing questions for diversity, and manually verifying ground truths to align with practical user queries.",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ]
    },
    {
      "question": "How does CRAG incorporate diverse question types and domains, and what specific keys are used for finance and movie queries?",
      "answer": "CRAG includes 4,409 QA pairs across five domains (Finance, Sports, Music, Movie, Open domain) and seven complex question types (e.g., conditions, multi-hop, false-premise). For finance queries, keys include 'market_identifier', 'metric', and 'datetime'; for movie queries, keys include 'movie_name', 'movie_aspect', and 'year'.",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ]
    },
    {
      "question": "What evaluation tasks are designed in CRAG to test RAG solutions, and how do they differentiate between hallucinated and missing answers?",
      "answer": "CRAG includes three tasks: retrieval summarization, knowledge graph/web retrieval, and end-to-end retrieval-augmented generation. The score system penalizes hallucinated answers more than missing answers, as hallucinations are considered more harmful to user trust.",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ]
    },
    {
      "question": "What are the specific keys for music and sports queries, and how do they contribute to the benchmark's richness and insightfulness?",
      "answer": "For music queries, keys include 'artist_name', 'artist_aspect', 'song_name', and 'song_aspect'; for sports queries, keys include 'sport_type', 'tournament', 'team', and 'datetime'. These keys enable detailed analysis of performance across diverse data slices, enhancing insightfulness and reflecting real-world complexity.",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ]
    },
    {
      "question": "Which model showed the highest truthfulness score in Task 3 of CRAG, and what does this indicate about its performance in handling complex RAG tasks?",
      "answer": "GPT-4 Turbo achieved the highest truthfulness score of 43.6% in Task 3. This indicates its superior ability to avoid hallucinations and provide accurate, reliable answers in end-to-end retrieval-augmented generation scenarios compared to other models.",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ]
    },
    {
      "question": "What are the two stages of the KDD Cup 2024 Meta CRAG challenge, and how are they evaluated? Please synthesize information from the data splitting, evaluation methods, and prompt structures described in the context.",
      "answer": "The KDD Cup 2024 Meta CRAG challenge has two stages: Stage 1 involves developing RAG solutions via a leaderboard, while Stage 2 determines final winners. Stage 1 uses auto-evaluation, and top teams are selected for manual evaluation in Stage 2. Data is split into validation (30%), public test (30%), and private test (40%) sets. Evaluation includes human scoring (accuracy, incorrectness, etc.) and auto-evaluation metrics like F1 scores (Table 10).",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ]
    },
    {
      "question": "How is the accuracy of responses evaluated in the RAG solutions, and what criteria are used for human evaluation? Synthesize details from the evaluation rubric and scoring system.",
      "answer": "Accuracy is evaluated using a rubric where responses are scored as perfect (1), acceptable (0.5), missing (0), or incorrect (-1). Human evaluation assesses factual correctness, relevance, and structural integrity. Responses are graded based on whether they answer the query, avoid hallucinations, and follow formatting guidelines. Auto-evaluation metrics like F1 scores (e.g., ChatGPT: 98.8, Llama 3: 98.9) are also used.",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ]
    },
    {
      "question": "What are the key components of the prompts used for RAG solutions, and how do they differ across tasks? Synthesize information from the Vanilla LLM, web search, and KG-based prompts.",
      "answer": "Prompts for RAG solutions include: (1) Vanilla LLM: Requires answering based on query time and internal knowledge. (2) Web search (Task 1): Incorporates references from web results. (3) KG and web search (Tasks 2-3): Adds knowledge graph data. All prompts emphasize brevity, 'I don\u2019t know' responses for uncertainty, and avoid false premises. The KG prompt explicitly separates web and knowledge graph references.",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ]
    },
    {
      "question": "How is the query time handled in the RAG evaluation process, and what role does it play in generating answers? Synthesize details from the prompt structures and evaluation criteria.",
      "answer": "Query time (formatted as 'mm/dd/yyyy, hh:mm:ss PT') is provided in prompts to contextualize answers. It ensures responses are time-sensitive and aligned with the query's temporal scope. For example, the Vanilla LLM and RAG prompts require answers to consider the query time, while the KG/web search prompts include references retrieved at the same query time.",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ]
    },
    {
      "question": "What is the purpose of the Query entity extraction prompt, and what structured information does it extract? Synthesize details from the entity extraction instructions and domain classification.",
      "answer": "The Query entity extraction prompt identifies the domain (e.g., finance, sports, music) and extracts structured entities. For 'encyclopedia' or 'other' queries, it captures 'main_entity'. For other domains, it extracts domain-specific attributes. The output is a JSON object with keys like 'domain' and 'main_entity', ensuring standardized parsing of query intent and context.",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ]
    },
    {
      "question": "What components make up the benchmark dataset described in the source context, and what specific metrics are used to evaluate RAG systems, including the scoring system for labeling answers?",
      "answer": "The benchmark dataset includes 220K webpages, a KG of 2.6M entities, and 38 mock APIs. RAG systems are evaluated using metrics that label answers as perfect, acceptable, missing, or incorrect. The scoring system assigns 1, 0.5, 0, and -1 for perfect, acceptable, missing, and incorrect answers, respectively, with penalties for hallucinations and preference for missing answers over incorrect ones.",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ]
    },
    {
      "question": "How does the two-step automatic evaluation method for RAG systems work, and what role do LLM evaluators like ChatGPT and Llama 3 play in this process?",
      "answer": "The two-step method first checks if an answer matches the ground truth exactly (considered accurate). If not, LLMs determine if the response is accurate, incorrect, or missing. ChatGPT and Llama 3 are used as evaluators to compute average accurate, hallucination, missing rates, and truthfulness scores, avoiding self-preference by averaging results from both models.",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ]
    },
    {
      "question": "According to the source, what criteria define a 'perfect' answer, and how does the truthfulness metric for RAG systems account for hallucinations and missing answers?",
      "answer": "A 'perfect' answer correctly addresses the user\u2019s question without hallucinations. Truthfulness is calculated as the average score across all examples, penalizing hallucinations (by assigning -1 to incorrect answers) and prioritizing 'missing' answers (score 0) over incorrect ones (score -1) to minimize harmful errors.",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ]
    },
    {
      "question": "What are the three key contributions of the CRAG benchmark as described in the source context, and how do they address the limitations of traditional QA benchmarks?",
      "answer": "The three key contributions of CRAG are: 1) the design of an effective automatic evaluation mechanism for fast iterations (Section 4), 2) a comprehensive evaluation of both straightforward RAG solutions and industry state-of-the-art RAG solutions (Section 5), and 3) the inclusion of diverse and dynamic challenges such as real-time facts, tail entities, and complex questions. These address limitations of traditional benchmarks like Natural Questions (NQ) and MS MARCO, which focus only on static web or KG retrieved content and lack coverage of dynamic, diverse, and complex real-world scenarios.",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ]
    },
    {
      "question": "How does the CRAG benchmark differ from traditional QA benchmarks in terms of evaluation metrics and testing scenarios, and what specific features make it more suitable for evaluating RAG systems?",
      "answer": "CRAG differs by incorporating realistic testing with mock APIs, dynamic question handling, diverse fact popularity, and content beyond Wikipedia. Traditional benchmarks use matching-based metrics like ROUGE or F1, which are less effective for LLMs generating free-form responses. CRAG's features, including coverage of fast-changing facts, tail entities, and complex questions requiring set answers or post-processing, make it more suitable for evaluating RAG systems' ability to handle real-world retrieval and synthesis challenges.",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ]
    },
    {
      "question": "What insights does the evaluation of state-of-the-art (SOTA) RAG solutions on CRAG reveal about their performance compared to straightforward RAG solutions, and what are the limitations still faced by SOTA systems?",
      "answer": "The evaluation shows SOTA RAG solutions achieve higher truthfulness (up to 51%) compared to straightforward solutions (44%), but they still have hallucination rates between 16-25%. Limitations include difficulties with real-time/fast-changing queries, torso/tail entities, and complex tasks like aggregation, multi-hop reasoning, or post-processing. Additionally, SOTA systems exhibit varying latency trade-offs (e.g., Copilot Pro has high latency but high truthfulness, while Meta SG has low latency but mid-tier truthfulness).",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ]
    },
    {
      "question": "How does the CRAG benchmark address the challenge of evaluating RAG systems across different dimensions of question complexity, and what role do traffic weights play in this evaluation?",
      "answer": "CRAG addresses question complexity by categorizing questions into types like simple, conditional, set, and dynamic, and by testing scenarios such as real-time facts, tail entities, and complex reasoning. Traffic weights are applied to reflect the frequency of each question type in real QA traffic, allowing evaluation of SOTA systems' performance in real-world use cases. This ensures assessments consider domain-agnostic distributions and practical relevance.",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ]
    },
    {
      "question": "What specific improvements in RAG systems are highlighted by the CRAG benchmark, and how do these improvements relate to challenges like hallucination rates and dynamic factual accuracy?",
      "answer": "The CRAG benchmark highlights improvements in handling set answers, false premises, and dynamic facts. For example, SOTA RAG systems show significant progress in providing accurate set answers and detecting false premises compared to straightforward solutions. However, challenges remain in reducing hallucination rates (16-25%) and improving accuracy for fast-changing facts, torso/tail entities, and complex reasoning tasks, which require better retrieval noise management and reasoning capabilities.",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ]
    },
    {
      "question": "What are the key limitations of existing RAG datasets according to the CRAG benchmark, and how does CRAG address these limitations through its design elements?",
      "answer": "Existing RAG datasets do not adequately represent the diverse and dynamic nature of real-world QA tasks. CRAG addresses this by introducing 4,409 question-answer pairs and mock APIs to simulate web and KG search, encapsulating questions across five domains and eight categories, with variations in entity popularity and temporal dynamism.",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ]
    },
    {
      "question": "How does the performance of LLMs and RAG systems compare on the CRAG benchmark, and what challenges does this highlight for RAG solutions?",
      "answer": "LLMs achieve 34% accuracy on CRAG, while straightforward RAG improves it to 44%. However, state-of-the-art RAG solutions only answer 63% of questions without hallucination. This highlights challenges like managing retrieval noises, selecting relevant info, and handling dynamic/fact-rich queries.",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ]
    },
    {
      "question": "What factors contribute to lower RAG truthfulness in specific domains and question types, and how does the CRAG benchmark reveal these issues?",
      "answer": "Lower RAG truthfulness is observed in Finance and Sports domains, and for complex questions. CRAG reveals this through evaluations showing reduced accuracy for dynamic facts, low-popularity entities, and multi-hop queries, emphasizing challenges in leveraging KG data and improving search ranking.",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ]
    },
    {
      "question": "How does the CRAG benchmark evaluate the impact of search ranking and knowledge graph (KG) data on QA accuracy and hallucination rates?",
      "answer": "CRAG shows that KG knowledge improves accuracy with lower hallucination rates due to its precision, but improvements are limited. Task 3's higher truthfulness (vs. Task 2) is attributed to better search ranking and recall, while 30% of ground truths remain inaccessible due to context window limitations.",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ]
    },
    {
      "question": "What is the significance of the CRAG benchmark in advancing RAG research, and what resources are provided to support its use?",
      "answer": "CRAG serves as a foundation for the KDD Cup 2024 challenge and provides 4,409 QA pairs, mock APIs, and evaluations to address RAG limitations. It is maintained for research communities and available at https://github.com/facebookresearch/CRAG/ to advance both RAG and general QA solutions.",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ]
    },
    {
      "question": "Which paper introduced the Symphony framework for natural language query answering over multi-modal data lakes, and how does the accuracy criteria outlined in the second passage relate to evaluating its effectiveness?",
      "answer": "The paper 'Symphony: Towards natural language query answering over multi-modal data lakes' by Chen et al. (2023) introduced the framework. The accuracy criteria in the second passage (Accuracy = 3 requiring factual correctness and completeness) would assess whether Symphony's answers meet these standards, ensuring no hallucinations and full relevance to user queries.",
      "chunk": "Passage 1:\n[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.\n\nPassage 2:\n[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
      "chunk_list": [
        "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
        "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge"
      ]
    },
    {
      "question": "How does the auto-evaluation process described in the second passage differentiate between 'accurate' and 'acceptable' answers, and which paper from the first passage might benefit most from this distinction?",
      "answer": "The auto-evaluation process merges 'perfect' and 'acceptable' into 'accurate' while penalizing incorrect answers. 'Acceptable' answers (Accuracy = 2) are accurate but incomplete or contain minor hallucinations. The 'Retrieval-augmented generation for large language models: A survey' by Gao et al. (2024) would benefit from this distinction, as RAG systems often balance completeness and precision.",
      "chunk": "Passage 1:\n[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.\n\nPassage 2:\n[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
      "chunk_list": [
        "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
        "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge"
      ]
    },
    {
      "question": "How does the second passage define 'minor hallucination,' and which paper from the first passage might require careful handling of such issues during evaluation?",
      "answer": "A 'minor hallucination' is defined as an answer addressing the user's question but containing off-details that don't alter the overall meaning. The 'Survey on hallucination in large language models' by Huang et al. (2023) would require careful handling of such issues, as it focuses on identifying and mitigating hallucinations in LLM outputs.",
      "chunk": "Passage 1:\n[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.\n\nPassage 2:\n[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
      "chunk_list": [
        "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
        "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge"
      ]
    },
    {
      "question": "What is the primary purpose of the CRAG benchmark as described in the source context, and what future improvements are planned for it? Provide specific details from the text.",
      "answer": "The primary purpose of CRAG is to advance research in retrieval-augmented generation (RAG) by identifying gaps in existing solutions and providing insights for improvement (S231-S232). Future improvements include expanding the benchmark for multi-lingual questions, multi-modal questions, and multi-turn conversations to adapt to emerging challenges and new research needs (S233).",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ]
    },
    {
      "question": "How was the data split for the CRAG benchmark, and what roles do the validation, public test, and private test sets play in the evaluation process?",
      "answer": "The data was split into validation (30%), public test (30%), and private (40%) sets (S176). The validation and public test sets were used by participants to develop and test models, while the private test set was used for evaluating submitted solutions (S177). Future offline users can use the validation set for development and the public test set for testing and result reporting (S178).",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ]
    },
    {
      "question": "What experimental setup was used to evaluate LLM-only and RAG solutions on CRAG, and how did the context window sizes vary across different models?",
      "answer": "LLM-only solutions were tested with simple prompts, while RAG solutions used web context windows (1K-4K tokens) and KG context windows (0.5K-2K tokens) depending on the model (S181-S183). For example, Falcon and FLAN-T5 used 1K tokens for web contexts, while Llama 3 Instruct and GPT-4 Turbo used 4K tokens. KG contexts varied from 0.5K to 2K tokens based on the model.",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ]
    },
    {
      "question": "According to the performance metrics in Table 5, what are the highest accuracy rates achieved by LLM-only solutions and straightforward RAG solutions, and how do these compare across tasks?",
      "answer": "LLM-only solutions achieved up to 34% accuracy (S187), while straightforward RAG solutions reached up to 44% accuracy (S187). For Task 3, Llama 3 70B Instruct and GPT-4 Turbo achieved 41.3% and 43.6% accuracy, respectively (Table 5).",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ]
    },
    {
      "question": "What factors are analyzed in Figure 3 regarding the auto-eval truthfulness of LLM-only and Task 3 solutions, and what is the significance of the 'truthfulnessa' subscript in the table?",
      "answer": "Figure 3 analyzes auto-eval truthfulness across domain, dynamism, popularity, and question type (S189). The 'truthfulnessa' subscript indicates that the results are reported by auto-eval, as noted in S188.",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ]
    },
    {
      "question": "What design decision was made regarding the evaluation of first-stage retrieval candidate pool construction in the CRAG benchmark, and how does this relate to the dataset's construction methodology?",
      "answer": "The CRAG benchmark's three tasks do not directly evaluate the construction of a first-stage retrieval candidate pool, as noted in [S950]. This decision ensures the competition remains challenging yet achievable within the KDD Cup's timeframe, while the dataset allows users to build a retriever using the union of all 220K web pages, as mentioned in [S952].",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ]
    },
    {
      "question": "How does the latency measurement for Perplexity.ai differ between API calls and web interface interactions, and what steps were taken to ensure fair comparisons across systems?",
      "answer": "Perplexity.ai's API call latency was measured at 2,455ms [S946], but web interface latency was reported separately in Table 6 due to incompatibility between API and web interface measurements [S947]. For Meta SG, on-device components like ASR and TTS were excluded to estimate latency comparable to web interface interactions [S949].",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ]
    },
    {
      "question": "What is the significance of including both web search and KG search contents in the CRAG retrieval tasks, and how does this impact the evaluation of RAG systems?",
      "answer": "Including web search and KG search contents simulates practical RAG scenarios [S130]. Web search recall was 85% using 50 pages, with full web pages increasing recall by 20% compared to snippets [S136, S142]. KG questions showed lower recall (74%) than web questions (93%), highlighting the importance of KGs for tasks involving torso and tail entities [S144].",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ]
    },
    {
      "question": "How were the question-answer pairs in CRAG constructed, and what distribution metrics are provided for entities in KG-based questions?",
      "answer": "QA pairs were constructed from KGs using entity templates and web contents via user-generated questions [S119]. For KG questions, 661, 658, and 665 pairs contained head, torso, and tail entities respectively [S124]. The dynamism distribution reflects domain-specific real-time needs, such as higher real-time questions in Finance [S127].",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ]
    },
    {
      "question": "What trade-offs were considered in the CRAG benchmark's design to balance challenge and feasibility, and how does this affect dataset usage?",
      "answer": "The benchmark excluded first-stage retrieval candidate pool evaluation to maintain feasibility within a three-month timeframe [S951]. However, users can build retrievers using the 220K web page corpus, enabling fair comparisons despite not covering the entire web [S952].",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ]
    },
    {
      "question": "What is the primary purpose of Task 3: End-to-end RAG, and how does it differ from Task 1 and Task 2 in terms of retrieval challenges?",
      "answer": "Task 3 tests how a RAG system ranks a larger number of retrieval results by providing 50 web pages instead of 5, increasing the likelihood of containing necessary information but also noise. Unlike Task 1 (which focuses on answer generation) and Task 2 (which tests structured data querying and synthesis), Task 3 emphasizes ranking in a larger, noisier retrieval set.",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ]
    },
    {
      "question": "According to the dynamism categories, what distinguishes 'Real-time' questions from 'Fast-changing' questions, and which domains have the highest concentration of Real-time questions?",
      "answer": "Real-time questions have answers changing over seconds (e.g., stock prices), while Fast-changing questions change no more than daily (e.g., game schedules). The Finance and Sports domains have the most Real-time questions.",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ]
    },
    {
      "question": "How were the QA pairs validated in Phase 1 and Phase 2, and what was the outcome of the answer validation process for KG questions?",
      "answer": "Phase 1 validated questions and meta-labels with 90% agreement via human review, resolving conflicts with a third auditor. Phase 2 involved extensive answer verification, resulting in a 5% answer correction rate for KG questions after manual API queries by engineers.",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ]
    },
    {
      "question": "What is the role of mock APIs in Task 2: KG and Web Retrieval Augmentation, and what types of information do they provide?",
      "answer": "Mock APIs in Task 2 access structured data from underlying mock KGs, providing input parameters parsed from questions to retrieve structured information. Answers may or may not exist in the mock KGs, testing RAG systems' ability to query structured sources and synthesize data.",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ]
    },
    {
      "question": "How were multi-hop questions constructed, and what is an example of a multi-hop question template derived from the KG traversal process?",
      "answer": "Multi-hop questions were created by sampling an entity and selecting two relation triplets in a two-hop path (e.g., company1 \u2192 is_parent \u2192 company2, then company1 \u2192 ceo \u2192 person). An example template is 'Who is the CEO of the parent company of [company2]?'.",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ]
    },
    {
      "question": "What is the signal-to-noise ratio of the mock knowledge graphs (KGs) used in CRAG, and how does this ratio affect the testing of RAG systems?",
      "answer": "The mock KGs have a signal-to-noise ratio of less than 1/30, containing necessary information and noises with similar entity or attribute names to simulate real settings.",
      "chunk": "Passage 1:\n[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID.",
      "chunk_list": [
        "[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID."
      ]
    }
  ],
  "all_chunks": [
    "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
    "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
    "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
    "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
    "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
    "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
    "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
    "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
    "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
    "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
    "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
    "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
    "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
    "[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID.",
    "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
    "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
    "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
    "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
    "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20"
  ]
}