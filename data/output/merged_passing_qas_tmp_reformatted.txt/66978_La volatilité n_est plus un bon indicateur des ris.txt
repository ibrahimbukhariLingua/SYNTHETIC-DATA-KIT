La volatilité n’est plus un bon indicateur des risques à venir


La peur est-elle un bon guide ? C’est la question de Ginjer AM, dont la gestion flexible repose sur la conviction que l’analyse fondamentale ne permet plus de comprendre les risques à venir sur les marchés sans une compréhension fine des différentes interventions : instruments, volumes, couvertures, etc. « La volatilité, qu’elle soit historique ou implicite (reflétant le prix des options sur actions) n’est plus le bon indicateur pour juger les risques , explique Mathieu Vaissié, directeur de la recherche chez Ginjer AM. Sur des marchés qui ne sont plus stationnaires, où l’information se diffuse instantanément et où les investisseurs peuvent s’exposer ou se couvrir en un clic, gérer son budget de risque uniquement en fonction de la volatilité revient à conduire sur une route de montagne les yeux rivés sur le rétroviseur… » L’analyse des marchés confirme que si l’indice de la peur, le VStoxx pour les actions européennes, a clôturé à un niveau supérieur ou égal à 2 écarts types au-dessus de sa moyenne historique à 425 reprises depuis janvier 1999, les cours ne se sont pas du tout comportés de manière homogène, plutôt de manière très dispersée après ces événements ( voir les graphiques ). Alors qu’une forte hausse de la volatilité implicite est censée annoncer une correction significative sur les sous-jacents, le marché ne présente pas, en moyenne, de tendance baissière forte. Le fait que la ligne médiane diminue davantage montre même que, quand les cours montent au lieu de baisser, c’est de manière plus sensible, car souvent sur de plus faibles volumes. Désintoxiquer le marché Surtout, le comportement du marché semble avoir changé depuis 2008, avec plus d’épisodes de peur mais moins d’impacts sur sa dynamique, d’où une moyenne quasiment neutre : « Parier sur une hausse ou sur une baisse des marchés sans connaître les ressorts d’un pic de volatilité revient désormais à jouer à pile ou face , résume Mathieu Vaissié . La peur n’est pas nécessairement le reflet d’un risque, elle peut aussi être une source d’opportunités, sous réserve d’analyser la nature du risque (la qualité de la volatilité plutôt que sa quantité), d’être patient, de s’affranchir du diktat de la volatilité à court terme. » Certains investisseurs réagissent très vite à chaque sursaut de volatilité, mais moins vite que des marchés désormais automatisés, « ce qui peut les amener à capter 80 % de la baisse, puis à l’inverse, du fait de ce même contretemps quand les cours remontent – bien souvent pour la même raison qui les a fait baisser –, seulement 20 % de la hausse » . Pour Yves Choueifaty aussi, « la volatilité ne peut pas être un bon indicateur de risque ex ante : elle mesure une agitation, un écart ex post, pas les dégâts à venir ; les indices de volatilité étaient ainsi faibles juste avant 2007 (moins de 10 %). Il est toujours mieux de s’intéresser davantage au potentiel de pertes d’un portefeuille » , note le président de Tobam. Simon Aninat, gérant volatilité chez Seeyond, confirme qu’ « il existe deux types de pics : quand le marché bouge pour des raisons conjoncturelles (8 fois sur 10) et oblige un certain nombre d’investisseurs (‘risk parity’, ‘stop loss’, etc.) à prendre leurs pertes, amplifiant artificiellement son mouvement, cela constitue alors un bon point d’entrée ; et les autres cas, plus rares, où il bouge pour des raisons structurelles comme en octobre 2008 » . Changement structurel Selon son modèle d’analyse de l’évolution de la structure de marché, dont la volatilité est une des variables, Ginger AM observe « une certaine incohérence entre le tumulte en surface (la volatilité) et le calme plus en profondeur sur la période récente, notamment en Europe : malgré de nombreuses corrections depuis fin 2012, la structure sous-jacente du marché est restée relativement stable. » Cette stabilité pourrait entraîner une répétition des comportements d’investisseurs et leur surexposition aux mêmes types de titres, « avec une concentration et des corrélations (visibles ou non) dangereuses » , poursuit Yves Choueifaty en prenant l’exemple des Gafa* aujourd’hui ou des financières en 2007. Simon Aninat nuance : « Si les grands indices ont une volatilité globale stable grâce à des bons fondamentaux macroéconomiques, les grandes rotations sectorielles induites par les changements en cours de politiques monétaires alimentent des pics de volatilité et des décorrélations sur les secteurs sous-jacents. » Les analyses se rejoignent sur un vrai changement structurel : « Avec le développement des produits synthétiques (dérivés, structurés, etc.), les marchés ne sont plus dirigés par l’offre et une quantité limitée de titres émis par une entreprise ou un Etat, mais par la demande et la volonté d’un investisseur de s’exposer dans des proportions potentiellement illimitées à un risque. Cela conduit à une plus grande complexité, de nature à en modifier profondément la dynamique avec l’apparition de nouvelles forces qui ne sont ni fondamentales ni exogènes au marché » , note Mathieu Vaissié. « Les banques doivent vendre ces produits quand tout le monde veut en acheter pour se couvrir, ce qui anesthésie techniquement le marché, par exemple de la volatilité, et d’autant plus dans un environnement de taux bas » , conclut Simon Aninat. *Géants du numérique tels Google, Apple, Facebook et Amazon.