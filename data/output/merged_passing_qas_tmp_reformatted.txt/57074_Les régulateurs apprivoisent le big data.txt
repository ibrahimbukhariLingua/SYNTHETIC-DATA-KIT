Les régulateurs apprivoisent le big data


Les sirènes du big data n’ont pas fait perdre la tête aux régulateurs financiers. Invité vendredi par la Banque de France, Benoît Coeuré , membre du directoire de la Banque centrale européenne (BCE), a reconnu que «les jeux de données traditionnels et nos modèles se sont parfois avérés inadaptés pour soutenir le processus de décision, reflétant de longs retards, des hypothèses linéaires et l’absence d’information plus granulaire» . Les banques centrales commencent à se frotter au big data, en amassant des montagnes de données plus riches qu’auparavant sur la transmission de leur politique monétaire, afin de mieux calibrer cette dernière. Depuis juillet 2016, la BCE collecte des données confidentielles quotidiennes sur les transactions individuelles en euros du marché monétaire. Ce programme baptisé MMSR (money market statistical reporting) recueille chaque jour des informations sur 40.000 transactions pour des volumes de 600 milliards d’euros. Les données aident déjà l’institution à évaluer l’impact de ses programmes de rachat d’actifs. Autre exemple, l’initiative AnaCredit , qui récolte des informations granulaires prêt par prêt sur le crédit bancaire. Outre son degré de précision, le big data apporte de nouveaux types d’informations (comme les comportements sur les réseaux sociaux), accessibles grâce aux évolutions technologiques (apprentissage automatisé, intelligence artificielle). Le système européen des banques centrales (SEBC) a ainsi montré que les requêtes Google liées à l’absence d’emploi pouvaient réduire les erreurs de prédiction du taux de chômage de 80% par rapport à des modèles classiques. La BCE utilise des techniques d’extraction automatique de texte pour déterminer si sa communication a été interprétée comme «dovish» (accommodante), ou inversement, «hawkish» . Embauche de data scientists Toutefois, cette volonté de transparence fait que «nous pourrions un jour être tentés d’esquisser nos déclarations de politique monétaire et nos discours à la lumière de la manière dont ils seront compris et interprétés par des algorithmes d’intelligence artificielle» , s’est alarmé Benoît Coeuré. A terme, «la dépendance aux données de recherche sur internet dans l’évaluation de la situation économique pourrait devenir auto-réalisatrice» . D’autres pièges existent. D’abord, «la qualité des données doit être maintenue» , a insisté vendredi le gouverneur de la Banque de France, François Villeroy de Galhau . «Cela requiert des ressources humaines nouvelles et significatives », soit l’embauche de data scientists, que s’arrachent déjà les entreprises privées. Ensuite, malgré sa masse de données, le big data ne supprime pas les biais statistiques, notamment d’échantillonnage. Analyser tous les tweets du monde serait omettre les comportements des personnes qui n’utilisent pas Twitter. Un autre risque est « que les individus utilisent d’autres sources de données disponibles immédiatement qui mineraient la confiance dans les statistiques officielles», a mis en garde Benoît Coeuré. Enfin, le plus grand danger du big data serait de «changer l’orientation de la politique, qui s’appuie sur des concepts et des données fournies par les gouvernements, comme le PIB» , a alerté le responsable. Car «les perceptions du public de ce que nous devrions viser peuvent différer des définitions abstraites que nous employons» . Or, à l’aune du big data, les liens renforcés entre les banques centrales et le public (y compris les marchés financiers) «pourraient à terme faire changer les définitions, et la conduite de la politique» . Les régulateurs tiennent à garder leur cap.