La chasse aux biais est ouverte


L’intelligence artificielle (IA) consacre la puissance de la science mais elle garde de l’homme qui l’a créée une part de subjectivité. Dès 2019, un groupe d’experts de haut niveau mandaté par l’Union européenne (UE) a édicté sept principes fondamentaux pour construire une intelligence artificielle « digne de confiance ». Ce sont la maîtrise par l’humain, la robustesse technique, la protection des données, la transparence, la non-discrimination et l’équité, le bien-être social et environnemental, et la responsabilité. Tout un programme pour éviter les dérives potentielles dans l’usage de ces outils très puissants. Car les algorithmes auto-apprenants à la base de l’intelligence artificielle peuvent produire des résultats biaisés. « Les biais apparaissent dans la construction d’un modèle parce qu’il est mal adapté à une population donnée ou qu’il ne prend pas en compte une catégorie de population, ou encore parce qu’il reproduit une dimension discriminatoire présente dans la société, détaille Hervé Phaure, associé en charge des activités risque de crédit chez Deloitte. Pour détecter ces biais, la démarche de Deloitte consiste à expliquer les écarts de résultats entre les modèles de ‘machine learning’ et les modèles de régression classique par rapport à une même population. Mais il faut être clair sur ce que l’on cherche à éviter et parvenir à déterminer quelles sont les variables à l’origine d’un biais. » Le sexe, l’origine ethnique, le lieu de résidence ou autres peuvent être à l’origine de biais. Pourtant, ces données sont la matière première des algorithmes. On ne peut donc pas les retirer avant le traitement, sous peine d’introduire d’autres biais. Mais comment redresser un modèle de machine learning puisqu’on ne peut pas lui imposer de règles en amont ? « Plutôt que de modifier artificiellement la composition des variables, mieux vaut analyser comment le modèle est utilisé dans le processus métier (‘scoring’, tarification…) et parfois modifier le processus, pas le modèle », complète Hervé Phaure. Confiance Les régulateurs ont commencé à dessiner un cadre pour l’intelligence artificielle, en attendant que l’UE définisse une réglementation générale. La Commission nationale de l’informatique et des libertés (Cnil), notamment, préconise « d’analyser les résultats pour pouvoir identifier les biais et de ne pas être passif face à un système de décision automatisé, résume Clémence Scottez, cheffe des affaires économiques. Nous insistons sur les principes de gouvernance et de maîtrise des modèles, ainsi que sur l’auditabilité des décisions. » La Cnil a régulièrement rappelé que chaque entreprise doit être capable de justifier ses décisions. Côté Autorité de contrôle prudentiel et de résolution (ACPR), le sujet fait l’objet de recherches scientifiques et Olivier Fliche, directeur du pôle fintech innovation, encourage « les établissements à sensibiliser leurs collaborateurs aux règles générales du droit et à leurs propres exigences éthiques, tout en appliquant des méthodes de test à l’issue des processus de traitement par l’IA pour vérifier qu’il n’existe pas de biais. Il est important aussi de suivre les interactions entre les algorithmes et les collaborateurs en contact avec les clients ». Le principe d’« explicabilité » apparaît comme la clé de la confiance dans l’IA. Il existe diverses solutions permettant de comprendre comment fonctionne un modèle de machine learning . IBM en propose toute une gamme car « l’explicabilité et la traçabilité sont absolument nécessaires et doivent s’appuyer sur un cadre éthique global », selon Jean-Philippe Desbiolles, vice-président monde pour la data et l’IA chez IBM. L’ACPR organisera en juin, avec des banques, un concours d’explicabilité sur des algorithmes d’octroi de crédit. Une façon ludique et pédagogique d’avancer ensemble vers la maîtrise de la technologie et de bonnes pratiques. Une labellisation des algorithmes n’est pas encore à l’ordre du jour mais elle pourrait être bénéfique.