Les chercheurs explorent les mondes parallèles de l'économie


Les prévisionnistes économiques sont à la peine. La pandémie du Covid-19 a détricoté de nombreuses corrélations entre des données, et les dynamiques d’inflation semblent insaisissables. La dislocation des chaînes d’approvisionnement et les conséquences de l’invasion de l’Ukraine par la Russie ont profondément changé les mécanismes économiques et sociaux, au point que les pronostics de croissance et d’inflation pour 2024 entre les plus grandes banques et sociétés de gestion ont très fortement divergé en fin d’année dernière. Dans ce contexte de grande interrogation sur la nature des données macroéconomiques, les partisans de l’école de la complexité économique jubilent. L’étude des systèmes complexes, qui a déjà montré son intérêt explicatif lors de la crise de 2008 , s’est une nouvelle fois épanouie, à la faveur d’une bifurcation de l’économie et des marchés financiers depuis 2020. Cette approche hétérodoxe, qui a émergé dans les années 1990, applique aux sciences sociales des méthodes utilisées par les sciences dures pour analyser des systèmes complexes, tels que le climat ou l’univers. En économie et en finance, les chercheurs veulent notamment comprendre le rôle des interactions sociales dans les cycles économiques, la construction des principaux agrégats, et les variations des prix. Avec pour armes principales des ordinateurs puissants et la capacité d’écrire des algorithmes pertinents, pour créer des mini-mondes économiques virtuels. Se concentrer sur les relations sociales Ces simulations informatiques explorent des mises en situation économiques dynamiques, sans a priori théorique. Le scénario comprend des règles économiques de base, et des agents économiques (des personnes physiques et morales) programmés par les chercheurs. Le codage ne réplique pas l’ensemble des humains ou des entreprises dans leur entière variété, mais intègre les principales contraintes économiques et traits psychologiques des agents. Par exemple, on peut fabriquer des ménages dotés de budgets, revenus, et objectifs différents, avec, pour chacun, des mécanismes de prise de décision plus ou moins optimaux. Les biais, tirés de l’économie comportementale, relèvent généralement de la routine, de la prise de décision rapide sous-optimale mais acceptable, ou encore du mimétisme. Puis les chercheurs répètent des milliers de fois leur protocole algorithmique, comme une simulation de Monte-Carlo, pour voir les scénarios macroéconomiques sur lesquels peuvent déboucher les conditions initiales. En procédant de la sorte, ils tentent d’observer ce qui peut se passer quand les consommateurs, les travailleurs, les investisseurs, ou les entreprises, tâtonnent. L’économie évolue ainsi en fonction des choix variés de ses agents et des mécanismes de relations entre eux. Ce sont donc les interactions entre des agents hétérogènes qui vont produire les agrégats économiques, comme dans la vie réelle. Et ce sont ces interactions qui vont être étudiées par les chercheurs. De nombreuses applications Les applications sont nombreuses. Les études sur les inefficiences des marchés financiers sont notamment utilisées dans la régulation . Mais le champ d’exploration est très vaste. Les équipes de recherche s’intéressent au marché de l’emploi, à l’impact du dérèglement climatique, aux effets des changements technologiques sur la croissance, à l’analyse des chaînes de valeurs, ou encore aux mécanismes d’inégalités économiques. Avec des applications tout aussi immédiates. Une équipe de chercheurs de l’Institute for New Economic Thinking (Inet) d’Oxford a, par exemple, anticipé avec précision dès avril 2020 le déclin du PIB britannique lors du deuxième trimestre 2020, en plein Covid-19. « Nous en sommes fiers, car nous avons prédit que le PIB du Royaume-Uni au deuxième trimestre 2020 se contracterait de 21,5%, contre 22,1% dans la réalité, et anticipé ce qui se passerait aussi pour la plupart des secteurs. Nous avons fait plusieurs recommandations au gouvernement britannique, et suggéré que la meilleure politique était de maintenir ouvertes toutes les industries situées en amont, celles produisant des biens de base, comme les vêtements, et de fermer les secteurs comportant de l’interaction avec les clients. Nous avons montré que cela aurait un impact économique moindre que les autres possibilités envisagées », raconte Doyne Farmer, l’un des vétérans de cette approche, qui dirige aujourd’hui le programme de complexité économique de l’Inet. Explorer les réalités alternatives Mais la plus grande force des modèles d’agents, c’est peut-être de pouvoir écrire des réalités alternatives. « Les données réelles sont très peu nombreuses. Sur 40 ans, on n’a qu’environ 10.000 données journalières du S&P 500. C’est beaucoup et peu à la fois », souligne Jean-Philippe Bouchaud. Ce pionnier de la branche éconophysique, qui applique des techniques de sciences physiques à l’économie et la finance, préside aussi et dirige la recherche du hedge fund Capital Fund Management. Pour explorer des réalités parallèles, les chercheurs utilisent des données synthétiques, c’est-à-dire fabriquées par leurs soins, qui respectent toutefois des motifs qu’ils ont identifiés avec les séries de données réelles. « Les modèles d’agents permettent d’essayer de trouver comment un processus sous-jacent conduit à ces statistiques. Si on le trouve, on peut alors engendrer des séries virtuelles en nombre considérable, et imaginer des scénarios que l’on n’a pas vus dans le passé », complète-t-il. Ces données artificielles doivent cependant être utilisées avec parcimonie, au risque de faire dérailler le programme. Des banques centrales testent l’approche Les modèles d’agents s’opposent principalement aux modèles macroéconomiques traditionnels, construits sur la base de la loi de l’équilibre général. Ces derniers restent la référence institutionnelle, bien qu’ils peinent à décortiquer la dernière vague d’inflation. Selon certaines études, c’est un point irrationnel qui semble l’expliquer : la volonté de certaines entreprises de gonfler les prix en période d’inflation déjà forte, en dépit du jeu de l’offre et de la demande. Les blocages théoriques de la communauté des économistes interpellent, jusqu'à voir Christine Lagarde, l’actuelle présidente de la Banque centrale européenne (BCE), s’en émouvoir lors du sommet de Davos 2024. Les économistes « sont les scientifiques les plus tribaux que l’on puisse connaître. Ils se citent les uns les autres, les hommes plus que les femmes, soit dit en passant. Ils ne vont pas au-delà de ce monde, car ils s’y sentent bien, et peut-être que les modèles y sont pour quelque chose », a-t-elle déclaré. Les grandes banques centrales ont pourtant déjà commencé à tester le potentiel des simulations numériques. A la suite de la crise financière de 2008, Jean-Claude Trichet, alors président de la BCE, plaidait déjà pour l’ajout des modèles d’agents dans la trousse des économistes de l’institution. Plusieurs programmes ont depuis été développés à la BCE, à la Banque d’Angleterre, ou encore à la Federal Reserve. Mais pas encore pour prédire l’inflation, et aider les banquiers centraux dans leur principale tâche, semble-t-il. Dans le flou face au futur de l'économie, les banques centrales préfèrent à nouveau se fier aux données publiées, plutôt que d’utiliser la forward guidance , dans la conduite de leur politique monétaire. Un retournement qui interroge les spécialistes sur leur propre approche. « Maintenant que les banques centrales se concentrent sur leur ‘data dependance’ pour évaluer le moment opportun pour commencer à baisser les taux, cela vaut la peine de réfléchir sérieusement à la valeur que nous pouvons tirer des données économiques sur une base mensuelle et à la meilleure façon d'évaluer ce qui se passe », écrivait en janvier James Pomeroy, l’économiste de HSBC, sans toutefois en arriver à l’économie de la complexité. Vers un double numérique de la totalité de l’économie L’usage des simulations numériques pourrait donc être l’un des moyens d’enrichir l’analyse des économistes. Les travaux restent bornés à de petits mondes. Toutefois, les récents succès prédictifs, couplés au développement de nouvelles méthodes d’analyse d’immenses bases de données, comme les réseaux de neurones artificiels, et à l’élévation de la puissance informatique, font rêver certains bien plus grand. Parmi eux, Doyne Farmer, à l’Inet, s’est ainsi lancé depuis trois ans dans la fabrication d’un double numérique de la totalité de l’économie réelle. Pour cela, il essaye de modéliser exactement son fonctionnement, qu’il superposera ensuite sur les entités qui existent dans le monde. « Je veux créer un modèle de l’économie qui contienne le même genre de détails que Google Maps. On pourra voir chaque entreprise. Les entreprises pourront voir toutes leurs concurrentes, ce qui se passe sur leur marché, ou encore les effets des politiques publiques. Les utilisateurs pourront voir ce qui se passe réellement dans un cycle économique, et qui sont les gagnants et les perdants. C’est un objectif », explique-t-il. Macrocosme, sa société, emploie pour le moment ses meilleurs étudiants. Il espère un jour réunir plusieurs milliers de collaborateurs, et se donne une dizaine d’années pour y parvenir. Ce n’est pas le seul projet en cours de développement. Sina Shahandeh, un entrepreneur spécialiste de la physique numérique, s’est récemment associé avec le chercheur Sebastian Poledna, de l’International Institute for Applied Systems Analysis, pour créer une réplique moins lourde et peut-être plus pratique. Leur approche consiste à calibrer un modèle d’agents sur l’ensemble des données publiques et privées accessibles, dont les microdonnées, comme la consommation précise des ménages. Le double numérique serait actualisé toutes les 24 heures, avec une collecte et un traitement des données au fil de l’eau. « Ce type d’outil ne sera pas une boule de cristal. Ce serait quelque chose qui pourrait améliorer les chances, comme deviner le PIB, l’inflation, que personne n’est capable de prédire correctement. Cela peut aussi aider tout le monde à décider de sa stratégie de couverture ou d’investissement », explique-t-il. Les deux «startuppers» recrutent actuellement chez les spécialistes de l’économie de la complexité. Ils cherchent à lever un million de dollars de financement pour fabriquer une première version utilisant les données publiques. Si elle fonctionne bien, ils espèrent passer à l’étape suivante en un an, en incluant des données alternatives, comme celles provenant des cartes de crédit. Et commercialiser leur outil d’ici à quelques années. Les doutes ne manquent pas Tout le monde ne perçoit pas la création d’un double numérique de l’économie réelle comme étant du domaine du possible, voire du souhaitable. « La complexité des phénomènes qui peuvent arriver à l’échelle macroscopique est telle qu’il faut déjà bien comprendre quels sont ces mécanismes d’agrégation et quels sont ces phénomènes émergents avant de se lancer dans une calibration de modèles réalistes. Je crains que l’on risque de faire des erreurs similaires, en amplitude, à celles de l’économie classique », prévient Jean-Philippe Bouchaud. Le chercheur, enseignant à l’Ecole normale supérieure, considère que les modèles d’agents doivent d’abord être des «aides à l’imagination» pour les économistes, et non contribuer à les enfermer dans le mythe de la précision des prévisions. A l’instar des métiers menacés par ChatGPT, les économistes devront en tout cas repenser leur rôle dans la cité.