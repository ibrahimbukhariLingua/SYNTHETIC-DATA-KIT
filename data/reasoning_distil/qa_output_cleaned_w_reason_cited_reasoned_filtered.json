{
  "summary": "",
  "qa_pairs": [
    {
      "question": "What are the five critical features of the CRAG benchmark, and how does it ensure realism in its questions by incorporating smart assistant use cases?",
      "answer": "<statement>The five critical features of the CRAG benchmark are Realism, Richness, Insightfulness, Reliability, and Longevity.<cite>[S29][S35][S37][S39][S41]</cite></statement> <statement>Realism ensures that the benchmark reflects real-world use cases, such as questions similar to those asked in practical scenarios, and incorporates smart assistant use cases to enhance realism.<cite>[S31][S48]</cite></statement> <statement>Richness involves diverse instance types, covering both common and complex challenges to represent real-world scenarios.<cite>[S35]</cite></statement> <statement>Insightfulness allows for analyzing performance across different data slices, highlighting a solution's ability to address varied challenges.<cite>[S37]</cite></statement> <statement>Reliability ensures accurate ground truths, robust metrics, and statistically significant evaluations for trustworthy assessments.<cite>[S39]</cite></statement> <statement>Longevity guarantees the benchmark remains relevant over time through updates and improvements, avoiding rapid obsolescence.<cite>[S41]</cite></statement> <statement>CRAG ensures realism by referencing smart assistant use cases during question generation, paraphrasing questions for diversity, and manually verifying ground truths.<cite>[S48]</cite></statement>",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the five critical features of the CRAG benchmark and how it ensures realism by incorporating smart assistant use cases.\n\nFirst, I need to look through the supporting context provided. The passage starts by mentioning that the CRAG benchmark has five critical features. Let me scan through the text to find those.\n\nLooking at the passage, there's a section that starts with \"We consider five critical features.\" Then each feature is listed with numbers 1 to 5. Let me check each one:\n\n1. Realism: The first feature is Realism. The text says it should reflect real use cases, and examples include questions similar to real-world QA scenarios. Also, the passage mentions that they referred to smart assistant use cases to make questions realistic. That's probably the answer for the second part of the question.\n\n2. Richness: The second feature is Richness, which involves diverse instance types, covering common and complex use cases.\n\n3. Insightfulness: The third feature is Insightfulness, allowing understanding of performance on different data slices.\n\n4. Reliability: The fourth feature is Reliability, ensuring accurate ground truths, reliable metrics, and statistical significance.\n\n5. Longevity: The fifth feature is Longevity, meaning the benchmark shouldn't expire and should be updated over time.\n\nNow, the second part of the question is about how CRAG ensures realism through smart assistant use cases. In the passage, there's a sentence that says, \"As we generated the questions, we referred to smart assistant use cases to make sure the questions are realistic...\" That's directly relevant. Also, paraphrasing questions and manual verification of ground truths are mentioned, but the key point here is the reference to smart assistant use cases.\n\nI need to make sure each feature is correctly cited. The features are mentioned in sentences [S29] to [S33] for Realism, [S35] for Richness, [S37] for Insightfulness, [S39] for Reliability, and [S41] for Longevity. The part about smart assistant use cases is in [S48].\n\nSo the answer should list the five features with their respective citations and then the realism part with [S48]. Each statement should be in <statement> tags with the correct citations."
    },
    {
      "question": "How does CRAG incorporate diverse question types and domains, and what specific keys are used for finance and movie queries?",
      "answer": "<statement>CRAG incorporates diverse question types and domains by including five domains: Finance, Sports, Music, Movie, and Open domain, along with seven complex question types such as Conditions, Comparison, Aggregation, Multi-hop, Set queries, Post-processing-heavy, and False-premise questions.<cite>[S45][S46]</cite></statement> <statement>For finance queries, specific keys include 'market_identifier' (e.g., stock symbols), 'metric' (e.g., 'price', 'P/E ratio'), and 'datetime' with a default of 'Query Time' if unspecified.<cite>[S915][S916][S917][S918][S919][S920]</cite></statement> <statement>For movie queries, specific keys include 'movie_name', 'movie_aspect' (e.g., 'release_date', 'budget'), 'person' (for person-related queries), and 'year' for queries about movies released in a specific year.<cite>[S921][S922][S923][S924][S925][S926][S927][S928]</cite></statement>",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking how CRAG incorporates diverse question types and domains, and what specific keys are used for finance and movie queries.\n\nFirst, I need to look through the supporting context provided. The passage mentions CRAG's dataset in Section 3. Specifically, sentence S45 says CRAG contains 4,409 QA pairs from five domains: Finance, Sports, Music, Movie, and Open domain. That answers the part about domains. Then, S46 mentions that besides simple-fact questions, there are seven types of complex questions: Conditions, Comparison, Aggregation, Multi-hop, Set queries, Post-processing-heavy, and False-premise. So that's the diverse question types.\n\nFor the specific keys for finance and movie queries, looking at Passage 2. Under 'finance' queries, the keys are listed as 'market_identifier', 'metric', 'datetime' (with defaults). The 'metric' has specific options like price, dividend, P/E ratio, etc. For 'movie' queries, the keys include 'movie_name', 'movie_aspect' with options like budget, genres, release_date, etc. There's also 'person' and 'year' mentioned for movies.\n\nI need to make sure the citations are correctly referenced. The domains and question types are in S45 and S46. The keys for finance are in S915-S919 and S920, while movie keys are in S921-S924 and S925-S928. Also, the specific keys for movie aspects are in S921-S922. I should structure each statement with the correct citations."
    },
    {
      "question": "What evaluation tasks are designed in CRAG to test RAG solutions, and how do they differentiate between hallucinated and missing answers?",
      "answer": "<statement>The evaluation tasks designed in CRAG to test RAG solutions include retrieval summarization, knowledge graph and web retrieval, and end-to-end retrieval-augmented generation. These tasks are intended to assess different components of RAG solutions comprehensively.<cite>[S53]</cite></statement> <statement>CRAG differentiates between hallucinated and missing answers by assigning a higher penalty to hallucinated answers, as they are considered more harmful to user trust and performance compared to missing answers, which are treated as incomplete but less detrimental.<cite>[S54]</cite></statement>",
      "chunk": "Passage 1:\n[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG\n\nPassage 2:\n[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
      "chunk_list": [
        "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
        "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the evaluation tasks designed in CRAG to test RAG solutions and how they differentiate between hallucinated and missing answers.\n\nFirst, I need to look through the provided supporting context. The passage mentions CRAG and its evaluation mechanism. Specifically, in the part where it says: \"Our second contribution is the evaluation mechanism to allow for reliable comparisons. We designed 3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph and web retrieval, and end-to-end retrieval-augmented generation (Section 2).\" That seems to answer the first part about the tasks. So the three tasks are retrieval summarization, knowledge graph and web retrieval, and end-to-end retrieval-augmented generation.\n\nNext, the question also asks how they differentiate between hallucinated and missing answers. Looking further in the context, there's a sentence: \"Instead of computing the percentage of correctly answered questions, our score system distinguishes hallucinated answers and missing answers, and gives the former a higher penalty as it can be more harmful to ruin user...\" This indicates that the evaluation mechanism differentiates them by penalizing hallucinations more than missing answers.\n\nI need to make sure these points are correctly cited. The tasks are mentioned in [S53], and the differentiation is in [S54]. Also, check if there are other relevant sentences. The rest of the passage talks about the benchmark's features and other details, but the key info is in those sections. So the answer should list the three tasks and explain the differentiation based on the penalty system."
    },
    {
      "question": "How is the accuracy of responses evaluated in the RAG solutions, and what criteria are used for human evaluation? Synthesize details from the evaluation rubric and scoring system.",
      "answer": "<statement>The accuracy of responses in RAG solutions is evaluated through human evaluation, where answers are scored as perfect (sp), acceptable (sa), missing (sm), or incorrect (sin), with truthfulness calculated by assigning sp = 1, sa = 0.5, sm = 0, and sin = -1.<cite>[S799][S800]</cite></statement> <statement>Human evaluation criteria prioritize factual accuracy, requiring answers to be factually correct and useful. Accuracy is rated as 0 (Missing) if there is no response, failure to answer, or follow-up questions, and 1 (Incorrect) for unintelligible, poorly formed, or factually erroneous responses.<cite>[S804][S805][S806][S824]</cite></statement> <statement>Specific criteria for \"Accuracy = 0 (Missing)\" include scenarios like no response, inability to answer (e.g., \"I can\u2019t find specific information\"), or responses that ask follow-up questions.<cite>[S807][S819][S823]</cite></statement> <statement>For \"Accuracy = 1 (Incorrect),\" criteria include major hallucinations (e.g., wrong dates, numbers), irrelevant answers, significant structural errors, or responses that fail to meet functional requirements.<cite>[S826][S828][S831][S833]</cite></statement> <statement>The evaluation process involves grading responses using a rubric that assesses factual correctness, with human evaluators using external search engines to verify accuracy against the criteria outlined in Section 4.1.<cite>[S804][S799]</cite></statement>",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about how the accuracy of responses is evaluated in RAG solutions and the criteria for human evaluation. They want details from the evaluation rubric and scoring system.\n\nFirst, I need to look through the provided supporting context. The passage has two parts. Let me start by scanning through them for relevant information.\n\nIn Passage 1, there's a section labeled \"A.2 Evaluation\" and \"A.2.1 Human evaluation\". That seems relevant. Let me check those parts. \n\nUnder \"A.2.1 Human evaluation\", it mentions that they run human evaluation to score each answer based on metrics from Section 4.1. The scoring is done with perfect (sp), acceptable (sa), missing (sm), or incorrect (sin) answers. The truthfulness is calculated by assigning sp=1, sa=0.5, sm=0, sin=-1. Then they average the truthfulness for all examples to get the score. \n\nThere's also a part about human-eval instructions. The instructions say to evaluate the factual accuracy using a grading rubric. The criteria for accuracy are given as 0 (Missing) or 1 (Incorrect). The \"Accuracy = 0 (Missing)\" covers situations like no response, failure to answer, or asking follow-up questions. \"Accuracy = 1 (Incorrect)\" includes things like unintelligible answers, major hallucinations, irrelevance, or structural errors. \n\nLooking at the citations here, the relevant sentences are [S799] to [S833]. For example, [S799] talks about scoring with sp, sa, sm, sin. [S800] defines truthfulnessh. [S804] mentions using an external search engine to evaluate factual accuracy. The criteria for accuracy are in [S805] to [S833].\n\nIn Passage 2, there's mention of auto-eval in Stage 1 and manual evaluation in Stage 2, but the user is asking about human evaluation criteria, so maybe that's less relevant. However, the prompts used in RAG solutions might not directly answer the question unless they relate to evaluation criteria. The prompts here are more about how the models are prompted rather than the evaluation criteria. \n\nSo the main details are in Passage 1's A.2.1 section. The answer should synthesize the evaluation rubric and scoring system. The scoring system uses the four categories (perfect, acceptable, missing, incorrect) with specific scores. The human evaluation criteria for accuracy are based on factual correctness, with missing and incorrect categories having specific scenarios. \n\nI need to structure the answer with statements citing the relevant sentences. For example, the scoring system is mentioned in [S799][S800], the truthfulness calculation in [S800], the accuracy criteria in [S805][S806], and the specific situations in [S807]-[S833]. \n\nI should make sure each statement is properly cited. Also, check if there are other parts in the passage that might contribute. For instance, the table (S829) shows accuracy and F1 scores for models, but that's about auto-evaluation, not human evaluation. So probably not relevant here.\n\nSo the final answer should outline the evaluation method (human evaluation with scoring metrics), the criteria for accuracy (missing vs incorrect), and the scoring system (sp, sa, sm, sin with their values). Each part should have the correct citations."
    },
    {
      "question": "What are the key components of the prompts used for RAG solutions, and how do they differ across tasks? Synthesize information from the Vanilla LLM, web search, and KG-based prompts.",
      "answer": "<statement>The key components of prompts for RAG solutions include the question, query time, and specific guidelines for answering. For Vanilla LLM, the prompt emphasizes brief answers and includes rules to address invalid questions or uncertainty, such as responding with \"I don\u2019t know\" [S880][S883][S884][S887].<cite>[S880][S883][S884][S887]</cite></statement>  \n\n<statement>RAG with web search (Task 1) adds \"References\" to the prompt, which may or may not aid in answering the question. The structure mirrors Vanilla LLM but incorporates retrieved web results as additional context [S888][S890][S892].<cite>[S888][S890][S892]</cite></statement>  \n\n<statement>RAG with KG and web search (Tasks 2 and 3) extends the references section to include both web results and knowledge graph (KG) data. This allows the model to leverage structured KG information alongside unstructured web content [S897][S899][S900].<cite>[S897][S899][S900]</cite></statement>  \n\n<statement>The prompts differ across tasks based on the data sources: Vanilla LLM relies solely on internal knowledge, web search integrates external textual references, and KG-based prompts combine web results with structured knowledge graph data. Additionally, task-specific prompts like query entity extraction focus on domain classification and entity extraction, adding JSON-based structured outputs [S906][S910][S912].<cite>[S906][S910][S912]</cite></statement>  \n\n<statement>Across all tasks, the prompts emphasize concise answers, query time awareness, and handling uncertainty. However, the inclusion of external data (web or KG) and task-specific requirements (e.g., entity extraction) distinguishes their design [S883][S892][S901].<cite>[S883][S892][S901]</cite></statement>",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the key components of prompts used in RAG solutions and how they differ across tasks, specifically looking at Vanilla LLM, web search, and KG-based prompts. They want a synthesis of information from the provided passages.\n\nFirst, I need to go through the supporting context given. The passages mention different prompts for various tasks. Let me start by identifying the relevant sections.\n\nLooking at Passage 2, there are sections about the prompts used in straightforward RAG solutions. Under \"A.3.1 Prompts used in straightforward RAG solutions,\" there are three subsections: Vanilla LLM, RAG with web search (Task 1), and RAG with KG and web search (Tasks 2 and 3). Each of these has their own prompt structure.\n\nFor the Vanilla LLM prompt, the key components are the question, query time, and guidelines for answering. The guidelines include checking for false premises and responding with \"I don't know\" if uncertain. The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\" (S881). The prompt also emphasizes brief answers (S883).\n\nNext, the RAG with web search (Task 1) prompt adds \"References\" to the components. The references may or may not help answer the question. The structure is similar to Vanilla LLM but includes the references section (S888-896). The guidelines are the same as Vanilla LLM.\n\nThen, the RAG with KG and web search (Tasks 2 and 3) prompt includes both web results and knowledge graph (KG) responses. The references section is split into \"web\" and \"kg_response\" (S897-905). The guidelines are the same as the previous ones, but the additional KG data is part of the references.\n\nThe user also mentioned \"query entity extraction\" in Passage 2 (S906-914). This prompt is for extracting structured information, determining the domain, and extracting entities. The domains are specified, and the output is JSON with keys like \"domain\" and \"main_entity\" for certain domains.\n\nNow, the question asks to synthesize information from Vanilla LLM, web search, and KG-based prompts. The key components for each would be:\n\n- Vanilla LLM: Question, Query Time, Guidelines (invalid question, I don't know).\n- Web Search: Adds References (web results).\n- KG-based: Adds both web and KG references.\n\nDifferences across tasks: The Vanilla LLM doesn't use external data, while web search and KG-based incorporate references. The KG-based includes knowledge graph data in addition to web results. The entity extraction prompt is a separate task but might be part of the broader RAG setup.\n\nI need to make sure I cite the correct sentence numbers. For example, the Vanilla LLM prompt components are in S880-S883, S884-S887. The web search prompt is S888-S896, and the KG-based is S897-S905. The entity extraction is S906-S914.\n\nI should structure the answer by listing each component and their citations, then explain the differences. Each statement should be in <statement> tags with appropriate citations. Need to check if all relevant parts are covered and cited correctly."
    },
    {
      "question": "How is the query time handled in the RAG evaluation process, and what role does it play in generating answers? Synthesize details from the prompt structures and evaluation criteria.",
      "answer": "<statement>The query time is incorporated into the prompts used in the RAG evaluation process to ensure answers are generated based on the temporal context of the query. For example, the Vanilla LLM prompt explicitly includes the \"Query Time\" formatted as \"mm/dd/yyyy, hh:mm:ss PT\" and instructs the model to answer the question considering this time. Similarly, RAG prompts with web search or knowledge graph (KG) references also include the query time to align the generated answers with the information available at that specific moment.<cite>[S881][S882][S889][S890][S898][S899]</cite></statement>  \n\n<statement>The role of query time is critical in ensuring factual accuracy by restricting the retrieval and synthesis of information to what was available up to the specified time. This prevents the use of outdated or future data, aligning the answers with the constraints of the original query. For instance, the evaluation process emphasizes that answers must be factually correct based on the query time and retrieved results, as highlighted in the prompt structures and human evaluation criteria.<cite>[S876][S877][S804]</cite></statement>  \n\n<statement>In the prompt design, query time is explicitly provided to the language model (LLM) alongside the question and retrieved references. This ensures the LLM generates answers that are temporally consistent with the query's context. For example, the RAG with KG and web search prompt includes the query time to guide the model in synthesizing answers that reflect the state of knowledge at that specific moment.<cite>[S897][S898][S900]</cite></statement>",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking how query time is handled in the RAG evaluation process and what role it plays in generating answers. They also want details synthesized from the prompt structures and evaluation criteria.\n\nFirst, I need to look through the provided supporting context. The passage mentions several things. In Passage 2, there's a section about the prompts used in straightforward RAG solutions. Specifically, under \"A.3.1 Prompts used in straightforward RAG solutions,\" there are different prompts for Vanilla LLM, RAG with web search, and RAG with KG and web search. Each of these prompts includes the \"Query Time\" as part of the input. \n\nFor example, the Vanilla LLM prompt says: \"You are given a Question and the time when it was asked in the Pacific Time Zone (PT), referred to as 'Query Time'.\" The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The task is to answer the question in as few words as possible, considering the query time. Similarly, the RAG prompts also include the query time, which is used to ensure the answers are based on the information available up to that time.\n\nAnother part in Passage 2, under \"A.3 Evaluating straightforward RAG solutions,\" states: \"We send each CRAG question in a prompt shown below. This prompt is designed to include the original Query Time for the question and ask the LLM to answer the question based on the query time and the retrieved result.\" This indicates that the query time is a critical component in the prompt, ensuring that the model uses the correct temporal context.\n\nAdditionally, the evaluation criteria mention that the answers must be factually accurate based on the query time. For instance, in the human evaluation section (Passage 1), there's a mention of evaluating factual accuracy using an external search engine, which would take into account the time the query was made. The query time ensures that the information retrieved and used to generate the answer is relevant to the time the query was asked, preventing the use of outdated or future data.\n\nThe role of query time in generating answers is to provide a temporal context. This is crucial because the information available at the time of the query might differ from the current data. By including the query time, the RAG system ensures that the answers are generated based on the data that was accessible up to that specific moment, enhancing the accuracy and relevance of the responses.\n\nLooking for citations, the relevant sentences are in Passage 2. The prompts for Vanilla LLM, RAG with web search, and RAG with KG and web search all mention the inclusion of query time. Specifically, sentences [S881]-[S883] for the Vanilla LLM, [S889]-[S892] for the RAG with web search, and [S898]-[S901] for the RAG with KG and web search. Also, the section in Passage 2 under \"A.3 Evaluating straightforward RAG solutions\" mentions including the original Query Time, which is [S876]. \n\nAdditionally, the part about the query time being formatted as \"mm/dd/yyyy, hh:mm:ss PT\" is in [S881] and [S889]. The role of query time in ensuring answers are based on the query time and retrieved results is in [S876] and [S877]. \n\nSo, the answer should explain that query time is included in the prompts to provide temporal context, ensuring answers are based on data available at that time. The citations would be the relevant sentences from the passages."
    },
    {
      "question": "What is the purpose of the Query entity extraction prompt, and what structured information does it extract? Synthesize details from the entity extraction instructions and domain classification.",
      "answer": "<statement>The purpose of the Query entity extraction prompt is to determine the domain of the query and extract structured information based on that domain. It categorizes queries into predefined domains such as \"finance,\" \"sports,\" \"music,\" \"movie,\" \"encyclopedia,\" or \"other\" and generates JSON output with domain-specific structured data.<cite>[S906][S909][S910][S911]</cite></statement> <statement>For queries in the \"encyclopedia\" or \"other\" domains, the structured information includes a key \"main_entity\" to capture the primary entity mentioned in the query. The prompt emphasizes extracting domain-dependent structured data directly into the JSON result.<cite>[S913][S914]</cite></statement>",
      "chunk": "Passage 1:\n[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy\n\nPassage 2:\n[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
      "chunk_list": [
        "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
        "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the purpose of the Query entity extraction prompt and what structured information it extracts. They also want me to synthesize details from the entity extraction instructions and domain classification.\n\nFirst, I need to look through the supporting context provided. The passage mentions several sections, but I should focus on where the Query entity extraction is discussed. Scanning through the passages, I see Passage 2 has a section labeled \"Query entity extraction\" with a prompt starting at [S906]. \n\nLooking at that section, the prompt says the agent should output JSON and perform two main tasks: determine the domain of the query and extract structured information. The domains listed are finance, sports, music, movie, encyclopedia, or other. For the structured information, the example given is for 'encyclopedia' and 'other' queries, where the key 'main_entity' is used. However, the user is asking about the structured info in general, not just those domains. \n\nBut the context might not have details on all domains. The passage mentions that depending on the domain, different keys are included. However, the specific examples provided in the context are only for 'encyclopedia' and 'other'. The user's question is about the purpose and the structured info extracted. The purpose is to determine the domain and extract structured data based on that. The structured info varies by domain, but the example given is 'main_entity' for some domains. \n\nWait, the user also mentioned synthesizing details from entity extraction instructions and domain classification. The domain classification is part of the process, where the query is categorized into one of the specified domains. The structured information extraction would then depend on that domain. However, the context provided doesn't list all possible keys for each domain. It only gives an example for 'encyclopedia' and 'other'. \n\nSo, based on the given context, the answer should state that the purpose is to identify the query's domain and extract structured data, with examples like 'main_entity' for certain domains. Since the context doesn't provide more details on other domains, I should stick to what's there. \n\nI need to make sure to cite the relevant sentences. The part about the purpose is in [S906] to [S914]. Specifically, [S906] mentions the prompt's purpose, [S909]-[S911] about domain classification, and [S913]-[S914] about extracting structured info. The example with 'main_entity' is in [S914]. \n\nSo the answer would be that the purpose is to determine the domain and extract structured info, and the structured info includes keys like 'main_entity' for certain domains. The citations would be [S906][S909]-[S911], [S913]-[S914]."
    },
    {
      "question": "What components make up the benchmark dataset described in the source context, and what specific metrics are used to evaluate RAG systems, including the scoring system for labeling answers?",
      "answer": "<statement>The benchmark dataset described in the source context consists of mock knowledge graphs (KGs), mock APIs, 220K webpages, and a KG containing 2.6M entities. Mock KGs include publicly available KG data, randomly selected entities of the same type, and \"hard negative\" entities. Mock APIs have pre-defined parameters to support structured search in the mock KGs. The dataset also includes 38 mock APIs for evaluation purposes.<cite>[S145][S147][S153]</cite></statement> <statement>The RAG systems are evaluated using four metrics for labeling answers: \"perfect,\" \"acceptable,\" \"missing,\" and \"incorrect.\" The scoring system assigns scores of 1, 0.5, 0, and -1, respectively. \"Perfect\" answers are correct with no hallucinations, \"acceptable\" answers are useful but may have minor errors, \"missing\" answers indicate inability to respond (e.g., \"I don\u2019t know\"), and \"incorrect\" answers provide wrong or irrelevant information. Truthfulness is calculated as the average score across all examples.<cite>[S157][S166]</cite></statement> <statement>Evaluation methods include human evaluation (human-eval) and model-based automatic evaluation (auto-eval). For auto-eval, perfect and acceptable answers are merged into \"accurate,\" with scores of 1, -1, and 0 for accurate, incorrect, and missing answers, respectively. A two-step process is used: exact matches are considered accurate, while others are assessed via LLMs (e.g., ChatGPT and Llama 3) to determine accuracy, hallucination, or missing status. This avoids self-preference issues by averaging results from multiple models.<cite>[S168][S170][S172][S173]</cite></statement>",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the components of the benchmark dataset and the specific metrics used to evaluate RAG systems, including the scoring system for labeling answers.\n\nFirst, I need to look through the provided supporting context. The passage mentions \"Mock KGs\" and \"Mock APIs\" in sentences S145 to S153. It says they created mock KGs with public data, selected entities, and hard negatives. Then there are mock APIs with predefined parameters. The dataset includes 220K webpages, 2.6M entities in the KG, and 38 mock APIs. So those are the components of the dataset.\n\nNext, the metrics part. In the section starting with \"4 Metrics and Evaluation,\" they talk about labeling answers as perfect, acceptable, missing, or incorrect. The scoring is 1, 0.5, 0, and -1 for each category respectively. They also mention truthfulness as the average score. For evaluation, they use human evaluation and model-based auto-eval, with a two-step method involving exact matches and LLMs. The scoring system for auto-eval merges perfect and acceptable into accurate, using 1, -1, 0 for accurate, incorrect, missing.\n\nI need to make sure I cite the correct sentences. The components are in S145-S153, specifically S145 (mock KGs), S147 (mock APIs), S153 (numbers). The metrics are in S157-S166, where they describe the labeling criteria and scores. The evaluation methods are in S168-S173, mentioning human and auto-eval, and the two-step method.\n\nI should structure the answer with each component and metric, citing the relevant sentences. Need to check if all parts of the question are covered: components (KGs, APIs, webpages, entities), metrics (perfect, acceptable, missing, incorrect with scores), and the scoring system. Also, the evaluation methods like human and model-based, and the two-step process."
    },
    {
      "question": "How does the two-step automatic evaluation method for RAG systems work, and what role do LLM evaluators like ChatGPT and Llama 3 play in this process?",
      "answer": "<statement>The two-step automatic evaluation method for RAG systems first checks if the answer matches the ground truth exactly, classifying it as accurate. If not, it employs LLMs to determine accuracy, incorrectness, or missing status of the response.<cite>[S171]</cite></statement> <statement>LLM evaluators like ChatGPT and Llama 3 are used in the second step to assess answers when there is no exact match with the ground truth. They help determine whether the response is accurate, incorrect, or missing, and their results are averaged to reduce bias.<cite>[S172]</cite></statement>",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the two-step automatic evaluation method for RAG systems and the role of LLM evaluators like ChatGPT and Llama 3.\n\nFirst, I need to find the relevant parts in the supporting context. The passage mentions \"two-step method for automatic evaluation\" in sentence S171. Let me check that. \n\nLooking at S171: \"We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly, it is considered accurate; otherwise, we use LLMs to determine whether the response is accurate, incorrect, or missing.\" So the first step is checking exact match with ground truth. If not, the second step uses LLMs.\n\nThen, the next part in S172 says: \"To avoid the self-preference problem [25], we use two LLM evaluators: ChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the average accurate, hallucination, missing rates, and truthfulness scores from the two models for each RAG system.\" This explains that the LLMs are used in the second step to assess the answers when there's no exact match. They use both models and average the results to reduce bias.\n\nSo the two-step method first checks for an exact match. If that fails, they use LLMs like ChatGPT and Llama 3 to evaluate the answer's accuracy. The LLMs help in determining if the answer is accurate, incorrect, or missing when the exact match isn't there. The citation for the two-step method is S171 and S172. The role of the LLMs is detailed in S172."
    },
    {
      "question": "According to the source, what criteria define a 'perfect' answer, and how does the truthfulness metric for RAG systems account for hallucinations and missing answers?",
      "answer": "<statement>The criteria for a 'perfect' answer are that the response correctly answers the user\u2019s question and contains no hallucinated content.<cite>[S159]</cite></statement> <statement>The truthfulness metric for RAG systems accounts for hallucinations and missing answers by assigning scores of 1, 0.5, 0, and \u22121 for perfect, acceptable, missing, and incorrect answers, respectively. Hallucinated answers are penalized, and missing answers are preferred over incorrect ones, with truthfulness defined as the average score across all examples.<cite>[S166][S167]</cite></statement>",
      "chunk": "Passage 1:\n[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.\n\nPassage 2:\n[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
      "chunk_list": [
        "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
        "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the criteria that define a 'perfect' answer according to the source and how the truthfulness metric for RAG systems accounts for hallucinations and missing answers.\n\nFirst, I need to look through the provided supporting context. The main passage here is Passage 1, which discusses metrics and evaluation for RAG systems. Let me scan through the sentences in that passage.\n\nLooking at the sentences, I see that in [S157], they mention labeling answers as perfect, acceptable, missing, or incorrect. Then, [S159] specifically defines \"Perfect\" as a response that correctly answers the user's question and contains no hallucinated content. That's the criteria for a perfect answer. So that's the first part of the answer.\n\nNext, the truthfulness metric. In [S166], it says they use a scoring method with 1, 0.5, 0, and -1 for perfect, acceptable, missing, and incorrect. They penalize hallucinated answers and prefer missing over incorrect. Then [S167] defines truthfulness as the average score from all examples. Also, in [S168], they mention the evaluation methods, and [S172] talks about using two LLM evaluators to determine accuracy, hallucination, etc. \n\nBut the question is about how the truthfulness metric accounts for hallucinations and missing answers. The key points are that perfect answers have no hallucinations, and missing answers are preferred over incorrect ones. The scoring system penalizes hallucinations by giving lower scores to incorrect answers, and missing answers are given a 0, which is better than -1 for incorrect. So the truthfulness metric uses these scores to account for them.\n\nI need to make sure I cite the correct sentences. For the perfect answer criteria, [S159] is the main one. For the truthfulness part, [S166] and [S167] are important. Also, [S160] mentions the 6, which might be a typo or a number from a figure, but maybe not relevant. The part about penalizing hallucinated answers is in [S166]. The mention of preferring missing over incorrect is also in [S166].\n\nSo the answer should state that a perfect answer is correct and has no hallucinations (cite [S159]), and truthfulness accounts for hallucinations by penalizing them and preferring missing answers, using the scoring system described in [S166] and [S167]."
    },
    {
      "question": "What are the three key contributions of the CRAG benchmark as described in the source context, and how do they address the limitations of traditional QA benchmarks?",
      "answer": "<statement>The three key contributions of the CRAG benchmark are: (1) the design of an effective automatic evaluation mechanism to enable fast evaluations and iterations [S57]; (2) a comprehensive evaluation of both straightforward RAG solutions and industry state-of-the-art RAG solutions [S58]; and (3) the inclusion of diverse features such as dynamic question handling, realistic testing with mock APIs, and extensive content beyond Wikipedia to address the limitations of traditional QA benchmarks [S72]. These contributions address the limitations of traditional QA benchmarks, which primarily focus on static web or knowledge graph (KG) retrieved content and lack coverage of dynamic, complex, and diverse challenges [S66].<cite>[S57][S58][S72][S66]</cite></statement> <statement>The CRAG benchmark's automatic evaluation mechanism [S57] and comprehensive evaluation of RAG solutions [S58] provide insights into system performance across multiple dimensions, while its features like dynamic questions, mock APIs, and diverse fact populations [S72] directly tackle the shortcomings of traditional benchmarks, which fail to represent real-world dynamics and complexity [S66].<cite>[S57][S58][S72][S66]</cite></statement>",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking for the three key contributions of the CRAG benchmark and how they address the limitations of traditional QA benchmarks. I need to look through the provided passage to find the relevant information.\n\nFirst, I'll scan the passage for mentions of the CRAG benchmark's contributions. The user mentioned that the answer should include citations, so I need to note the sentence numbers where each contribution is discussed.\n\nLooking at the passage, there's a section that says: \"Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry state-of-the-art solutions on RAG (Section 5).\" That's one contribution. Then, there's a part about the evaluation serving two roles: demonstrating CRAG's difficulty and highlighting research gaps. But maybe that's part of the third contribution. Wait, the user is asking for three contributions. Let me check again.\n\nAnother part says: \"The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge...\" but that might be more about the impact rather than a contribution. Then, looking at the comparison with existing benchmarks, the passage mentions that traditional benchmarks don't cover diverse and dynamic challenges. The CRAG has features like comprehensive coverage, realistic testing with mock APIs, dynamic question handling, etc. But are these the contributions or just features?\n\nWait, the user is asking for the three key contributions. Let me check the exact sentences. The passage states: \"We also design an effective automatic evaluation mechanism...\" (S57), \"Our third contribution is a comprehensive evaluation...\" (S58). So maybe the first contribution is the automatic evaluation mechanism. The second contribution could be the comprehensive evaluation of RAG solutions. The third might be the features like dynamic questions, mock APIs, etc. But the exact contributions might be listed in the text.\n\nWait, the passage says: \"Our third contribution is a comprehensive evaluation...\" So that's the third contribution. Then, the first contribution is the automatic evaluation mechanism (S57). The second contribution might be the design of the benchmark itself with diverse features. But the text doesn't explicitly list three contributions. Let me check again.\n\nLooking at the passage, the user might be referring to the three contributions mentioned in the context. The passage mentions three contributions: 1) effective automatic evaluation mechanism (S57), 2) comprehensive evaluation of RAG solutions (S58), and 3) the benchmark's features like dynamic questions, mock APIs, etc. But the exact wording might be different. Alternatively, the three contributions could be the comprehensive coverage, realistic testing, and dynamic handling mentioned in S72.\n\nWait, the user's question is about the three key contributions as described in the source context. The passage says: \"Our third contribution is a comprehensive evaluation...\" So that's one. Then, the first contribution is the automatic evaluation (S57). The second contribution might be the design of the benchmark with diverse aspects. But the text might not explicitly list three. Alternatively, the three contributions could be the automatic evaluation, the comprehensive evaluation of solutions, and the benchmark's features. \n\nAlso, the passage mentions that traditional benchmarks have limitations, and CRAG addresses them by including dynamic questions, mock APIs, etc. So the three contributions would be the automatic evaluation, the comprehensive evaluation of solutions, and the benchmark's design with diverse features. Each of these would address the limitations of traditional benchmarks, which didn't cover dynamic challenges, had limited diversity, etc.\n\nSo, compiling this, the three contributions are:\n\n1. Effective automatic evaluation mechanism (S57)\n2. Comprehensive evaluation of RAG solutions (S58)\n3. Features like dynamic question handling, mock APIs, diverse fact popularity, etc. (S72)\n\nThen, how they address traditional limitations: traditional benchmarks didn't handle dynamic, diverse, or complex questions, while CRAG's features do. The citations would be [S57], [S58], [S72]. \n\nI need to make sure each statement is in <statement> tags with the correct citations. Also, check if the answer is accurate based on the passage."
    },
    {
      "question": "How does the CRAG benchmark differ from traditional QA benchmarks in terms of evaluation metrics and testing scenarios, and what specific features make it more suitable for evaluating RAG systems?",
      "answer": "<statement>The CRAG benchmark differs from traditional QA benchmarks by addressing diverse and dynamic challenges that RAG systems face, such as real-time facts, tail entities, and complex questions requiring set answers or post-processing, which traditional benchmarks like NQ, TriviaQA, MS MARCO, and QALD-10 do not adequately represent.<cite>[S66]</cite></statement> <statement>Traditional QA benchmarks typically use matching-based metrics like ROUGE or F1, which are less effective for LLMs generating free-form responses, whereas CRAG emphasizes metrics tailored for evaluating RAG systems' reliability and handling of dynamic, diverse, and complex scenarios.<cite>[S70][S71]</cite></statement> <statement>CRAG's specific features include comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation, making it more suitable for assessing RAG systems' ability to manage real-world information retrieval and synthesis challenges.<cite>[S72][S73]</cite></statement> <statement>CRAG introduces testing scenarios such as tail entities, dynamic questions, and false premises, which highlight gaps in RAG systems' performance, particularly in handling low-popularity facts, real-time data, and complex reasoning tasks that traditional benchmarks overlook.<cite>[S210][S212]</cite></statement>",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ],
      "thinking": "Okay, let's tackle this question. The user wants to know how the CRAG benchmark differs from traditional QA benchmarks in terms of evaluation metrics and testing scenarios, and what specific features make it more suitable for evaluating RAG systems.\n\nFirst, I need to look through the provided passage to find relevant information. Let me scan through the text. \n\nIn the passage, there's a section that says: \"Traditional QA benchmarks such as Natural Questions (NQ), TriviaQA, MS MARCO, and QALD-10 have advanced QA in the past decade but consider only web retrieved or KG retrieved contents, and do not adequately represent the diverse and dynamic challenges that RAG is facing.\" That seems to address the difference in testing scenarios. Traditional benchmarks only use web or knowledge graph retrieval, while CRAG includes more diverse and dynamic challenges.\n\nThen, looking at evaluation metrics: \"Traditional QA benchmarks usually adopt matching-based metrics such as ROUGE or F1 to evaluate the quality of the responses... These metrics, although working well for extractive methods, are known to not perform very effectively for LLMs that generate free-form responses.\" So CRAG might use different metrics that are better for LLMs. The passage also mentions \"comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation.\" These are specific features of CRAG.\n\nAnother part says: \"CRAG offers several distinctive advantages: comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation.\" These features make it suitable for RAG systems. Also, the mention of \"dynamic question handling\" and \"realistic testing with mock APIs\" suggests that CRAG's testing scenarios are more varied and realistic compared to traditional ones.\n\nThe passage also notes that CRAG includes \"questions with higher dynamism, lower popularity, or higher complexity,\" which traditional benchmarks don't cover. Additionally, the evaluation includes aspects like handling set answers, post-processing, and false premises, which are specific to RAG systems.\n\nSo putting this together, the differences in evaluation metrics are the shift from ROUGE/F1 to metrics that handle free-form responses, and the testing scenarios include dynamic, diverse, and real-world challenges. The specific features of CRAG are comprehensive coverage, mock APIs, dynamic questions, diverse facts, beyond Wikipedia content, and fast evaluation."
    },
    {
      "question": "What insights does the evaluation of state-of-the-art (SOTA) RAG solutions on CRAG reveal about their performance compared to straightforward RAG solutions, and what are the limitations still faced by SOTA systems?",
      "answer": "<statement>The evaluation of state-of-the-art (SOTA) RAG solutions on CRAG reveals that they achieve higher truthfulness (up to 51%) compared to straightforward RAG solutions, which only reach 44% accuracy when adding RAG in a straightforward manner. However, SOTA systems still face significant hallucination rates (16%\u201325%), indicating limitations in trustworthiness.<cite>[S59][S223][S224]</cite></statement>  \n\n<statement>SOTA RAG solutions struggle with dynamic, low-popularity, or complex factual queries, such as real-time data, torso and tail entities, and tasks requiring aggregation, multi-hop reasoning, or post-processing. These challenges highlight gaps in handling retrieval noise and complex reasoning compared to straightforward solutions.<cite>[S60][S227]</cite></statement>  \n\n<statement>While SOTA systems show advancements in addressing set answers and false premises compared to straightforward solutions, their performance on these tasks remains suboptimal, indicating ongoing research needs for robust reasoning and synthesis of diverse information sources.<cite>[S230]</cite></statement>  \n\n<statement>The evaluation also underscores that SOTA systems' truthfulness scores are not fully comparable to straightforward solutions due to differences in retrieval access and evaluation methods (e.g., auto-eval vs. human-eval), but the overall trend highlights persistent limitations in reliability and adaptability.<cite>[S225]</cite></statement>",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the insights from evaluating SOTA RAG solutions on CRAG compared to straightforward RAG solutions and the limitations they still face.\n\nFirst, I need to look through the provided passage to find relevant sentences. Let me scan through the text. \n\nIn Passage 1, there's a mention of evaluating straightforward RAG solutions and industry SOTA solutions. Specifically, [S59] says that most advanced LLMs have 34% accuracy on CRAG, and adding RAG straightforwardly improves it to 44%. Then [S60] states that SOTA industry RAG solutions answer 63% without hallucination but struggle with dynamic, less popular, or complex facts. \n\nIn Passage 2, [S223] notes that SOTA solutions have better truthfulness (up to 51%) than straightforward ones, but hallucination rates are still 16-25%. [S225] mentions that truthfulness scores aren't fully comparable due to different retrieval access and evaluation methods. [S227] points out that SOTA systems still face difficulties with real-time queries, torso/tail entities, and tasks requiring aggregation or multi-hop reasoning. [S230] adds that SOTA improved on set and false premise questions but there's still room for improvement.\n\nSo, the key points are: SOTA systems perform better in truthfulness but have high hallucination rates. They still struggle with dynamic facts, less popular entities, complex queries, and tasks needing multi-step reasoning. The limitations include handling retrieval noise, real-time data, and complex reasoning tasks. \n\nI need to structure the answer with these insights, citing the relevant sentences. Each statement should be in <statement> tags with citations. Make sure to separate each insight into individual statements if needed."
    },
    {
      "question": "What specific improvements in RAG systems are highlighted by the CRAG benchmark, and how do these improvements relate to challenges like hallucination rates and dynamic factual accuracy?",
      "answer": "<statement>The CRAG benchmark highlights specific improvements in RAG systems, including enhanced handling of dynamic factual accuracy, tail entities, and complex questions requiring set answers or post-processing. These improvements are evidenced by state-of-the-art (SOTA) RAG solutions achieving 63% accuracy without hallucination, though challenges remain in addressing facts with higher dynamism, lower popularity, or complexity [S60].<cite>[S60]</cite></statement>  \n\n<statement>CRAG emphasizes dynamic question handling and diverse fact popularity, which address challenges like real-time factual accuracy. For example, SOTA RAG systems show improved performance on torso and tail entities compared to straightforward RAG solutions, though they still struggle with head entities and fast-changing facts [S212].<cite>[S212]</cite></statement>  \n\n<statement>The benchmark also reveals advancements in reducing hallucination rates. While SOTA RAG solutions achieve higher truthfulness (up to 51%) than straightforward methods, their hallucination rates range from 16% to 25%, indicating ongoing challenges in ensuring reliability [S224].<cite>[S224]</cite></statement>  \n\n<statement>CRAG\u2019s focus on realistic testing, such as mock APIs and diverse content beyond Wikipedia, enables evaluations of RAG systems\u2019 ability to handle dynamic and complex queries. This includes improvements in detecting false premises and providing accurate set answers, though reasoning-heavy tasks like multi-hop or aggregation queries remain challenging [S229].<cite>[S229]</cite></statement>  \n\n<statement>Additionally, the benchmark underscores the trade-offs between latency and quality in SOTA systems, such as Copilot Pro\u2019s high truthfulness but increased latency versus Meta SG\u2019s lower latency but mid-tier performance, highlighting the need for further optimization in dynamic environments [S226].<cite>[S226]</cite></statement>",
      "chunk": "Passage 1:\n[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set\n\nPassage 2:\n[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
      "chunk_list": [
        "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
        "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the specific improvements in RAG systems highlighted by the CRAG benchmark and how these relate to challenges like hallucination rates and dynamic factual accuracy.\n\nFirst, I need to look through the provided supporting context. The passage mentions the CRAG benchmark and its evaluations. Let me scan through the sentences to find relevant parts.\n\nLooking at the sentences, [S60] talks about state-of-the-art industry RAG solutions answering 63% of questions without hallucination but still having lower accuracy on dynamic, less popular, or complex facts. That relates to hallucination rates and dynamic accuracy. \n\nThen, [S72] mentions CRAG's advantages like dynamic question handling and diverse fact popularity. This suggests that CRAG addresses dynamic factual accuracy by including such questions. \n\nIn [S212], there's a mention of straightforward RAG improving QA quality for torso and tail entities but lowering it for head entities. This shows improvements in handling less common (tail) facts but issues with common ones. \n\n[S225] notes that even SOTA solutions have hallucination rates between 16-25%, indicating that while there's improvement, there's still room for reducing hallucinations. \n\nAlso, [S229] states that SOTA solutions improved on set and false premise questions, showing advancements in handling complex queries, which ties into dynamic factual accuracy. \n\nI need to structure these points into statements with proper citations. Make sure each sentence cites the correct numbers. Check if all relevant parts are covered and that the answer connects the improvements to the challenges mentioned."
    },
    {
      "question": "What are the key limitations of existing RAG datasets according to the CRAG benchmark, and how does CRAG address these limitations through its design elements?",
      "answer": "<statement>Existing RAG datasets do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks, according to the CRAG benchmark.<cite>[S4]</cite></statement> <statement>Key limitations include inadequate coverage of real-world QA dynamics, lower accuracy for questions involving high-dynamics, low-popularity entities, or complex queries, and insufficient mitigation of hallucinations in state-of-the-art RAG solutions.<cite>[S10][S11]</cite></statement> <statement>CRAG addresses these limitations through its design elements, including a diverse dataset of 4,409 question-answer pairs spanning five domains and eight categories, mock APIs to simulate web/KG search, and inclusion of temporal dynamism (from seconds to years) and entity popularity variations (popular to long-tail).<cite>[S6]</cite></statement> <statement>The benchmark also highlights the gap in trustworthiness, as even advanced LLMs achieve only 34% accuracy, and RAG improvements are limited to 44%, emphasizing the need for better retrieval synthesis and noise handling.<cite>[S8][S9]</cite></statement> <statement>CRAG's design further challenges researchers to address issues like retrieval noise, KG data utilization, and search ranking, as evidenced by its evaluation metrics and domain-specific performance disparities.<cite>[S193][S195]</cite></statement>",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the key limitations of existing RAG datasets according to the CRAG benchmark and how CRAG addresses these through its design elements.\n\nFirst, I need to look through the provided passage to find where the limitations are mentioned. The abstract mentions that existing RAG datasets don't adequately represent the diverse and dynamic nature of real-world QA tasks. That's one limitation. Then, in the introduction, there's talk about hallucinations in LLMs, which RAG aims to address. But the CRAG benchmark points out that even with RAG, there are issues like lower accuracy for dynamic facts, less popular entities, and complex questions.\n\nLooking at the specific sentences, [S4] states that existing datasets don't capture the diversity and dynamics. [S10] mentions that state-of-the-art RAG solutions only answer 63% without hallucination, indicating a limitation. [S11] points out lower accuracy for dynamic, less popular, or complex facts. \n\nNow, how does CRAG address these? The abstract says CRAG is designed with diverse questions across domains and categories, varying entity popularity, and temporal dynamics. [S6] mentions the benchmark includes 4,409 pairs and mock APIs. Also, the evaluation highlights the gap in trustworthiness, leading to the design elements that test these aspects. The introduction's mention of CRAG's design elements like diverse domains and categories, and handling dynamism, popularity, and complexity would be the answers.\n\nI need to make sure each statement is cited correctly. For example, the first limitation is from [S4], and the design elements from [S6]. The lower accuracy in certain cases is from [S11], and the addressing through design is in [S6] and the introduction. Also, the part about CRAG's evaluation showing the gap is [S7]. \n\nI should structure the answer by first stating the limitations with their citations, then the design elements addressing them. Need to check each part to ensure all citations are correctly linked and that each statement is properly formatted."
    },
    {
      "question": "How does the performance of LLMs and RAG systems compare on the CRAG benchmark, and what challenges does this highlight for RAG solutions?",
      "answer": "<statement>The performance of LLMs and RAG systems on the CRAG benchmark shows that most advanced LLMs achieve approximately 34% accuracy, while adding RAG in a straightforward manner improves accuracy to 44%.<cite>[S8][S9]</cite></statement> <statement>State-of-the-art industry RAG solutions, however, only answer 63% of questions without any hallucination, highlighting limitations in ensuring factual accuracy despite improved reliability.<cite>[S10]</cite></statement> <statement>The CRAG benchmark reveals that RAG systems face challenges in handling questions with higher dynamism, lower popularity, or higher complexity, as accuracy drops significantly in these cases.<cite>[S11]</cite></statement> <statement>A key challenge for RAG solutions is balancing retrieval relevance and hallucination risk, as even the best RAG systems struggle to maintain truthfulness above 20%, often due to noise from irrelevant retrieval results.<cite>[S193]</cite></statement> <statement>Additionally, while Knowledge Graph (KG) data improves accuracy with lower hallucination rates, leveraging KG effectively remains a challenge, as improvements are described as \"mediocre\" in practice.<cite>[S195]</cite></statement> <statement>The benchmark also emphasizes the critical role of search ranking in RAG, as over 30% of ground truths are available in web retrieval results but excluded due to context window limitations.<cite>[S198]</cite></statement>",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the performance comparison between LLMs and RAG systems on the CRAG benchmark and the challenges it highlights for RAG solutions.\n\nFirst, I need to look through the provided supporting context to find relevant information. The passage mentions several key points. In the abstract, it states that most advanced LLMs achieve around 34% accuracy on CRAG, while adding RAG improves it to 44%. Also, state-of-the-art RAG solutions answer 63% of questions without hallucination. Then, in Passage 2, there's more detail: the best LLM-only solutions (GPT-4 Turbo) have 34% accuracy and 20% truthfulness. RAG solutions get up to 44% accuracy but none exceed 20% truthfulness because of hallucinations from irrelevant retrievals. There's also mention of challenges like handling dynamic facts, low popularity entities, and complex questions.\n\nI need to structure the answer with citations. The first part about LLMs at 34% and RAG at 44% comes from S8 and S9. The 63% without hallucination is S10. The truthfulness issue with RAG is in S193. The challenges like dynamic facts and complexity are in S11. Also, the part about KG helping but with mediocre improvement is S195. The search ranking importance is S198. \n\nI should make sure each statement is correctly cited. For example, the accuracy numbers are from S8 and S9, the truthfulness from S193 and S10. The challenges are spread across S11, S193, S195, and S198. Need to check each sentence's citations properly."
    },
    {
      "question": "What factors contribute to lower RAG truthfulness in specific domains and question types, and how does the CRAG benchmark reveal these issues?",
      "answer": "<statement>The factors contributing to lower RAG truthfulness include higher dynamism of facts, lower popularity of entities, and higher complexity of questions. Specifically, CRAG highlights that RAG accuracy drops for facts with rapid changes, less popular entities, and complex question types like multi-hop or post-processing. [S11]</statement> <cite>[S11]</cite>  \n\n<statement>CRAG reveals these issues by evaluating performance across five domains, eight question categories, and varying levels of entity popularity and temporal dynamism. For example, it shows significantly lower RAG truthfulness in domains like Finance and Sports, as well as in tasks involving fast-changing or static facts. [S202]</statement> <cite>[S11][S202]</cite>  \n\n<statement>The benchmark also demonstrates that RAG solutions struggle with hallucinations caused by irrelevant retrieval results, particularly in dynamic or low-popularity contexts. This is evidenced by the 20% truthfulness ceiling for RAG systems despite improved accuracy, indicating challenges in filtering noise from retrieval outputs. [S193]</statement> <cite>[S11][S193]</cite>  \n\n<statement>CRAG\u2019s design, including mock APIs for web and KG search, allows systematic analysis of these issues by isolating factors like domain specificity, question complexity, and temporal dynamics, which traditional datasets fail to capture. [S6]</statement> <cite>[S6][S11]</cite>",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the factors that contribute to lower RAG truthfulness in specific domains and question types, and how the CRAG benchmark reveals these issues. \n\nFirst, I need to look through the provided supporting context to find relevant information. The passage mentions several points. In the abstract, it says that CRAG reveals lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity. That's a key point. Then, in the introduction, there's a mention of GPT-4's accuracy being below 15% for fast-changing facts and below 35% for less popular entities. \n\nLooking at the later parts, like in Passage 2, there's a table (Table 5) that discusses the CRAG benchmark's evaluation. It mentions that RAG solutions have higher hallucination rates due to irrelevant retrieval results. Also, in the results section, there's a part about lower RAG truthfulness in Finance and Sports domains. The question types like multi-hop or post-processing also have issues. \n\nThe CRAG benchmark's design, which includes diverse domains and question categories, helps highlight these issues by testing different scenarios. The benchmark's evaluation shows where RAG struggles, such as with dynamic facts, low popularity entities, complex questions, and specific domains. \n\nI need to structure the answer by listing the factors: higher dynamism, lower popularity, higher complexity, specific domains (Finance, Sports), and certain question types (multi-hop, post-processing). Then explain how CRAG reveals these through its diverse testing framework and evaluation results. Make sure to cite the relevant sentences from the passage."
    },
    {
      "question": "How does the CRAG benchmark evaluate the impact of search ranking and knowledge graph (KG) data on QA accuracy and hallucination rates?",
      "answer": "<statement>The CRAG benchmark evaluates the impact of search ranking and knowledge graph (KG) data on QA accuracy and hallucination rates by comparing performance across different tasks, domains, and question types, while simulating web and KG search through mock APIs.<cite>[S6][S194][S196][S202]</cite></statement> <statement>CRAG highlights that adding RAG improves accuracy from 34% (LLM-only) to 44%, but hallucination rates remain high, as RAG solutions introduce more hallucinations from irrelevant retrieval results.<cite>[S191][S193]</cite></statement> <statement>The benchmark demonstrates that KG data improves accuracy with lower hallucination rates compared to web search alone, though the improvement is limited, indicating challenges in leveraging KG effectively.<cite>[S194][S195]</cite></statement> <statement>CRAG reveals that better search ranking (e.g., Task 3) increases truthfulness by retrieving more accurate results, as 30% of ground truths are missed due to context window limitations in lower-ranking results.<cite>[S196][S197]</cite></statement> <statement>Evaluation across domains (e.g., Finance, Sports) and question dynamics (real-time vs. static) shows that CRAG captures variations in accuracy and hallucination rates, emphasizing the role of search ranking and KG data in handling complex or dynamic facts.<cite>[S202][S203]</cite></statement>",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking how the CRAG benchmark evaluates the impact of search ranking and knowledge graph (KG) data on QA accuracy and hallucination rates.\n\nFirst, I need to look through the provided supporting context to find relevant information. The passage mentions CRAG's evaluation in several places. Let me scan through the sentences.\n\nLooking at the abstract part, there's a mention of CRAG being a factual QA benchmark with mock APIs to simulate web and KG search. That's probably relevant. Then, in the introduction, there's a part about RAG solutions and their challenges. But the specific evaluation details might be in the later sections.\n\nIn Passage 2, there's a lot of information. For example, Table 5 discusses evaluation results. Sentence [S191] talks about LLM-only solutions having 34% accuracy and 20% truthfulness. Then [S192] mentions RAG solutions getting up to 44% accuracy. But [S193] points out that RAG solutions don't exceed 20% truthfulness because of hallucinations from irrelevant data. \n\nThen [S194] states that KG knowledge helps improve accuracy with lower hallucination rates. [S195] mentions that the improvement from KG is mediocre, indicating a challenge. [S196] discusses Task 3 having higher truthfulness due to better search ranking and recall. [S197] notes that 30% of ground truths are in web results but not in the prompt due to context limits, highlighting search ranking's importance. \n\nAlso, Figure 3 and the tables show results across domains, dynamism, popularity, and question types. The evaluation looks at different factors like domain (Finance, Sports), dynamism (real-time vs. static), popularity (head vs. tail entities), and question types (simple, multi-hop, etc.). \n\nSo, putting this together, CRAG evaluates by comparing accuracy and hallucination rates across different scenarios. It uses mock APIs for web and KG searches, tests various domains and question types, and analyzes how factors like search ranking and KG data affect performance. The citations would be the sentences that directly mention these evaluations. \n\nI need to make sure each statement is correctly cited. For example, the part about KG helping accuracy with lower hallucination is [S194], and the search ranking importance is [S197]. The comparison between tasks (Task 2 and 3) is in [S195] and [S196]. The mention of different domains and question types is in [S202] and others. \n\nI should structure the answer by explaining the evaluation methods, citing the relevant sentences. Each point should be a separate statement with appropriate citations."
    },
    {
      "question": "What is the significance of the CRAG benchmark in advancing RAG research, and what resources are provided to support its use?",
      "answer": "<statement>The CRAG benchmark is significant in advancing RAG research as it addresses the limitations of existing datasets by providing a comprehensive factual question-answering benchmark with 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search, reflecting real-world diversity and dynamism across five domains and eight question categories.<cite>[S6][S7]</cite></statement> <statement>CRAG highlights critical gaps in current RAG systems, such as the limited accuracy improvement from straightforward RAG implementations (44% vs. 34% for LLMs) and the challenge of reducing hallucinations, while also revealing lower accuracy for dynamic, less popular, or complex facts, which guides future research directions.<cite>[S8][S10][S11]</cite></statement> <statement>Resources provided to support CRAG include its open-source availability on GitHub, a KDD Cup 2024 challenge to foster community engagement, and detailed evaluation metrics from industry-state-of-the-art RAG systems, such as truthfulness scores and latency data, to benchmark progress.<cite>[S12][S14][S204][S205]</cite></statement>",
      "chunk": "Passage 1:\n[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.\n\nPassage 2:\n[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
      "chunk_list": [
        "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
        "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the significance of the CRAG benchmark in advancing RAG research and the resources provided to support its use.\n\nFirst, I need to look through the supporting context given. The passage starts with the abstract of the CRAG benchmark. From the abstract, I see that CRAG is introduced to address the gap in existing RAG datasets not representing real-world QA tasks adequately. It mentions CRAG has 4,409 question-answer pairs and mock APIs to simulate web and KG search. Also, it's designed to cover five domains and eight categories, with varied entity popularity and temporal dynamics. The evaluation shows that even with RAG, accuracy only reaches 44%, and state-of-the-art solutions have 63% accuracy without hallucination. Additionally, CRAG is linked to a KDD Cup 2024 challenge and is available on GitHub.\n\nIn the introduction, there's mention of RAG's challenges like selecting relevant info, reducing latency, and synthesizing info. The passage also talks about the importance of search ranking in RAG and the challenges in leveraging KG data.\n\nLooking at the other parts, Table 5 and 6 discuss evaluation results, showing CRAG's non-trivial nature and the performance of different systems. The resources mentioned include the GitHub repository and the KDD Cup challenge.\n\nSo, the significance of CRAG is that it provides a comprehensive benchmark with diverse data, highlighting gaps in current RAG systems and guiding future research. The resources include the dataset itself, mock APIs, the KDD Cup challenge, and the GitHub repository.\n\nNow, I need to structure the answer with citations. The key points are:\n\n1. Significance: Comprehensive benchmark with 4,409 QA pairs, mock APIs, diverse domains/categories, highlighting accuracy gaps (citations S4-S14, S19-S20, S204-S206).\n2. Resources: GitHub link (S14), KDD Cup 2024 (S12), and the dataset with mock APIs (S6).\n\nI need to make sure each statement is correctly cited. For example, the mention of 4,409 pairs is in S6. The KDD Cup is in S12. The GitHub link is S14. The evaluation results and challenges are in S4-S10, S19-S20, S204-S206.\n\nI should check each sentence to ensure the citations are accurate. Also, avoid any markdown and follow the specified format."
    },
    {
      "question": "Which paper introduced the Symphony framework for natural language query answering over multi-modal data lakes, and how does the accuracy criteria outlined in the second passage relate to evaluating its effectiveness?",
      "answer": "<statement>The paper that introduced the Symphony framework for natural language query answering over multi-modal data lakes is cited in [S311]. This reference describes the work titled \"Symphony: Towards natural language query answering over multi-modal data lakes\" by Z. Chen, Z. Gu, L. Cao, J. Fan, S. Madden, and N. Tang, presented at CIDR in 2023.<cite>[S311]</cite></statement> <statement>The accuracy criteria outlined in Passage 2 classify answers into \"Acceptable\" (Accuracy = 2) and \"Accurate\" (Accuracy = 3). These criteria evaluate whether answers are factually correct, complete, and free from significant hallucinations. For Symphony, this framework would assess its effectiveness by measuring how well its responses align with these standards, ensuring accuracy and reliability in natural language query answering over multi-modal data lakes.<cite>[S834][S835][S836][S841][S845]</cite></statement>",
      "chunk": "Passage 1:\n[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.\n\nPassage 2:\n[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
      "chunk_list": [
        "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
        "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking two things: first, which paper introduced the Symphony framework for natural language query answering over multi-modal data lakes, and second, how does the accuracy criteria in the second passage relate to evaluating its effectiveness.\n\nLooking at the supporting context, Passage 1 has a citation [S311] that mentions \"Symphony: Towards natural language query answering over multi-modal data lakes.\" The authors listed here are Z. Chen, Z. Gu, L. Cao, J. Fan, S. Madden, and N. Tang, and it's from CIDR 2023. So that's the paper that introduced Symphony.\n\nNow, the second part is about the accuracy criteria in Passage 2 and how it relates to evaluating Symphony's effectiveness. Passage 2 talks about accuracy levels: Accuracy = 2 (Acceptable) and Accuracy = 3 (Accurate). The criteria mention that acceptable answers are correct but might miss some info or have minor hallucinations, while accurate answers are fully correct with no hallucinations. \n\nThe question is how this accuracy criteria relates to evaluating Symphony. Since the Symphony paper is about NLP query answering, the accuracy criteria from Passage 2 would be used to assess how well Symphony's answers meet these standards. The criteria provide a framework to check if Symphony's responses are accurate, complete, and free from significant errors or hallucinations. The second passage's evaluation metrics (like accuracy, hallucination, and missing answers) would be applied to measure Symphony's performance, ensuring that its answers are both correct and reliable. \n\nI need to make sure the citations are correctly referenced. The first part cites [S311], and the second part refers to the accuracy criteria in Passage 2, which is [S834] to [S847], but the exact sentences about accuracy are in [S834], [S835], [S836], etc. However, the user might want the specific sentences that define the accuracy criteria. For example, [S834] defines Accuracy = 2 and 3, and [S841] mentions Accuracy = 3. Also, the part about truthfulness being accuracy multiplied by hallucination is in [S845]. But the exact relation to Symphony isn't directly stated, so it's inferred based on the evaluation criteria mentioned in Passage 2."
    },
    {
      "question": "How does the second passage define 'minor hallucination,' and which paper from the first passage might require careful handling of such issues during evaluation?",
      "answer": "<statement>The second passage defines 'minor hallucination' as an answer that addresses the user\u2019s question but might be off on some additional details.<cite>[S839]</cite></statement> <statement>The paper by Gao et al. (S347) titled \"LLM-based nlg evaluation: Current status and challenges\" might require careful handling of such issues during evaluation.<cite>[S347]</cite></statement>",
      "chunk": "Passage 1:\n[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.\n\nPassage 2:\n[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
      "chunk_list": [
        "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
        "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking two things: first, how the second passage defines 'minor hallucination,' and second, which paper from the first passage might require careful handling of such issues during evaluation.\n\nStarting with the first part, I need to look at Passage 2. The user mentioned that the second passage defines 'minor hallucination.' Scanning through Passage 2, I see the following sentences:\n\n[S839] \u201cMinor hallucination\u201d means the answer addressed the user\u2019s question but might be off on some additional details.\n\nThat seems to be the direct definition. So the answer here is that a minor hallucination is when the answer addresses the question but has some inaccuracies in additional details. The citation would be [S839].\n\nNow, the second part: which paper from the first passage might require careful handling of such issues during evaluation. The first passage has a list of references, like [S311] Chen et al., [S332] Chung et al., etc. The question is about a paper that deals with issues that need careful handling, possibly related to hallucinations.\n\nLooking through the first passage, I need to find a paper that discusses hallucinations or evaluation challenges. For example, [S373] Huang et al. have a survey on hallucination in large language models. Another one is [S374] which is the same as [S373], maybe? Also, [S347] Gao et al. discuss LLM-based NLG evaluation, current status and challenges. That might be relevant because evaluation challenges could include handling hallucinations.\n\nAlternatively, [S373] mentions a survey on hallucination, which would definitely require careful handling during evaluation. So the paper by Huang et al. (S373) is a candidate. Also, [S347] by Gao et al. might be another. But the question is which one from the first passage. The user might be looking for the one that specifically mentions evaluation challenges. \n\nLooking at the first passage's entries, [S347] is \"LLM-based nlg evaluation: Current status and challenges\" which is about evaluation challenges. That paper would need careful handling of issues like hallucinations during evaluation. So the answer would be [S347].\n\nBut I need to check if there's another one. Also, [S373] is about hallucination in LLMs, which would require handling such issues. So maybe both? But the question says \"which paper,\" so maybe one. The user might expect the one that's directly about evaluation challenges. So [S347] is the answer here.\n\nSo putting it all together, the first part is defined in [S839], and the paper is [S347]."
    },
    {
      "question": "What is the primary purpose of the CRAG benchmark as described in the source context, and what future improvements are planned for it? Provide specific details from the text.",
      "answer": "<statement>The primary purpose of the CRAG benchmark is to advance research in retrieval-augmented generation (RAG) by providing a rich and comprehensive framework for evaluating and improving RAG solutions.<cite>[S231]</cite></statement> <statement>Future improvements for CRAG include expanding the benchmark to handle multi-lingual questions, multi-modal questions, multi-turn conversations, and other evolving research needs, ensuring it remains at the forefront of RAG research and adapts to emerging challenges.<cite>[S233]</cite></statement>",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the primary purpose of the CRAG benchmark and the future improvements planned for it, with specific details from the text.\n\nFirst, I need to look through the provided supporting context. The passage mentions CRAG in several places. Let me scan through the sentences again.\n\nLooking at the first passage, sentence S231 says: \"This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in retrieval-augmented generation (RAG).\" That seems to directly answer the primary purpose. So the purpose is to advance RAG research.\n\nThen, sentence S232 mentions that CRAG reviewed gaps in existing RAG solutions and provided insights for future improvement. But the user is asking about future improvements planned, so maybe that's part of it. However, the specific future improvements are mentioned in S233: \"We plan to continue improving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-turn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to emerging challenges, and evolves for new research needs.\"\n\nSo the future improvements include multi-lingual, multi-modal, and multi-turn conversations. The primary purpose is advancing RAG research, and the future plans are those expansions.\n\nI need to make sure there's no other information in other parts of the context. The rest of the passages talk about test data splits, experiments, and results, which don't directly answer the question. The references and other sections are about acknowledgments and citations, which aren't relevant here.\n\nSo the answer should state that the primary purpose is to advance RAG research, and the future improvements include multi-lingual, multi-modal, and multi-turn conversations. The citations would be S231 for the purpose and S233 for the future improvements."
    },
    {
      "question": "How was the data split for the CRAG benchmark, and what roles do the validation, public test, and private test sets play in the evaluation process?",
      "answer": "<statement>The data for the CRAG benchmark was split into validation (30%), public test (30%), and private test (40%) sets, as stated in the passage. <cite>[S175]</cite></statement> <statement>The validation and public test sets were used for model development, testing, and result reporting, while the private test set was reserved for evaluating submitted solutions. The validation set also supports fine-tuning and validation, and the public test set is used for testing and reporting results. <cite>[S176][S178]</cite></statement>",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about how the data was split for the CRAG benchmark and the roles of the validation, public test, and private test sets in the evaluation process.\n\nFirst, I need to look through the supporting context provided. The user mentioned two passages. Let me check each one. \n\nLooking at Passage 1, there's a lot of acknowledgments and references, but I don't see any information about data splitting here. The numbers in Passage 1 seem to be part of a list or some other data, but not related to the split. \n\nNow, moving to Passage 2. There's a section titled \"Test data split\" starting at [S175]. Let me read that. It says, \"We split the data randomly into validation (30%), public test (30%), and private (40%), and released the validation and public test sets (Appendix A.2.3).\" That answers the first part about the data split. \n\nThen, the next sentences explain the roles. [S176] mentions that participants can use validation and public test sets for development and testing, while submissions are evaluated on the private test set. [S178] adds that future users can use validation for development, fine-tuning, and validation, and public test for testing and reporting. \n\nSo, the validation set is for development and testing, the public test is for testing and reporting, and the private test is for evaluation of submitted solutions. The split is 30%, 30%, and 40% respectively. \n\nI need to make sure there are no other mentions. The rest of Passage 2 talks about experiments and results, which isn't relevant here. The answer should cite [S175] for the split percentages and [S176] and [S178] for the roles. \n\nWait, [S176] says participants use validation and public test for development and testing, and private for evaluation. [S178] clarifies that future users can use validation for development, fine-tuning, validation, and public test for testing and reporting. So the roles are covered in both [S176] and [S178]. \n\nI should structure the answer with two statements: one stating the split and citing [S175], another explaining the roles citing [S176] and [S178]. Let me check if there's any other citation needed. The rest of the context doesn't mention the data split again. \n\nYes, that's all. So the final answer should have two sentences with the appropriate citations."
    },
    {
      "question": "What experimental setup was used to evaluate LLM-only and RAG solutions on CRAG, and how did the context window sizes vary across different models?",
      "answer": "<statement>The experimental setup involved evaluating LLM-only solutions and RAG solutions on the CRAG benchmark, using specific models and context window configurations. For LLM-only solutions, models like Llama 2 Chat, Llama 3 Instruct, Mixtral, Falcon, FLAN-T5, and GPT-4 Turbo were tested on the CRAG public test set with 1,335 questions, employing simple prompts to encourage brief answers and \"I don\u2019t know\" responses when confidence was low.<cite>[S181]</cite></statement> <statement>For web-only RAG solutions (Task 1), fixed-length web context windows were used, varying by model: 1K tokens for Falcon and FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo. KG-based solutions (Tasks 2, 3) additionally used fixed-length KG context windows, with sizes of 0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama 3 Instruct and GPT-4 Turbo.<cite>[S182][S183]</cite></statement>",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the experimental setup used to evaluate LLM-only and RAG solutions on CRAG and how the context window sizes varied across different models.\n\nFirst, I need to look through the supporting context provided. The main passage here is Passage 2, as it discusses the experiment setup. Let me scan through the sentences in Passage 2.\n\nLooking at sentence [S180], it mentions that in the section on benchmarking, they present the performance of LLMs and RAG systems. Then, in [S181], there's a detailed experiment setup. It says they ran LLM-only solutions on the CRAG public test set with 1,335 questions using simple prompts. They also list the models used: Llama 2 Chat, Llama 3 Instruct, Mixtral, Falcon, FLAN-T5, and GPT-4 Turbo. \n\nNext, [S182] talks about web-only RAG solutions (Task 1) using a fixed-length web context window. The sizes are 1K for Falcon and FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo. They concatenated webpage snippets in original order until the window was filled. \n\nThen [S183] discusses KG-based solutions (Tasks 2, 3) with a fixed-length KG context window. The sizes here are 0.5K for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama 3 Instruct and GPT-4 Turbo. They used llama-3-8B-instruct for extracting entities and concatenated results from mock APIs. \n\nSo, the experimental setup involved different models and context window sizes. The answer needs to mention the models used for LLM-only and RAG, and the varying context window sizes. The citations would be [S181], [S182], [S183]. \n\nWait, the question also asks about how context window sizes varied across models. The answer should specify each model's context window. For example, Falcon and FLAN-T5 had 1K for web and 0.5K for KG, while Llama 3 Instruct had 4K web and 2K KG. \n\nI need to structure the statements properly, each with their citations. Let me check if there are other sentences that might be relevant. [S184] mentions the best-performing models but doesn't add to the setup. [S185] is about the section focus. So the key sentences are [S181], [S182], [S183]. \n\nSo the first statement would describe the experimental setup, citing [S181]. The second statement about context windows would cite [S182] and [S183]. Maybe split into two sentences if needed. Let me make sure each part is covered."
    },
    {
      "question": "According to the performance metrics in Table 5, what are the highest accuracy rates achieved by LLM-only solutions and straightforward RAG solutions, and how do these compare across tasks?",
      "answer": "<statement>The highest accuracy rates achieved by LLM-only solutions and straightforward RAG solutions are 34% and 44%, respectively, according to Table 5.<cite>[S187]</cite></statement> <statement>These accuracy rates vary across tasks, with LLM-only solutions achieving up to 34% and RAG solutions reaching 44% in specific tasks, though the exact task-specific comparisons are detailed in Table 5.<cite>[S187]</cite></statement>",
      "chunk": "Passage 1:\n[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.\n\nPassage 2:\n[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
      "chunk_list": [
        "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
        "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the highest accuracy rates from LLM-only and straightforward RAG solutions according to Table 5, and how they compare across tasks.\n\nFirst, I need to look at the supporting context provided. The relevant part is in Passage 2, specifically Table 5. The passage mentions that Table 5 shows the performance of straightforward RAG solutions. The key sentence here is [S187], which states, \"LLM-only solutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy.\" That directly answers the first part of the question.\n\nNow, the second part is about how these compare across tasks. The table in [S186] probably breaks down the accuracy by tasks. However, the exact details of the tasks (like Task 1, Task 2, Task 3) aren't fully described in the provided text. But the mention of \"Task 1\", \"Task 2\", and \"Task 3\" in the passage suggests that the accuracy varies per task. For example, in the table, Llama 3 70B Instruct and GPT-4 Turbo have different accuracy rates across tasks. However, without the full table data, I can only rely on the information given. The passage does mention that the highest accuracy for LLM-only is 34% and for RAG is 44%, but the specific task comparisons aren't detailed beyond that. \n\nSo, the answer should cite [S187] for the highest accuracy rates. The comparison across tasks is implied by the mention of different tasks in the table, but since the exact numbers per task aren't provided, the answer should focus on the highest values stated."
    },
    {
      "question": "What design decision was made regarding the evaluation of first-stage retrieval candidate pool construction in the CRAG benchmark, and how does this relate to the dataset's construction methodology?",
      "answer": "<statement>The design decision regarding the evaluation of first-stage retrieval candidate pool construction in the CRAG benchmark was to exclude direct evaluation of this task, ensuring the competition remained challenging yet achievable within the KDD Cup\u2019s three-month timeframe. This decision is explicitly stated in the passage, which notes that the three tasks in the benchmark do not directly evaluate the construction of a first-stage retrieval candidate pool [S950][S951].<cite>[S950][S951]</cite></statement> <statement>This design choice relates to the dataset's construction methodology by providing users with a comprehensive corpus of 220K web pages to build their own retrievers, even though the benchmark tasks do not focus on this aspect. The dataset allows for fair comparisons and manageable costs by offering this union of web pages as a resource for retrieval system development [S952][S953].<cite>[S952][S953]</cite></statement>",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the design decision regarding the evaluation of first-stage retrieval candidate pool construction in the CRAG benchmark and how it relates to the dataset's construction methodology.\n\nFirst, I need to look through the supporting context provided. The main passage mentions something about the three tasks in the benchmark not directly evaluating the construction of a first-stage retrieval candidate pool. That's in sentence S950. The next sentence, S951, says this design decision ensures the competition is challenging yet achievable within the three-month timeframe. \n\nSo the design decision was not to have the tasks directly evaluate the first-stage retrieval candidate pool. Instead, they focused on other aspects. Then, the dataset's construction methodology, as per the context, involves creating QA pairs from both KGs and web contents. The dataset includes web search and KG search contents for retrieval. \n\nBut how does the design decision relate to the dataset's construction? The dataset allows users to use the union of all 220K web pages as a corpus to build a retriever, even though the tasks don't directly evaluate the candidate pool. This suggests that while the tasks don't focus on that specific aspect, the dataset provides the necessary resources for users to handle it themselves. \n\nI need to make sure I'm citing the correct sentences. The key points are in S950 and S951 for the design decision, and the dataset construction is covered in passages about QA pairs and retrieval contents. However, the direct relation is mentioned in S952 and S953, where they mention the corpus available for building a retriever. \n\nSo the answer should state that the design decision was not to evaluate the first-stage retrieval candidate pool directly, allowing for flexibility in dataset construction, and the dataset provides a corpus for users to build their own retrievers. Citations would be S950, S951, S952, S953."
    },
    {
      "question": "How does the latency measurement for Perplexity.ai differ between API calls and web interface interactions, and what steps were taken to ensure fair comparisons across systems?",
      "answer": "<statement>The latency measurement for Perplexity.ai differs between API calls and web interface interactions because API call latency (2,455ms) and web interface latency are not directly comparable, necessitating separate measurements for each method.<cite>[S946][S947]</cite></statement> <statement>To ensure fair comparisons across systems, on-device components like Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) were excluded for Meta SG, and standardized parameters such as conversation style and temperature were set for all systems.<cite>[S941][S943]</cite></statement> <statement>Additionallly, latency measurements were conducted on specific dates for each system to control for temporal variations, and base LLMs were selected consistently across platforms.<cite>[S942][S944]</cite></statement>",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the difference in latency measurement between Perplexity.ai's API calls and web interface interactions, and the steps taken to ensure fair comparisons across systems.\n\nFirst, I need to look through the supporting context provided. The main passage here is Passage 1. Let me scan through the sentences. \n\nLooking at the sentences, I see that in [S945], there's a section titled \"A.4.2 Latency\". The next sentences mention measuring latency by the time difference between query submission and response. Then [S946] states that Perplexity.ai's API latency is 2,455ms. \n\nThen [S947] says that API call latency and web interface interactions aren't directly comparable, so they called Perplexity.ai via the web interface and reported that latency in Table 6. Also, [S948] notes that this web interface latency might not match the API accuracy numbers. \n\nFor the fair comparison steps, [S941] mentions excluding on-device components like ASR and TTS for Meta SG to ensure answer quality wasn't affected. Also, [S942] talks about the dates they used for each system, and [S943] and [S944] mention setting specific parameters like conversation style and temperature. \n\nSo, the key points are: API latency is 2455ms, web interface latency is different and measured separately. Steps for fairness include excluding on-device components, standardizing parameters, and using specific dates. \n\nI need to structure the answer with these points, citing the relevant sentences. Make sure each statement has the correct citations. Check if all necessary info is covered and citations are accurate."
    },
    {
      "question": "What is the significance of including both web search and KG search contents in the CRAG retrieval tasks, and how does this impact the evaluation of RAG systems?",
      "answer": "<statement>The inclusion of both web search and KG search contents in CRAG retrieval tasks simulates practical scenarios for RAG systems, ensuring they can handle diverse information sources and evaluate their robustness in real-world conditions.<cite>[S130]</cite></statement> <statement>This dual inclusion allows testing whether RAG solutions can admit \"I don\u2019t know\" when retrieval results lack necessary information, addressing limitations in coverage and ensuring systems are evaluated on their ability to handle incomplete data.<cite>[S141]</cite></statement> <statement>The evaluation benefits from comparing recall rates, as web search achieves 85% overall recall, while KG questions show significantly lower recall (74% vs. 93% for web questions), highlighting the importance of leveraging KGs for specific tasks and assessing system adaptability.<cite>[S136][S143][S144]</cite></statement> <statement>By incorporating both data types, CRAG emphasizes the necessity of extracting and understanding HTML contents, as full web pages increase recall by ~20% compared to snippets, reflecting the practical challenges of information retrieval.<cite>[S142]</cite></statement>",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the significance of including both web search and KG search contents in the CRAG retrieval tasks and how this affects the evaluation of RAG systems.\n\nFirst, I need to look through the provided supporting context to find relevant information. The passage mentions that CRAG includes two types of contents for retrieval: web search and KG search. The user wants to know why both are included and the impact on evaluating RAG systems.\n\nLooking at the context, in Passage 2, there's a section titled \"3.2 Contents for retrieval\" which states: \"We included two types of contents for retrieval to simulate the practical scenario for RAG: web search and KG search.\" This seems directly relevant. The significance here is that including both types simulates real-world scenarios where RAG systems might need to access both web and knowledge graph data.\n\nAnother part mentions that web search results are estimated to contain the ground truth facts for 85% of questions. It also notes that full web pages increase recall by about 20% compared to snippets, highlighting the importance of HTML content. Additionally, there's a mention that KG questions have lower recall (74% vs. 93% for web questions), which underscores the need for KGs in certain tasks. This suggests that evaluating RAG systems with both data sources tests their ability to handle different information types and their effectiveness in different scenarios.\n\nThe passage also talks about the design of the benchmark. It says that including both types allows testing whether RAG solutions can handle cases where retrieval doesn't have necessary info (like saying \"I don't know\"). This impacts evaluation by making sure systems are robust and can manage incomplete data.\n\nSo, putting this together, the significance is simulating real-world scenarios, testing robustness, and evaluating effectiveness across different data sources. The impact on evaluation is more comprehensive testing, ensuring systems can handle both web and KG data, and assessing their ability to manage varying recall rates and data types.\n\nNow, I need to cite the relevant sentences. The key sentences are in Passage 2, specifically [S130] about including both types for practical scenarios, [S136] and [S143] about recall rates and the importance of HTML content, and [S144] regarding KG questions' lower recall. Also, [S141] mentions testing \"I don't know\" responses. \n\nWait, the user's question has two parts: significance and impact. The significance is why both are included, and the impact is how it affects evaluation. The answer should address both. The citations should be from the sentences that explain these points."
    },
    {
      "question": "How were the question-answer pairs in CRAG constructed, and what distribution metrics are provided for entities in KG-based questions?",
      "answer": "<statement>The question-answer (QA) pairs in CRAG were constructed from both knowledge graphs (KGs) and web contents. For KG-based QA pairs, entities were collected from publicly available data, and 600+ question templates were created based on selected entity types and relations. Entities with different popularities (head, torso, and tail) were sampled from KGs to fill these templates and generate questions and answers.<cite>[S120][S121]</cite></statement> <statement>For KG-based questions, distribution metrics include the counts of entities categorized as head, torso, and tail, with 661, 658, and 665 KG questions containing these entities, respectively. Additionally, the distribution of questions across dimensions (e.g., fast-changing facts) allows for metrics with a 5% margin of error (95% confidence level), reflecting domain-specific characteristics like real-time requirements in Finance.<cite>[S124][S126][S127]</cite></statement>",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking how the question-answer pairs in CRAG were constructed and what distribution metrics are provided for entities in KG-based questions.\n\nFirst, I need to look through the supporting context provided. The passage has two main sections. Let me start by scanning Passage 2 because the question mentions CRAG and KG-based questions. \n\nIn Passage 2, there's a section titled \"3.1 Question answering pairs\". The first sentence says CRAG covers five domains and eight question types, all in English. Then, under \"QA pairs constructed from KGs,\" there's a mention of collecting entities from publicly available data and creating question templates. Specifically, sentence [S120] states they collected entities and created 600+ templates. Then, they sampled entities with different popularities (head, torso, tail) from KGs to fill templates. \n\nAnother part in Passage 2, sentence [S124] talks about QA pairs from web contents, where annotators wrote questions based on web search results. But the user is asking about KG-based questions, so I should focus on the KG part. \n\nLooking at the distribution metrics for entities in KG-based questions, there's a mention in [S124] that they collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658, and 665 KG Questions containing head, torso, and tail entities respectively. Also, [S126] mentions the distribution across dimensions and that the size of each dimension slice allows for metrics with a 5% margin of error. Additionally, [S127] talks about dynamism distribution reflecting the domain's nature, like more real-time questions in Finance. \n\nI need to make sure I'm citing the correct sentences. The construction part is covered in [S120] and [S121]. The distribution metrics are in [S124], [S126], and [S127]. \n\nWait, the user also asked about distribution metrics for entities in KG-based questions. The part about head, torso, tail entities in [S124] is directly relevant. Also, [S126] mentions the distribution across dimensions and the margin of error. [S127] discusses dynamism distribution. \n\nSo, putting it all together: the QA pairs were constructed from KGs by collecting entities and creating templates, then sampling entities with different popularities. The distribution metrics include the counts of head, torso, tail entities and the dynamism distribution. \n\nI need to structure the answer with statements and citations. Each sentence in the answer should have the relevant citations. Let me check again to ensure I didn't miss any key points."
    },
    {
      "question": "What trade-offs were considered in the CRAG benchmark's design to balance challenge and feasibility, and how does this affect dataset usage?",
      "answer": "<statement>The CRAG benchmark's design considered trade-offs by excluding the evaluation of first-stage retrieval candidate pool construction, ensuring the competition remains challenging yet achievable within a three-month timeframe. This decision balances complexity with feasibility for participants.<cite>[S950][S951]</cite></statement> <statement>The dataset limits the corpus to 220K web pages, allowing fair comparisons and manageable costs while acknowledging it does not cover the entire web. This trade-off ensures practical usability without overwhelming users with excessive data.<cite>[S952][S953]</cite></statement> <statement>By focusing on tasks that avoid overly complex retrieval challenges, the benchmark enables participants to prioritize other aspects of RAG systems, such as answer accuracy and latency, while still providing a scalable dataset for research.<cite>[S951][S953]</cite></statement>",
      "chunk": "Passage 1:\n[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20\n\nPassage 2:\n[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
      "chunk_list": [
        "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20",
        "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the trade-offs considered in the CRAG benchmark's design to balance challenge and feasibility, and how that affects dataset usage. \n\nFirst, I need to look through the provided supporting context. The passage mentions a few key points. In Passage 1, there's a section under \"A.5 Limitations\" where it says that the three tasks in the benchmark don't directly evaluate the construction of a first-stage retrieval candidate pool. This is mentioned in [S950]. The next sentence [S951] states that this design decision ensures the competition remains challenging yet achievable within the KDD Cup's three-month timeframe. So that's a trade-off: not evaluating a complex task (retrieval candidate pool) to keep the challenge manageable.\n\nThen, [S952] mentions that users can use the union of all 220K web pages as a corpus to build a retriever, even though the dataset doesn't cover the entire web. This allows for fair comparisons and manageable costs, which is another trade-off. They're limiting the corpus size to make it feasible for participants but still providing enough data for meaningful comparisons.\n\nLooking at Passage 2, there's information about the dataset's construction. For example, the QA pairs are made from both KGs and web contents, which might relate to balancing different data sources. However, the specific trade-offs about challenge and feasibility are more directly addressed in Passage 1's limitations section.\n\nSo, the main trade-offs are: not including the first-stage retrieval candidate pool construction to keep the task feasible, and limiting the corpus size to 220K web pages for manageable costs. These choices affect dataset usage by making the benchmark more accessible but possibly less comprehensive.\n\nI need to make sure the citations are correctly referenced. The sentences about the limitations are in [S950] and [S951], and the part about the corpus is [S952]. Also, [S953] mentions the manageable costs. So the citations would be [S950][S951][S952][S953]. \n\nWait, the user's question also asks how this affects dataset usage. The answer should connect the trade-offs to the dataset's usage. For example, by not including the retrieval candidate pool, the dataset focuses on other aspects, and by limiting the corpus size, it's more feasible for users but might require them to use external data for certain tasks. \n\nI need to structure the answer with statements citing the relevant sentences. Each statement should be in <statement> tags with citations. Let me check again to ensure I didn't miss any relevant parts."
    },
    {
      "question": "What is the primary purpose of Task 3: End-to-end RAG, and how does it differ from Task 1 and Task 2 in terms of retrieval challenges?",
      "answer": "<statement>The primary purpose of Task 3: End-to-end RAG is to test how a RAG system ranks a larger number of retrieval results, as it provides 50 web pages instead of 5, increasing the likelihood of noise while ensuring necessary information is available.<cite>[S101][S102][S103]</cite></statement> <statement>Task 3 differs from Task 1 and Task 2 in retrieval challenges by expanding the number of web pages from 5 (Task 1) to 50, requiring more robust ranking mechanisms to handle increased noise, whereas Task 1 focuses on answer generation with limited retrieval and Task 2 integrates structured data via mock APIs alongside web results.<cite>[S92][S95][S96][S101]</cite></statement>",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the primary purpose of Task 3: End-to-end RAG and how it differs from Task 1 and Task 2 in terms of retrieval challenges.\n\nFirst, I need to look through the supporting context provided. The passage mentions three tasks: Task 1, Task 2, and Task 3. Let me find the relevant sections.\n\nLooking at the passage, Task 3 is mentioned in sentences S100 to S104. Specifically, S100 says \"Task 3: End-to-end RAG.\" Then S101 states that Task 3 provides both web search results and mock APIs but with 50 web pages instead of 5. S102 mentions that the larger set of web pages is more likely to have necessary info but also more noise. S103 says Task 3 tests how a RAG system ranks a larger number of retrieval results. \n\nSo the primary purpose of Task 3 is to test the ranking of a larger number of retrieval results, dealing with more noise. \n\nNow, comparing to Task 1 and Task 2. Task 1 (S92) provides up to five web pages, focusing on answer generation. Task 2 (S95) adds mock APIs for structured data. The difference is that Task 3 increases the number of web pages from 5 to 50, increasing retrieval challenges due to more noise and the need for better ranking. \n\nI need to make sure the citations are correct. The sentences about Task 3 are S100, S101, S102, S103. For Task 1 and Task 2, S92, S93, S95, S96, S97, S99. \n\nSo the answer should state that Task 3's purpose is to test ranking with more retrieval results, and the difference is the increased number of web pages (50 vs 5) leading to more noise and challenges in retrieval."
    },
    {
      "question": "According to the dynamism categories, what distinguishes 'Real-time' questions from 'Fast-changing' questions, and which domains have the highest concentration of Real-time questions?",
      "answer": "<statement>According to the dynamism categories, 'Real-time' questions are distinguished by answers that change over seconds, such as stock prices, while 'Fast-changing' questions have answers that change no more than daily, such as event schedules.<cite>[S724][S725]</cite></statement> <statement>Finance and Sports domains have the highest concentration of Real-time questions.<cite>[S111]</cite></statement>",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the differences between 'Real-time' and 'Fast-changing' questions according to the dynamism categories, and which domains have the highest concentration of Real-time questions.\n\nFirst, I need to look through the supporting context provided. The passage mentions Table 3 and Table 4, but the key definitions are in Passage 2. Specifically, looking at Table 7 in Passage 2, which defines the dynamism categories. \n\nIn the context, there's a section labeled \"A.1.2 Definition of dynamism categories\" with Table 7. The entries for Real-time and Fast-changing are there. The Real-time definition says the answer changes over seconds, like \"What\u2019s Costco\u2019s stock price today?\" Fast-changing is defined as changing no more than daily, such as \"When is Laker\u2019s game tonight?\" So the main distinction is the frequency of change: Real-time is every second, Fast-changing is daily or less.\n\nThen, the user also asks which domains have the highest concentration of Real-time questions. Looking back at Table 3 in the passage, it states that Finance and Sports domains have the most Real-time and Fast-changing questions. The exact sentence is [S111]: \"Finance and Sports domain have the most Real-time and Fast-changing questions.\"\n\nSo the answer should state that Real-time questions change every second, Fast-changing daily, and Finance and Sports have the most Real-time. Citations would be [S724] and [S725] for the definitions, and [S111] for the domains."
    },
    {
      "question": "What is the role of mock APIs in Task 2: KG and Web Retrieval Augmentation, and what types of information do they provide?",
      "answer": "<statement>The role of mock APIs in Task 2: KG and Web Retrieval Augmentation is to provide access to structured data from underlying mock knowledge graphs (KGs), enabling RAG systems to query structured data sources and synthesize information from multiple sources.<cite>[S96][S97][S99]</cite></statement> <statement>Mock APIs provide structured data relevant to the questions, which may or may not exist in the mock KGs, and they take input parameters parsed from the question to deliver this structured information.<cite>[S97][S98]</cite></statement>",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the role of mock APIs in Task 2: KG and Web Retrieval Augmentation, and what types of information they provide. \n\nFirst, I need to look through the supporting context provided. The passage mentions Task 2 in several places. Let me scan through the sentences again.\n\nLooking at the passage, specifically sentences S95 to S99. S95 says \"Task 2: KG and Web Retrieval Augmentation.\" Then S96 mentions \"mock APIs to access information from underlying mock KGs.\" S97 states that the mock KGs store structured data relevant to the questions. S98 explains that the mock APIs take input parameters, often parsed from the question, and provide structured data from the mocked KGs. S99 says this task tests how well a RAG system queries structured data sources and synthesizes information from different sources.\n\nSo the role of mock APIs here is to provide access to structured data from the mock KGs. The information they provide is structured data relevant to the questions. The answer should mention that mock APIs enable querying structured data sources (like the KGs) and that they supply structured data based on parameters from the questions. The citations would be S96, S97, S98, and S99. \n\nWait, the user also asked for the types of information. The passage says the mock KGs store structured data relevant to the questions. So the types are structured data, which could include entities, relationships, etc., but the exact details might not be specified beyond that. The answer should note that the information may or may not exist in the KGs, but the APIs provide whatever is there. \n\nI need to make sure each statement is correctly cited. Let me structure the answer with two sentences: one about the role and one about the information types. Each with their citations."
    },
    {
      "question": "How were multi-hop questions constructed, and what is an example of a multi-hop question template derived from the KG traversal process?",
      "answer": "<statement>Multi-hop questions were constructed by sampling an entity e1 from the KG and selecting two relation triplets following a two-hop path: pe1, r1, e2q and pe1, r2, e3q. This process involved creating a question template that describes the path, ensuring the answer could be derived from the structured data in the KG.<cite>[S717][S718][S719]</cite></statement> <statement>An example of a multi-hop question template is \"who is the CEO of the parent company of [company2]?,\" derived from a two-hop traversal where the first hop identifies the parent company (e2) and the second hop retrieves the CEO (e3) from the KG.<cite>[S720]</cite></statement>",
      "chunk": "Passage 1:\n[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)\n\nPassage 2:\n[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
      "chunk_list": [
        "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
        "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey"
      ],
      "thinking": "Okay, let's tackle this question. The user is asking how multi-hop questions were constructed and wants an example of a multi-hop question template derived from the KG traversal process. \n\nFirst, I need to look through the supporting context provided. The passage has two parts, and I should check both. Let me start by scanning for mentions of multi-hop questions.\n\nIn Passage 1, there's a mention under [S84] about questions that require chaining multiple pieces of information, like \"who acted in Ang Lee\u2019s latest movie?\" But that's more of an example, not the construction method. Then, in Passage 2, looking at [S717], it says: \"Finally, we created multi-hop questions in three steps, similar to those described in [31].\" That's a start. Then [S718] mentions sampling an entity e1 from the KG and selecting two relation triplets following a two-hop path. So the process involves selecting entities and relations in a knowledge graph.\n\nNext, [S719] says they created a question template describing the path. The example given is \"who is the CEO of the parent company of [company2]?\". This template is based on a two-hop path: company1's parent and then company1's CEO. The answer would be e3 from the second triplet. \n\nSo the construction involves selecting entities and relations in a KG, creating a path, and then formulating a question template around that path. The example provided in the context is the CEO question. \n\nI need to make sure I cite the correct sentences. The construction steps are in [S717]-[S719], and the example is in [S720]. Also, [S718] explains the two-hop path. \n\nSo the answer should explain that multi-hop questions were constructed by sampling entities and relations in a KG, creating a path, and then forming a template. The example is the CEO question. Citations would be [S717][S718][S719][S720]."
    },
    {
      "question": "What is the signal-to-noise ratio of the mock knowledge graphs (KGs) used in CRAG, and how does this ratio affect the testing of RAG systems?",
      "answer": "<statement>The signal-to-noise ratio of the mock knowledge graphs (KGs) used in CRAG is less than 1/30.<cite>[S759]</cite></statement> <statement>This low signal-to-noise ratio introduces significant noise into the data, simulating real-world conditions where RAG systems must filter out irrelevant information to extract accurate answers.<cite>[S758]</cite></statement>",
      "chunk": "Passage 1:\n[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID.",
      "chunk_list": [
        "[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID."
      ],
      "thinking": "Okay, let's tackle this question. The user is asking about the signal-to-noise ratio of the mock knowledge graphs (KGs) used in CRAG and how this ratio affects the testing of RAG systems.\n\nFirst, I need to find the relevant part of the supporting context. The passage mentions \"mock KGs\" in several places. Looking through the sentences, I see that sentence [S759] states: \"Our mock KGs contain about 2.6M entities and have a signal-to-noise ratio of less than 1/30.\" That directly answers the first part of the question about the ratio.\n\nNext, how does this ratio affect the testing of RAG systems? The surrounding sentences might explain this. The previous sentence [S758] says the mock KGs contain both necessary information and noises with similar entity or attribute names, simulating real settings. The sentence after [S759] is cut off, but the context implies that the low signal-to-noise ratio introduces more noise, making it harder for RAG systems to extract the correct information. This would test the system's ability to handle real-world data where noise is prevalent.\n\nSo, the answer should state the ratio is less than 1/30 and that this high noise level challenges RAG systems' effectiveness in filtering relevant information. The citations would be [S759] for the ratio and [S758] for the effect on testing."
    }
  ],
  "all_chunks": [
    "[S1] 4\n2\n0\n2\n\nv\no\nN\n1\n\n]\nL\nC\n. [S2] s\nc\n[\n\n2\nv\n4\n4\n7\n4\n0\n. [S3] 6\n0\n4\n2\n:\nv\ni\nX\nr\na\n\nCRAG \u2013 Comprehensive RAG Benchmark\n\nXiao Yang\u02da1, Kai Sun\u02da1, Hao Xin\u02da3, Yushi Sun\u02da3, Nikita Bhalla1, Xiangsen Chen4, Sajal\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\nand Xin Luna Dong1\n\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\n\nAbstract\n\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\ntion to alleviate Large Language Model (LLM)\u2019s deficiency in lack of knowledge. [S4] Existing RAG datasets, however, do not adequately represent the diverse and dy-\nnamic nature of real-world Question Answering (QA) tasks. [S5] To bridge this gap, we\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\nand Knowledge Graph (KG) search. [S6] CRAG is designed to encapsulate a diverse\narray of questions across five domains and eight question categories, reflecting\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\nfrom years to seconds. [S7] Our evaluation of this benchmark highlights the gap to\nfully trustworthy QA. [S8] Whereas most advanced LLMs achieve \u010f 34% accuracy\non CRAG, adding RAG in a straightforward manner improves the accuracy only\nto 44%. [S9] State-of-the-art industry RAG solutions only answer 63% of questions\nwithout any hallucination. [S10] CRAG also reveals much lower accuracy in answer-\ning questions regarding facts with higher dynamism, lower popularity, or higher\ncomplexity, suggesting future research directions. [S11] The CRAG benchmark laid the\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\nand submissions. [S12] We commit to maintaining CRAG to serve research communities\nin advancing RAG solutions and general QA solutions. [S13] CRAG is available at\nhttps://github.com/facebookresearch/CRAG/. [S14] 1\n\nIntroduction\n\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. [S15] Despite the advancements,\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\nfactual accuracy or grounding [14,27,30,32]. [S16] Studies have shown that GPT-4\u2019s accuracy in answering\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\nchanging) facts, GPT-4\u2019s accuracy in answering questions referring to torso-to-tail (less popular)\nentities is below 35% [29]. [S17] Overcoming hallucinations thus becomes a priority in building reliable\nQA systems [13, 14]. [S18] Retrieval-Augmented Generation (RAG) [6, 8, 12, 19] has recently emerged as a promising solution to\nalleviate LLM\u2019s deficiency in lack of knowledge and attracted a lot of attention from both academia\nresearch and industry. [S19] Given a question, a RAG system searches external sources to retrieve relevant\ninformation and then provides grounded answers [7, 12, 19] (see Figure 1 for an illustration). [S20] Despite\n\n\u02daEqual contribution. [S21] Correspondence to: Xiao Yang (xiaoyangfb@meta.com). [S22] 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. [S23] Figure 1: QA using LLMs (a) without RAG vs. [S24] (b) with RAG. [S25] its potential, RAG still faces many challenges, such as selecting the most relevant information,\nreducing question answering latency, and synthesizing information to answer complex questions.",
    "[S26] A comprehensive benchmark is currently missing to advance continued research efforts in this field. [S27] Our goal is to build a benchmark that can provide a holistic view of the important capabilities and\nfast but reliable evaluation for RAG to propel the area forward. [S28] What is a good benchmark for QA\nover LLMs? [S29] We consider five critical features. [S30] 1. [S31] Realism: First and foremost, a good benchmark shall best reflect real use cases. [S32] In other\nwords, a solution that achieves high metrics in the benchmark shall also perform very well in\nreal scenarios. [S33] For example, the questions in a RAG benchmark shall be similar to questions\npeople ask in real-world QA scenarios. [S34] 2. [S35] Richness: The benchmark shall contain a diverse set of instance types, covering both\ncommon use cases and some complex and advanced use cases, to represent real-world\nchallenges and reveal possible limitations of existing solutions. [S36] 3. [S37] Insightfulness: The benchmark shall allow for an easy understanding of performance on\ndifferent slices of the data, reflecting the capability of the solution in addressing different\ntypes of challenges. [S38] 4. [S39] Reliability: The benchmark shall allow reliable assessment of metrics: the ground truths\nshall be accurate; the metrics shall well capture the performance of the model; the evaluation\nshall be easy and reliable, and the computed metrics shall hold statistical significance. [S40] 5. [S41] Longevity: Finally, to enable research and experimental comparison in a long term, the\nscenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshed\nand improved over time. [S42] We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG \u2013\nComprehensive benchmark for RAG. [S43] Our work makes three contributions. [S44] Our first contribution is the dataset itself (Section 3). [S45] CRAG contains a rich set of 4,409 QA pairs\nfrom five domains: Finance, Sports, Music, Movie, and Open domain. [S46] In addition to simple-fact\nquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions to\ncover real user queries: questions with Conditions, Comparison questions, Aggregation questions,\nMulti-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions. [S47] CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging from\nseconds to years, allowing easy deep dives for insights. [S48] As we generated the questions, we referred\nto smart assistant use cases to make sure the questions are realistic, paraphrased the questions to\nincrease the diversity of expressions, and manually verified ground truths to ensure reliability. [S49] In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range of\navailable information. [S50] This includes up to 50 full HTML pages for each question returned from a\nreal-world search engine\u2014the Brave Search API [5], and mock KGs with 2.6 million entities. [S51] For the\nmock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. [S52] Our second contribution is the evaluation mechanism to allow for reliable comparisons. [S53] We designed\n3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graph\nand web retrieval, and end-to-end retrieval-augmented generation (Section 2). [S54] Instead of computing\nthe percentage of correctly answered questions, our score system distinguishes hallucinated answers\nand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user\n\n2\n\nLLMWhat is the gold price today?Gold price is at $1626.81 per ounce today Oct 21 2022.Gold price is at $2020.8 per ounce today Jan 28 2024.DocumentsWeb SearchReal-time APIsKnowledge GraphRetrieved relevantknowledgeQuestion (a) LLM Direct Generation(b) RAG: Retrieved-Augmented Generation with LLM\fTable 1: Comparing CRAG to existing benchmarks for factual question answering. [S55] Benchmark\n\nQALD-10 [35]\nMS MARCO [4]\nNQ [18]\nRGB [6]\nFreshLLM [36]\nCRAG",
    "[S56] Web\nretrieval\n\u2717\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\nKG\nsearch\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nMock\nAPI\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nDynamic\nquestion\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2713\n\u2713\n\nTorso and\ntail facts\n\u2717\nnot explicitly\nnot explicitly\n\u2717\n\u2717\n\u2713\n\nBeyond\nWikipedia\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\nQuestion\nsize\n\n0.8K\n100K\n323K\n1K\n0.6K\n4.4K\n\ntrust. [S57] We also design an effective automatic evaluation mechanism to allow for fast evaluations and\niterations (Section 4). [S58] Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industry\nstate-of-the-art solutions on RAG (Section 5). [S59] Whereas most advanced LLMs achieve \u010f 34%\naccuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. [S60] State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, still\nhaving much lower accuracy in answering questions regarding facts with higher dynamism, lower\npopularity, or higher complexity. [S61] These evaluations serve two roles: first, they demonstrate that\nCRAG has appropriate level of difficulty and allows insights drawn from different dimensions of\ndiversities the benchmark has incorporated; second, they highlight the gaps and research directions to\na fully trustworthy QA system. [S62] The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousands\nof participants and submissions within the first 50 days of the competition. [S63] We commit to maintaining\nCRAG to serve research communities in advancing RAG solutions and general QA solutions. [S64] Comparison with existing benchmarks. [S65] Table 1 compares CRAG with existing benchmarks for\nfactual question answering. [S66] Traditional QA benchmarks such as Natural Questions (NQ) [18],\nTriviaQA [16], MS MARCO [4], and QALD-10 [35] have advanced QA in the past decade but\nconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverse\nand dynamic challenges that RAG is facing. [S67] New benchmarks for LLM or RAG usually target certain\ncapabilities of the QA system. [S68] Researchers created benchmarks to evaluate how well the systems\ncan answer simple knowledge questions [23, 29, 30] and handle more advanced scenarios. [S69] These\ninclude answering questions with changing answers [36], integrating information from multiple\ndocuments [6], addressing multi-hop questions [33], and answering questions with long texts [26]. [S70] Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE [21]\nor F1 to evaluate the quality of the responses [16, 18]. [S71] These metrics, although working well for\nextractive methods, are known to not perform very effectively for LLMs that generate free-form\nresponses [11]. [S72] Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:\ncomprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact\npopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. [S73] These features make\nCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providing\na shared testbed to evaluate how these systems handle real-world, dynamic, and diverse information\nretrieval and synthesis challenges for reliable LLM-based question answering. [S74] 2 Problem Description\n\nA RAG QA system takes a question Q as input and outputs an answer A; the answer is generated\nby LLMs according to information retrieved from external sources or directly from the knowledge\ninternalized in the model. [S75] The answer should provide useful information to answer the question\nwithout adding any hallucination. [S76] 2https://www.aicrowd.com/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024\n\n3\n\n\fTable 2: Definition of CRAG question types. [S77] Question type\n\nDefinition\n\nSimple\n\nQuestions asking for simple facts that are unlikely to change overtime, such as the\nbirth date of a person and the authors of a book. [S78] Simple w. [S79] Condition Questions asking for simple facts with some given conditions, such as stock prices\n\non a certain date and a director\u2019s recent movies in a certain genre. [S80] Set",
    "[S81] Comparison\n\nAggregation\n\nMulti-hop\n\nPost-processing-\nheavy\n\nFalse Premise\n\nQuestions that expect a set of entities or objects as the answer (e.g., \u201cwhat are the\ncontinents in the southern hemisphere?\u201d). [S82] Questions that compare two entities (e.g., \u201cwho started performing earlier, Adele or\nEd Sheeran?\u201d). [S83] Questions that require aggregation of retrieval results to answer (e.g., \u201chow many\nOscar awards did Meryl Streep win?\u201d). [S84] Questions that require chaining multiple pieces of information to compose the answer\n(e.g., \u201cwho acted in Ang Lee\u2019s latest movie?\u201d). [S85] Questions that need reasoning or processing of the retrieved information to obtain\nthe answer (e.g., \u201chow many days did Thurgood Marshall serve as a Supreme Court\njustice?\u201d). [S86] Questions that have a false preposition or assumption (e.g., \u201cWhat\u2019s the name of\nTaylor Swift\u2019s rap album before she transitioned to pop?\u201d (Taylor Swift has not yet\nreleased any rap album)). [S87] We designed three tasks. [S88] They share the same set of (question, answer) pairs but differ in the external\ndata accessible for retrieval to augment QA. [S89] Here, we provide the content that can be leveraged in\nQA to ensure fair comparisons. [S90] We describe how we generated the data in Section 3. [S91] Task 1: Retrieval Summarization. [S92] In Task 1, we provide up to five web pages for each question. [S93] These web pages are likely, but not guaranteed, to be relevant to the question. [S94] This task aims to test\nthe answer generation capability of a RAG system. [S95] Task 2: KG and Web Retrieval Augmentation. [S96] In Task 2, we in addition provide mock APIs to\naccess information from underlying mock KGs. [S97] The mock KGs store structured data relevant to the\nquestions; answers to the questions may or may not exist in the mock KGs. [S98] The mock APIs take\ninput parameters, oftentimes parsed from the question, and provide structured data from the mocked\nKGs to support answer generation. [S99] This task tests how well a RAG system 1) queries structured data\nsources and 2) synthesizes information from different sources. [S100] Task 3: End-to-end RAG. [S101] Similar to Task 2, Task 3 also provides both web search results and mock\nAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. [S102] The larger set\nof web pages are more likely to provide necessary information to answer the question, but meanwhile\nare more likely to contain noises. [S103] As such, Task 3 in addition tests how a RAG system ranks a larger\nnumber of retrieval results. [S104] The three tasks, each adding upon the previous one, allow testing different capabilities of the RAG\nsystems. [S105] The only component of a RAG system not covered by these tasks is search retrieval. [S106] One\nmay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fully\nend-to-end testing. [S107] 3 Dataset Description\n\nCRAG contains two parts of data: the QA pairs and the contents for retrieval. [S108] We now describe each\npart of the data. [S109] Data generation details can be found in Appendix A.1.1\u2013A.1.6. [S110] 4\n\n\fTable 3: The numbers and percentages (%, in parenthesis) of questions for each category of dynamism,\ndecided manually. [S111] Finance and Sports domain have the most Real-time and Fast-changing questions. [S112] Dynamism\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nReal-time\nFast-changing\nSlow-changing\nStatic\n\n434 (42)\n204 (20)\n183 (18)\n218 (21)\n\n0 ( 0)\n275 (33)\n215 (26)\n343 (41)\n\n2 ( 0)\n40 ( 6)\n152 (24)\n430 (69)\n\n0 ( 0)\n17 ( 2)\n253 (22)\n855 (76)\n\n1 ( 0)\n28 ( 4)\n204 (26)\n555 (70)\n\n437 (10)\n564 (13)\n1,007 (23)\n2,401 (54)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\nTable 4: The number and percentages (%, in parenthesis) of questions for each question type, decided\nmanually. [S113] Simple and simple with condition questions constitute 43% of all questions. [S114] Question type\n\nFinance\n\nSports\n\nMusic\n\nMovie\n\nOpen\n\nTotal\n\nSimple\nSimple w. [S115] condition\nSet\nComparison\nAggregation\nMulti-hop\nPost-processing heavy\nFalse Premise\n\n466 (45)\n113 (11)\n48 ( 5)\n146 (14)\n69 ( 7)\n86 ( 8)\n26 ( 3)\n85 ( 8)\n\n23 ( 3)\n250 (30)\n93 (11)\n85 (10)\n137 (16)\n64 ( 8)\n24 ( 3)\n157 (19)",
    "[S116] 112 (18)\n92 (15)\n72 (12)\n102 (16)\n96 (15)\n55 ( 9)\n26 ( 4)\n69 (11)\n\n519 (46)\n112 (10)\n104 ( 9)\n105 ( 9)\n71 ( 6)\n90 ( 8)\n28 ( 2)\n96 ( 9)\n\n85 (11)\n122 (15)\n86 (11)\n98 (12)\n116 (15)\n87 (11)\n76 (10)\n118 (15)\n\n1,205 (27)\n689 (16)\n403 ( 9)\n536 (12)\n489 (11)\n382 ( 9)\n180 ( 4)\n525 (12)\n\nAll\n\n1,039\n\n833\n\n624\n\n1,125\n\n788\n\n4,409\n\n3.1 Question answering pairs\n\nCRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types of\nquestions, all in English. [S117] The question types are listed in Table 2. [S118] We constructed the question-answer\npairs both from underlying KGs and web contents. [S119] QA pairs constructed from KGs. [S120] We constructed QA pairs from KGs by collecting a set of entities\nbased on publicly available data and then creating 600+ question templates based on selected entity\ntypes and relations. [S121] Next, we sampled entities with different popularities (head, torso and tail)\nfollowing [29] from the KGs to fill in the templates and generate the full question and answer. [S122] QA pairs constructed from web contents. [S123] We asked annotators to write down possible questions\nthat users may ask (e.g., \u201cmost popular action movies in 2023\u201d) and created QA pairs from the\ncorresponding web search results. [S124] Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,\nand 665 KG Questions containing head, torso, and tail entities respectively. [S125] Tables 3 and 4 summarize\nthe distribution of the questions across different dimensions. [S126] The size of each dimension slice (e.g.,\nfast-changing facts) allows us to get metrics with \u0103 5% margin-of-error (with 95% confidence level)\nfor most of the cases. [S127] The dynamism distribution roughly reflects the nature of the domain (e.g.,\nmuch more real-time questions for Finance than for other domains). [S128] See Appendix A.1.2 for the\ndefinition of the dynamism categories. [S129] 3.2 Contents for retrieval\n\nWe included two types of contents for retrieval to simulate the practical scenario for RAG: web search\nand KG search. [S130] Web search results. [S131] For each question, we used the question text as the search query and stored up\nto 50 HTML pages from the Brave search API [5]. [S132] See Table 8 in Appendix A.1.5 for an example. [S133] We estimated the web search recall with a heuristic-based method: first check whether the ground\ntruth answer URL was found among the pages; if not, decide whether the fact in the ground truths is\ncontained in the page snippet or content with an LLM. [S134] In particular, we pass the question, ground\ntruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answer\nthe question. [S135] 5\n\n\fFigure 2: For 85% of CRAG questions, the web search results are estimated to contain the ground\ntruth facts. [S136] The curve shows that the retrieval recall grows sharply at the beginning and flattens out\nlater on. [S137] Figure 2 shows the estimated web search recall curve for all CRAG questions, with an overall recall\nof 85% when using all 50 pages. [S138] It reflects multiple advantages of the benchmark by design. [S139] First,\nthe recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages is\nabout 69%. [S140] This is comparable to what we observe in practice when developing a RAG system. [S141] The\nnon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit \u201cI\ndon\u2019t know\u201d when the retrieval results do not contain the necessary information. [S142] Second, compared\nto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing the\nimportance of extracting and understanding HTML contents. [S143] Moreover, the estimated web search\nrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantly\nlower recall for KG questions than web questions. [S144] This aligns with our observations that web search\nrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs in\nTask 2 and 3.",
    "[S145] Mock KGs. [S146] We created mock KGs that contain publicly available KG data used to generate the\nquestions, randomly selected entities of the same type, and also \u201chard negative\u201d entities with similar\nnames (e.g., \u201cphantom\u201d for \u201cphantom of the opera\u201d). [S147] Mock APIs. [S148] We created mock APIs with pre-defined parameters to support structured search in the\nmock KGs. [S149] For example, for queries asking for stock prices, an example mock API is in the form of\nget_price_history(ticker). [S150] We collected snapshots of the KG and web search data concurrently while posing real-time and\nfast-changing questions. [S151] This approach ensures that we capture the \u201csnapshot\u201d of the information\nworld at the time of question answering. [S152] A RAG solution that performs well on the benchmark should\nalso be capable of reasoning over time and generalizing to evolving questions. [S153] In total, the resulting data contains 220K webpages, a KG of 2.6M entities, and 38 mock APIs. [S154] See\nTable 9 in the Appendix for a complete list of the mock APIs. [S155] 4 Metrics and Evaluation\n\nIn this section, we present the metrics for evaluating the RAG systems and briefly describe the 2024\nMeta KDD Cup challenge in Appendix A.2.3. [S156] 4.1 Metrics\n\nWe use a scoring method to assess the performance of RAG systems. [S157] For each question in the\nevaluation set, we first label the answer with perfect, acceptable, missing, or incorrect, according to\nthe following criteria. [S158] Perfect. [S159] The response correctly answers the user\u2019s question and contains no hallucinated content. [S160] 6\n\n05101520253035404550Number of retrieved web pages0102030405060708090100Estimated recall (%)45.152.856.157.659.560.861.862.863.564.469.476.279.080.181.182.282.983.683.984.6Page snippets+ Full pages\fAcceptable. [S161] The response provides a useful answer to the user\u2019s question but may contain minor\nerrors that do not harm the usefulness of the answer. [S162] Missing. [S163] The response is \u201cI don\u2019t know\u201d, \u201cI\u2019m sorry I can\u2019t find ...\u201d, a system error such as an empty\nresponse, or a request from the system to clarify the original question. [S164] Incorrect. [S165] The response provides wrong or irrelevant information to answer the user\u2019s question. [S166] We use a scoring method with score 1, 0.5, 0, and \u00b41 for each perfect, acceptable, missing, and\nincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers to\nincorrect ones. [S167] We then define truthfulness as the average score from all examples in the evaluation\nset for a given RAG system. [S168] 4.2 Evaluation\n\nSimilar to previous work [37], we employ both human evaluation (human-eval) and model-based\nautomatic evaluation (auto-eval). [S169] In the former, we use manual grading to judge perfect, acceptable,\nmissing, and incorrect for each answer. [S170] In the latter, we merge perfect and acceptable, call it accurate,\nand use a three-way scoring system with 1, \u00b41, 0 for accurate, incorrect, and missing answers. [S171] We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,\nit is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,\nincorrect, or missing. [S172] To avoid the self-preference problem [25], we use two LLM evaluators:\nChatGPT (gpt-3.5-turbo-0125) [24] and Llama 3 (llama-3-70B-instruct) [2] and report the\naverage accurate, hallucination, missing rates, and truthfulness scores from the two models for each\nRAG system. [S173] Our offline experiment shows that this two-step method yields an average F1 score of\n94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. [S174] See Appendix A.2.2 for more\ndetails.",
    "[S175] Test data split. [S176] We split the data randomly into validation (30%), public test (30%), and private\n(40%), and released the validation and public test sets (Appendix A.2.3). [S177] Participants of the KDD\nCup challenge can use the validation and public test sets to develop and test their models, and the\nsubmitted solutions were evaluated on the private test set. [S178] Future offline users of CRAG can use\nthe validation set for development, fine-tuning, and validation, and the public test set for testing and\nresult reporting. [S179] 5 Benchmarking\n\nIn this section, we present the performance of LLMs and RAG systems on CRAG, demonstrating\nthat CRAG has a reasonable level of difficulty and can help draw insights and show directions in\ndeveloping RAG techniques. [S180] 5.1 Straightforward RAG solutions\n\nExperiment setup: We started with running LLM-only solutions on the CRAG public test\nset with 1, 335 questions, using simple prompts that encourage brief answers and \u201cI don\u2019t\nknow\u201d answers when the confidence is low (Appendix A.3.1). [S181] We employed Llama 2 Chat\n(llama-2-7b-chat and llama-2-70b-chat) [34], Llama 3 Instruct (llama-3-8B-instruct and\nllama-3-70B-instruct) [2], Mixtral (Mixtral-8x7B-Instruct-v0.1) [15], Falcon (40B) [3],\nFLAN-T5 (FLAN-T5-XXL) [9], and GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1]. [S182] The web-only\nRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falcon\nand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenated\nwebpage snippets using the original order from the data as the reference text, until filling up the\nwindow (similar to [17, 23, 36]). [S183] Our KG-based solutions (Tasks 2, 3) additionally used a fixed-length\nKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama\n3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevant\nquery entities using llama-3-8B-instruct with in-context learning (similar to [28]) detailed in\nAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on the\nextracted entities), until filling up the window. [S184] We provide an extensive comparison of all LLMs\nin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4\nTurbo) in this section. [S185] 7\n\n\fTable 5: Performance of straightforward RAG solutions. [S186] All numbers are in percentage. [S187] LLM-only\nsolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. [S188] The\nsubscript in \u201ctruthfulnessa\u201d denotes the result is reported by auto-eval. [S189] Model\n\nAccuracy Hallucination Missing Truthfulnessa\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\nLlama 3 70B Instruct\nGPT-4 Turbo\n\n32.3\n33.5\n\n35.6\n35.9\n\n37.5\n41.3\n\n40.6\n43.6\n\n28.9\n13.5\n\n31.1\n28.2\n\n29.2\n25.1\n\n31.6\n30.1\n\n38.8\n53.0\n\n33.3\n35.9\n\n33.3\n33.6\n\n27.8\n26.3\n\n3.4\n20.0\n\n4.5\n7.7\n\n8.3\n16.2\n\n9.1\n13.4\n\nFigure 3: LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,\ndynamism, popularity, and question type.",
    "[S190] Table 5 shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) and\nillustrates that the CRAG benchmark is non-trivial. [S191] First, the best LLM-only solutions (GPT-4 Turbo)\nobtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement. [S192] Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra information\ndoes help answer more questions reliably. [S193] Interestingly, none of the RAG solutions obtain truthfulness\nhigher than 20%; this is because all RAG solutions introduce more hallucinations generated from\nirrelevant retrieval results, showing a big challenge in RAG\u2014How to judiciously use retrieval results\nwithout being distracted by retrieval noises? [S194] Third, we found that Task 2 truthfulness scores are\nhigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or even\nlower hallucination rate, because the KG knowledge is typically brief but precise. [S195] Unfortunately, the\nimprovement is mediocre, showing a second challenge in RAG\u2014How to best leverage the power of\nKG data? [S196] Finally, the truthfulness for Task 3 are also higher than Task 2, because of better search\nranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)\nand better search recall. [S197] In particular, we found that the ground truths of over 30% of questions are\navailable in the web retrieval results but are not included in the prompt due to the context window\nlimitation. [S198] This shows the importance of search ranking in RAG. [S199] Figure 3 shows the auto-eval results across the domain, dynamism, popularity, and question type\ndimension. [S200] The results reveal a lot of interesting observations and show that the CRAG benchmark\nallows more insightful conclusions. [S201] First, it shows which slices of the benchmark are harder. [S202] For\nexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real-\n\n8\n\nFinanceSportsMusicMovieOpen10010203040-4-131338-13619194040192623-109303135(a) DomainReal-timeFast-changingSlow-changingStatic100102030-1-7-148-15-149250-1524-5-81528(b) DynamismWebHeadTorsoTail05101520-16-101612110152111818171814(c) PopularitySimpleSimple w. [S203] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise403020100102030141-2727-30105-132111-93314235-172513-10250622333151535162716-37(d) Question typeLlama 3 70B Instruct (LLM only)Llama 3 70B Instruct (LLM + web + KG)GPT-4 Turbo (LLM only)GPT-4 Turbo (LLM + web + KG)\fTable 6: Benchmarking CRAG questions with industry SOTA RAG systems. [S204] Perfect, acceptable\n(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)\nare in percentages. [S205] The best system achieves truthfulness of 51% and provides perfect answers for up\nto 63% of questions. [S206] System\n\nPerfect Acc. [S207] Hall. [S208] Miss. [S209] Truthh\n\nLatency (ms)\n\nEqual\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\nTraffic\nweighted Gemini Advanced\n\nCopilot Pro\n\nChatGPT Plus\nMeta SG\nPerplexity.ai\n\n62.6\n60.8\n59.8\n52.5\n55.8\n\n70.0\n67.1\n61.8\n61.0\n63.7\n\n11.7\n10.1\n13.3\n9.7\n8.8\n\n9.5\n10.0\n11.4\n7.1\n6.3\n\n17.9\n16.6\n25.0\n16.0\n25.3\n\n14.3\n12.7\n25.7\n14.1\n20.9\n\n7.8\n12.5\n1.9\n21.8\n10.1\n\n6.1\n10.2\n1.3\n17.8\n9.1\n\n50.6\n49.3\n41.5\n41.4\n34.9\n\n60.5\n59.3\n41.8\n50.5\n45.9\n\n11,596\n5,246\n6,195\n3,431\n4,634\n\n-\n-\n-\n-\n-",
    "[S210] time and fast-changing facts, for tail entities, and for complex questions requiring set answers,\npost-processing, and with false premises. [S211] Second, it shows where it is harder to leverage retrieval\nresults. [S212] Take the popularity slices as an example, we observed that GPT-4 Turbo\u2019s truthfulness\ndropped from head (21%) to torso (11%) to tail (8%), consistent with past observations [29]; however,\nthe straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso\n(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). [S213] Finally, although\nour goal is not to compare different LLMs, the different dimensions allow us to understand the\nstrengths and weaknesses of each method. [S214] For example, although the RAG system based on Llama\n3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has a\nsimilar or slightly higher truthfulness in answering simple and comparison questions, whereas much\nlower truthfulness in answering set and post-processing questions, suggesting investigations on the\nreasoning capabilities. [S215] 5.2 State-of-the-art industry solutions\n\nNext, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. [S216] We\nselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAG\nquestions, collected the responses, and applied manual grading (details in Appendix A.4). [S217] In addition, we applied traffic weights to the questions to understand the solutions in real-world use\ncases. [S218] The traffic weights reflect the frequency of each question type, as defined in Table 2, in real\nQA traffic. [S219] We gave the same weights to all domains and reported the macro average across domains. [S220] This is because we have observed quite different domain-level distributions in different use cases, but\nhave been observing similar distributions at the query-type level. [S221] Table 6 and Figure 4 show the overall performance of the SOTA systems and their performance\nacross different dimensions. [S222] The evaluation results confirm our belief that the CRAG benchmark\nreveals interesting insights and shows room for improvement for existing RAG solutions. [S223] First,\nthe results from SOTA solutions achieve much better truthfulness (highest 51%) compared to the\nstraightforward solutions. [S224] However, the hallucination rate ranges from 16% to 25%, so the answers\nare still not trustworthy. [S225] Note that the truthfulness scores between the SOTA solutions and the\nstraightforward solutions are not completely comparable, as they have different accesses to retrieval\ncontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;\nhowever, the trend is valid. [S226] Second, we observed very different latency, ranging from 3.4s to 11.6s,\nreflecting the different design options in trading off latency and quality; for example, Copilot Pro\nhas the highest truthfulness, but meanwhile highest latency, whereas Meta SG [10] has mid-tier\ntruthfulness but lowest latency. [S227] (See Appendix A.4.2 for additional results and how we measured\nlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult for\nSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,\nshowing the improvement needed for handling retrieval noises when the system relies on retrieval\nresults to answer the question; as another example, we see lower truthfulness for queries requiring\naggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning\n\n9\n\n\fFigure 4: SOTA systems human-eval truthfulness scores (in percentage) across different dimensions. [S228] in question answering. [S229] Last, truthfulness on set and false premise questions improved significantly\nin the SOTA solutions compared to the straightforward solutions, showing advancement in RAG\nsystems in providing accurate and complete set answers and detecting false premises. [S230] 6 Conclusion",
    "[S231] This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research in\nretrieval-augmented generation (RAG). [S232] With detailed empirical studies, CRAG reviewed gaps in\nexisting RAG solutions and provided valuable insights for future improvement. [S233] We plan to continue\nimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-\nturn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts to\nemerging challenges, and evolves for new research needs. [S234] Acknowledgements\n\nWe would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with the\npartner teams. [S235] We thank Alex Boesenberg for enabling us to create web search results. [S236] We thank\nSam Wexler for coordinating this project with various partners. [S237] We thank Hejia Zhang, Rares Bostan,\nEryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regarding\nLLM and RAG evaluation. [S238] We thank Katie Manlove for coordinating with the budget and annotation\nneeds. [S239] We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,\nRani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,\nJoshua Aranzaso. [S240] We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotation\ntasks. [S241] We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,\nTy Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,\nJen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,\nand Amy Lee. [S242] Lei Chen\u2019s work is partially supported by National Key Research and Development Program of\nChina Grant No. [S243] 2023YFF0725100, National Science Foundation of China (NSFC) under Grant\nNo. [S244] U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE Project\nAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, Guangdong\nProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grants\nMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft Research\nAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-Chuanglin\nGraph Data Joint Lab. [S245] 10\n\nFinanceSportsMusicMovieOpen01020304050607080134557687419346757784246057701231515662431523662(a) DomainReal-timeFast-changingSlow-changingStatic100102030405060275665-3204963-7-551548154750-623946(b) DynamismWebHeadTorsoTail01020304050605948333960453227534419114843312549231215(c) PopularitySimpleSimple w. [S246] ConditionSetComparisonAggregationMulti-hopPost-processingFalse Premise010203040506070575168504034464054495944233836724430625731402365232554121395464029612918202141(d) Question typeCopilot ProGemini AdvancedChatGPT PlusMeta SGPerplexity.ai\fReferences\n\n[1] J. [S247] Achiam, S. [S248] Adler, S. [S249] Agarwal, L. [S250] Ahmad, I. [S251] Akkaya, F. [S252] L. [S253] Aleman, D. [S254] Almeida,\nJ. [S255] Altenschmidt, S. [S256] Altman, S. [S257] Anadkat, et al. [S258] GPT-4 technical report. [S259] arXiv preprint\narXiv:2303.08774, 2023. [S260] [2] AI@Meta. [S261] Llama 3 model card. [S262] 2024. [S263] [3] E. [S264] Almazrouei, H. [S265] Alobeidli, A. [S266] Alshamsi, A. [S267] Cappelli, R. [S268] Cojocaru, M. [S269] Debbah, \u00c9. [S270] Goffinet,\nD. [S271] Hesslow, J. [S272] Launay, Q. [S273] Malartic, et al. [S274] The falcon series of open language models. [S275] arXiv\npreprint arXiv:2311.16867, 2023. [S276] [4] P. [S277] Bajaj, D. [S278] Campos, N. [S279] Craswell, L. [S280] Deng, J. [S281] Gao, X. [S282] Liu, R. [S283] Majumder, A. [S284] McNamara,\nB. [S285] Mitra, T. [S286] Nguyen, M. [S287] Rosenberg, X. [S288] Song, A. [S289] Stoica, S. [S290] Tiwary, and T. [S291] Wang. [S292] MS MARCO:\nA human generated machine reading comprehension dataset, 2018. [S293] [5] Brave Software. [S294] Brave Search API. [S295] [6] J. [S296] Chen, H. [S297] Lin, X. [S298] Han, and L. [S299] Sun. [S300] Benchmarking large language models in retrieval-\n\naugmented generation. [S301] arXiv preprint arXiv:2309.01431, 2023. [S302] [7] W. [S303] Chen, H. [S304] Hu, X. [S305] Chen, P. [S306] Verga, and W. [S307] Cohen. [S308] MuRAG: Multimodal retrieval-augmented\nIn Proceedings of the 2022\n\ngenerator for open question answering over images and text. [S309] Conference on Empirical Methods in Natural Language Processing, Dec. [S310] 2022.",
    "[S311] [8] Z. [S312] Chen, Z. [S313] Gu, L. [S314] Cao, J. [S315] Fan, S. [S316] Madden, and N. [S317] Tang. [S318] Symphony: Towards natural language\n\nquery answering over multi-modal data lakes. [S319] In CIDR, 2023. [S320] [9] H. [S321] W. [S322] Chung, L. [S323] Hou, S. [S324] Longpre, B. [S325] Zoph, Y. [S326] Tay, W. [S327] Fedus, Y. [S328] Li, X. [S329] Wang, M. [S330] Dehghani,\nS. [S331] Brahma, et al. [S332] Scaling instruction-finetuned language models. [S333] Journal of Machine Learning\nResearch, 25(70):1\u201353, 2024. [S334] [10] X. [S335] L. [S336] Dong. [S337] The journey to a knowledgeable assistant with retrieval-augmented generation (rag). [S338] In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS\n\u201924, page 3, New York, NY, USA, 2024. [S339] Association for Computing Machinery. [S340] [11] M. [S341] Gao, X. [S342] Hu, J. [S343] Ruan, X. [S344] Pu, and X. [S345] Wan. [S346] Llm-based nlg evaluation: Current status and\n\nchallenges. [S347] arXiv preprint arXiv:2402.01383, 2024. [S348] [12] Y. [S349] Gao, Y. [S350] Xiong, X. [S351] Gao, K. [S352] Jia, J. [S353] Pan, Y. [S354] Bi, Y. [S355] Dai, J. [S356] Sun, Q. [S357] Guo, M. [S358] Wang, and H. [S359] Wang. [S360] Retrieval-augmented generation for large language models: A survey. [S361] 2024. [S362] [13] L. [S363] Huang, W. [S364] Yu, W. [S365] Ma, W. [S366] Zhong, Z. [S367] Feng, H. [S368] Wang, Q. [S369] Chen, W. [S370] Peng, X. [S371] Feng, B. [S372] Qin,\net al. [S373] A survey on hallucination in large language models: Principles, taxonomy, challenges,\nand open questions. [S374] arXiv preprint arXiv:2311.05232, 2023. [S375] [14] Z. [S376] Ji, N. [S377] Lee, R. [S378] Frieske, T. [S379] Yu, D. [S380] Su, Y. [S381] Xu, E. [S382] Ishii, Y. [S383] J. [S384] Bang, A. [S385] Madotto, and P. [S386] Fung. [S387] Survey of hallucination in natural language generation. [S388] ACM Comput. [S389] Surv., 55(12), mar 2023. [S390] [15] A. [S391] Q. [S392] Jiang, A. [S393] Sablayrolles, A. [S394] Mensch, C. [S395] Bamford, D. [S396] S. [S397] Chaplot, D. [S398] d. [S399] l. [S400] Casas, F. [S401] Bressand,\nG. [S402] Lengyel, G. [S403] Lample, L. [S404] Saulnier, et al. [S405] Mistral 7b. [S406] arXiv preprint arXiv:2310.06825, 2023. [S407] [16] M. [S408] Joshi, E. [S409] Choi, D. [S410] Weld, and L. [S411] Zettlemoyer. [S412] TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. [S413] Association for Computational Linguistics, July\n2017. [S414] [17] N. [S415] Kandpal, H. [S416] Deng, A. [S417] Roberts, E. [S418] Wallace, and C. [S419] Raffel. [S420] Large language models struggle to\nlearn long-tail knowledge. [S421] In International Conference on Machine Learning, pages 15696\u2013\n15707. [S422] PMLR, 2023. [S423] [18] T. [S424] Kwiatkowski, J. [S425] Palomaki, O. [S426] Redfield, M. [S427] Collins, A. [S428] Parikh, C. [S429] Alberti, D. [S430] Epstein,\nI. [S431] Polosukhin, M. [S432] Kelcey, J. [S433] Devlin, K. [S434] Lee, K. [S435] N. [S436] Toutanova, L. [S437] Jones, M.-W. [S438] Chang, A. [S439] Dai,\nJ. [S440] Uszkoreit, Q. [S441] Le, and S. [S442] Petrov. [S443] Natural questions: a benchmark for question answering\nresearch. [S444] Transactions of the Association of Computational Linguistics, 2019. [S445] [19] P. [S446] Lewis, E. [S447] Perez, A. [S448] Piktus, F. [S449] Petroni, V. [S450] Karpukhin, N. [S451] Goyal, H. [S452] K\u00fcttler, M. [S453] Lewis, W. [S454] tau\nYih, T. [S455] Rockt\u00e4schel, S. [S456] Riedel, and D. [S457] Kiela. [S458] Retrieval-augmented generation for knowledge-\nintensive nlp tasks, 2021. [S459] [20] V. [S460] Li\u00e9vin, C. [S461] E. [S462] Hother, A. [S463] G. [S464] Motzfeldt, and O. [S465] Winther. [S466] Can large language models reason\n\nabout medical questions? [S467] Patterns, 5(3), 2024. [S468] 11\n\n\f[21] C.-Y. [S469] Lin. [S470] Rouge: A package for automatic evaluation of summaries. [S471] In Text summarization\n\nbranches out, pages 74\u201381, 2004. [S472] [22] P. [S473] Liu, W. [S474] Yuan, J. [S475] Fu, Z. [S476] Jiang, H. [S477] Hayashi, and G. [S478] Neubig. [S479] Pre-train, prompt, and predict:\nA systematic survey of prompting methods in natural language processing. [S480] ACM Computing\nSurveys, 55(9):1\u201335, 2023. [S481] [23] A. [S482] Mallen, A. [S483] Asai, V. [S484] Zhong, R. [S485] Das, D. [S486] Khashabi, and H. [S487] Hajishirzi. [S488] When not to trust\nlanguage models: Investigating effectiveness of parametric and non-parametric memories. [S489] In\nACL, 2023. [S490] [24] OpenAI. [S491] ChatGPT. [S492] https://openai.com/index/chatgpt/, 2023. [S493] Accessed: 2024-06-04. [S494] [25] A. [S495] Panickssery, S. [S496] R. [S497] Bowman, and S. [S498] Feng. [S499] Llm evaluators recognize and favor their own\n\ngenerations. [S500] arXiv preprint arXiv:2404.13076, 2024. [S501] [26] R. [S502] Pradeep, N. [S503] Thakur, S. [S504] Sharifymoghaddam, E. [S505] Zhang, R. [S506] Nguyen, D. [S507] Campos, N. [S508] Craswell,\nand J. [S509] Lin. [S510] Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-\naugmented generation track. [S511] arXiv preprint arXiv:2406.16828, 2024.",
    "[S512] [27] V. [S513] Rawte, S. [S514] Chakraborty, A. [S515] Pathak, A. [S516] Sarkar, S. [S517] Tonmoy, A. [S518] Chadha, A. [S519] P. [S520] Sheth, and A. [S521] Das. [S522] The troubling emergence of hallucination in large language models\u2013an extensive definition,\nquantification, and prescriptive remediations. [S523] arXiv preprint arXiv:2310.04988, 2023. [S524] [28] T. [S525] Schick, J. [S526] Dwivedi-Yu, R. [S527] Dessi, R. [S528] Raileanu, M. [S529] Lomeli, L. [S530] Zettlemoyer, N. [S531] Cancedda, and\nT. [S532] Scialom. [S533] Toolformer: Language models can teach themselves to use tools. [S534] arXiv, 2023. [S535] [29] K. [S536] Sun, Y. [S537] E. [S538] Xu, H. [S539] Zha, Y. [S540] Liu, and X. [S541] L. [S542] Dong. [S543] Head-to-Tail: How knowledgeable are\nlarge language models (llms)? [S544] a.k.a. [S545] will llms replace knowledge graphs? [S546] arXiv preprint\narXiv:2308.10168, 2024. [S547] [30] Y. [S548] Sun, H. [S549] Xin, K. [S550] Sun, Y. [S551] E. [S552] Xu, X. [S553] Yang, X. [S554] L. [S555] Dong, N. [S556] Tang, and L. [S557] Chen. [S558] Are large\nlanguage models a good replacement of taxonomies? [S559] Proc. [S560] VLDB Endow., 17(11):2919\u20132932,\naug 2024. [S561] [31] A. [S562] Talmor and J. [S563] Berant. [S564] The web as a knowledge-base for answering complex questions, 2018. [S565] [32] N. [S566] Tang, C. [S567] Yang, J. [S568] Fan, L. [S569] Cao, Y. [S570] Luo, and A. [S571] Y. [S572] Halevy. [S573] Verifai: Verified generative AI. [S574] In\n\nCIDR, 2024. [S575] [33] Y. [S576] Tang and Y. [S577] Yang. [S578] Multihop-rag: Benchmarking retrieval-augmented generation for multi-\n\nhop queries. [S579] arXiv preprint arXiv:2401.15391, 2024. [S580] [34] H. [S581] Touvron, L. [S582] Martin, K. [S583] Stone, P. [S584] Albert, A. [S585] Almahairi, Y. [S586] Babaei, N. [S587] Bashlykov, S. [S588] Batra,\nP. [S589] Bhargava, S. [S590] Bhosale, D. [S591] Bikel, L. [S592] Blecher, C. [S593] C. [S594] Ferrer, M. [S595] Chen, G. [S596] Cucurull, D. [S597] Esiobu,\nJ. [S598] Fernandes, J. [S599] Fu, W. [S600] Fu, B. [S601] Fuller, C. [S602] Gao, V. [S603] Goswami, N. [S604] Goyal, A. [S605] Hartshorn, S. [S606] Hosseini,\nR. [S607] Hou, H. [S608] Inan, M. [S609] Kardas, V. [S610] Kerkez, M. [S611] Khabsa, I. [S612] Kloumann, A. [S613] Korenev, P. [S614] S. [S615] Koura, M.-A. [S616] Lachaux, T. [S617] Lavril, J. [S618] Lee, D. [S619] Liskovich, Y. [S620] Lu, Y. [S621] Mao, X. [S622] Martinet, T. [S623] Mihaylov, P. [S624] Mishra,\nI. [S625] Molybog, Y. [S626] Nie, A. [S627] Poulton, J. [S628] Reizenstein, R. [S629] Rungta, K. [S630] Saladi, A. [S631] Schelten, R. [S632] Silva, E. [S633] M. [S634] Smith, R. [S635] Subramanian, X. [S636] E. [S637] Tan, B. [S638] Tang, R. [S639] Taylor, A. [S640] Williams, J. [S641] X. [S642] Kuan, P. [S643] Xu, Z. [S644] Yan,\nI. [S645] Zarov, Y. [S646] Zhang, A. [S647] Fan, M. [S648] Kambadur, S. [S649] Narang, A. [S650] Rodriguez, R. [S651] Stojnic, S. [S652] Edunov, and\nT. [S653] Scialom. [S654] Llama 2: Open foundation and fine-tuned chat models, 2023. [S655] [35] R. [S656] Usbeck, X. [S657] Yan, A. [S658] Perevalov, L. [S659] Jiang, J. [S660] Schulz, A. [S661] Kraft, C. [S662] M\u00f6ller, J. [S663] Huang, J. [S664] Reineke,\nA.-C. [S665] Ngonga Ngomo, et al. [S666] QALD-10\u2013the 10th challenge on question answering over linked\ndata. [S667] Semantic Web, (Preprint):1\u201315, 2023. [S668] [36] T. [S669] Vu, M. [S670] Iyyer, X. [S671] Wang, N. [S672] Constant, J. [S673] Wei, J. [S674] Wei, C. [S675] Tar, Y.-H. [S676] Sung, D. [S677] Zhou, Q. [S678] Le, and\nT. [S679] Luong. [S680] FreshLLMs: Refreshing large language models with search engine augmentation,\n2023. [S681] [37] F. [S682] Xu, Y. [S683] Song, M. [S684] Iyyer, and E. [S685] Choi. [S686] A critical evaluation of evaluations for long-form\n\nquestion answering. [S687] In Association of Computational Linguistics, 2023. [S688] [38] M. [S689] Yasunaga, H. [S690] Ren, A. [S691] Bosselut, P. [S692] Liang, and J. [S693] Leskovec. [S694] QA-GNN: Reasoning with\nlanguage models and knowledge graphs for question answering. [S695] Association for Computational\nLinguistics, 2021. [S696] [39] Y. [S697] Zhu, S. [S698] Du, B. [S699] Li, Y. [S700] Luo, and N. [S701] Tang. [S702] Are large language models good statisticians? [S703] In\n\nNeurIPS, 2024. [S704] 12\n\n\fA Appendix\n\nA.1 Dataset\n\nA.1.1 Constructing QA pairs from KGs\n\nWe first collected a set of entities based on publicly available data. [S705] Then we created question-answer\npairs in three steps for Simple static and dynamic questions. [S706] Step 1. [S707] For each domain, we first selected an entity type and a meaningful relation pe, rq and created\na question template. [S708] For example, for (music artist, first album), we create a template \u201cwhat is the\nfirst album of [music artist]?\u201d. [S709] Step 2. [S710] We then sampled entities from the KGs to fill in the templates and generate the full question. [S711] We adopted the method described in [29] and sampled entities of top, middle, and bottom popularity. [S712] We defined popularity based on heuristics for each entity type and created an equal number of\nquestions for each bucket. [S713] Step 3. [S714] Last, we took the associated attribute values as the answer to the question to create question-\nanswer pairs.",
    "[S715] We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in a\nsimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampled\nthe subject entities that fit the question. [S716] We used heuristics to select entity types for these question\ncategories. [S717] Finally, we created multi-hop questions in three steps, similar to those described in [31]. [S718] We first\nsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:\npe1, r1, e2q and pe1, r2, e3q. [S719] We then created a question template describing the path. [S720] For example,\nfor path (company1, is_parent, company2) followed by (company1, ceo, person), we created the\ntemplate \"who is the CEO of the parent company of [company2]?\". [S721] The answer to the new question\nwill be e3 in the second triplet. [S722] A.1.2 Definition of dynamism categories\n\nTable 7: Definition of dynamism categories. [S723] Dynamism\n\nDefinition\n\nReal-time\n\nThe answer to the question changes over seconds\n(e.g., \u201cWhat\u2019s Costco\u2019s stock price today?\u201d). [S724] Fast-changing\n\nThe answer to the question changes no more than daily\n(e.g., \u201cWhen is Laker\u2019s game tonight?\u201d). [S725] Slow-changing\n\nThe answer to the question changes no more than yearly\n(e.g., \u201cWho won the Grammy award last year?\u201d). [S726] Static\n\nThe answer to the question does not change over time,\nsuch as the birth date of a person. [S727] A.1.3 Constructing QA pairs from web contents\n\nStep 1. [S728] Ask annotators to write down a list of questions that could possibly be answered by web\nsearch based on a general guideline (e.g., \u201cwhat is the most popular action movie in 2023?\u201d). [S729] Step 2. [S730] Generate the web search results to answer the question. [S731] Step 3. [S732] Finally, annotators reviewed the web search results to determine the ground truth answers to\nthe questions: 1) If the search results successfully provided the necessary information, annotators\nrecorded the ground truth answer text and the URL associated with it based on the retrieved content. [S733] Note that the answer is determined by the query_time at which the web search happened, especially\nfor the Fast-changing and Real-time questions. [S734] 2) Otherwise, annotators conducted further web\nsearches to document the correct answers. [S735] 13\n\n\fBesides the QA pairs, the annotators will also provide labels for the domain, dynamism, question\ntypes, and an answer URL (a URL that contains the answer to the question) for Web Questions. [S736] A.1.4 Validation for the QA pairs\n\nWe conducted two phases of dataset validation with our in-house Linguist team. [S737] Phase 1. [S738] Question and meta-label validation. [S739] After the initial round of QA pair generation, an\naudit session was conducted, where expert annotators reviewed the question template, questions,\nand meta-labels (domain, question type, etc), applying edits as necessary with 2x human review\n(agreement rate 90%`). [S740] All problematic questions (e.g., a wrong false-premise question) were\nrevised, and all conflicting labels were resolved by a third more experienced auditor. [S741] Phase 2. [S742] Answer validation. [S743] To ensure the answers in the benchmark are correct, we further\nconducted an auditing process for all the answers. [S744] For the web questions, an annotation team\nreviewed each question and conducted an extensive search to make sure the answer is factually\ncorrect and includes comprehensive information (such as for set questions) with 2x human review\n(agreement rate 90%`). [S745] A third more experienced auditor then reviewed all conflicting answers and\nprovided a resolution. [S746] For the KG questions, a team of five engineers carefully checked the questions\nand queried the mock APIs manually to validate the answers. [S747] This step resulted in a 5% answer\ncorrection. [S748] In both phases, we paid special attention to examples where the straightforward solutions output\ndifferent answers from the ground truth answers and asked the auditing team to double-check those\nexamples. [S749] This step yielded an additional 2% answer updates. [S750] A.1.5 An example of retrieved web search results\n\nKey",
    "[S751] \"page name\"\n\"page url\"\n\n\"page snippet\"\n\n\"page last modified\"\n\"html page\"\n\nTable 8: An example of web search results. [S752] Value\n\n\"A Short History Of ChatGPT: How We Got To Where We Are Today\"\n\"https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-\nhow...\"\n\"OpenAI\n2022</strong>...\"\n\"2024-1-18 15:32:24\"\n\"<!DOCTYPE html><html\nhref=\"https...\"\n\nreleased an early demo of ChatGPT on <strong>November 30,\n\nlang=\"en\"><head><link rel=\"preload\" as=\"font\"\n\nA.1.6 The mock data and mock APIs\n\nCRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the real\nretrieval environment, allowing accessible facilitating data and fair comparison. [S753] CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-\ntion to test the effectiveness of RAG systems in leveraging a diverse range of available information. [S754] First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-\nworld search engine\u2014the Brave Search API [5]. [S755] Different from existing benchmarks that use snippets\nor selected text chunks [4, 6], CRAG provides full HTML pages, containing more information and\npotentially more noises as in a realistic setting. [S756] Second, CRAG provides mock KG search APIs to test\nstructured search for RAG. [S757] The mock KGs, though much smaller in size, contain both information\nnecessary to answer a subset of questions in the benchmark and noises that have similar entity or\nattribute names, again simulating real settings. [S758] Our mock KGs contain about 2.6M entities and have\na signal-to-noise ratio of less than 1/30. [S759] 14\n\n\fTable 9: Mock APIs and Descriptions. [S760] APIs\n\nDescriptions\n\nopen_search_entity_by_name(query: str) -> dict\nopen_get_entity(entity: str) -> dict\n\nmovie_get_person_info(person_name: str) -> dict\nmovie_get_movie_info(movie_name: str) -> dict\nmovie_get_year_info(year: str) -> dict\n\nmovie_get_movie_info_by_id(movie_id: int) -> dict\nmovie_get_person_info_by_id(person_id: int) -> dict\n\nfinance_get_company_name(query: str) -> dict\nfinance_get_ticker_by_name(query: str) -> dict\nfinance_get_price_history(ticker_name: str) -> dict\nfinance_get_detailed_price_history(ticker_name: str) -> dict\nfinance_get_dividends_history(ticker_name: str) -> dict\nfinance_get_market_capitalization(ticker_name: str) -> dict\nfinance_get_eps(ticker_name: str) -> dict\nfinance_get_pe_ratio(ticker_name: str) -> dict\nfinance_get_info(ticker_name: str) -> dict\n\nmusic_search_artist_entity_by_name(artist_name: str) -> dict\nmusic_search_song_entity_by_name(song_name: str) -> dict\nmusic_get_billboard_rank_date(rank: int, date: str = None) -> dict\nmusic_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dict\nmusic_grammy_get_best_artist_by_year(year: int) -> dict\nmusic_grammy_get_award_count_by_artist(artist_name: str) -> dict\nmusic_grammy_get_award_count_by_song(song_name: str) -> dict\nmusic_grammy_get_best_song_by_year(year: int) -> dict\nmusic_grammy_get_award_date_by_artist(artist_name: str) -> dict\nmusic_grammy_get_best_album_by_year(year: int) -> dict\nmusic_grammy_get_all_awarded_artists() -> dict\nmusic_get_artist_birth_place(artist_name: str) -> dict\nmusic_get_artist_birth_date(artist_name: str) -> dict\nmusic_get_members(band_name: str) -> dict\nmusic_get_lifespan(artist_name: str) -> dict\nmusic_get_song_author(song_name: str) -> dict\nmusic_get_song_release_country(song_name: str) -> dict\nmusic_get_song_release_date(song_name: str) -> dict\nmusic_get_artist_all_works(artist_name: str) -> dict\n\nSearch for entities by name in the Open domain. [S761] Retrieve detailed information about an entity in the\nOpen domain. [S762] Get information about a person related to movies. [S763] Get information about a movie. [S764] Get information about movies released in a specific\nyear. [S765] Get movie information by its unique ID. [S766] Get person information by their unique ID.",
    "[S767] Search for company names in the finance domain. [S768] Retrieve the ticker symbol for a given company name. [S769] Get the price history for a given ticker symbol. [S770] Get detailed price history for a ticker symbol. [S771] Get dividend history for a ticker symbol. [S772] Retrieve market capitalization for a ticker symbol. [S773] Get earnings per share (EPS) for a ticker symbol. [S774] Get the price-to-earnings (PE) ratio for a ticker symbol. [S775] Get financial information for a ticker symbol. [S776] Search for music artists by name. [S777] Search for songs by name. [S778] Get Billboard ranking for a specific rank and date. [S779] Get attributes of a song from Billboard rankings. [S780] Get the Grammy Best New Artist for a specific year. [S781] Get the total Grammy awards won by an artist. [S782] Get the total Grammy awards won by a song. [S783] Get the Grammy Song of the Year for a specific year. [S784] Get the years an artist won a Grammy award. [S785] Get the Grammy Album of the Year for a specific year. [S786] Get all artists awarded the Grammy Best New Artist. [S787] Get the birthplace of an artist. [S788] Get the birth date of an artist. [S789] Get the member list of a band. [S790] Get the lifespan of an artist. [S791] Get the author of a song. [S792] Get the release country of a song. [S793] Get the release date of a song. [S794] Get all works by an artist. [S795] sports_soccer_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_games_on_date(team_name: str, date: str) -> dict\nsports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dict\n\nGet soccer games on a specific date. [S796] Get NBA games on a specific date. [S797] Get NBA play by play data for a set of game ids. [S798] A.2 Evaluation\n\nA.2.1 Human evaluation\n\nWe run human evaluation to score each answer with respect to the metrics defined in Section 4.1. [S799] We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,\nrespectively and define truthfulnessh as the score of the answer by setting sp \u201c 1, sa \u201c 0.5, sm \u201c 0,\nand sin \u201c \u00b41. [S800] We then compute the average truthfulness for all examples in the evaluation set as the\ntruthfulness score for the RAG solution. [S801] Human-eval instructions. [S802] The human-eval instructions are as follows. [S803] Given a query, a query day and time at which the query was made, the chatbot\u2019s response, grade the\naccuracy for the response according to the criteria below:\nUsing an external search engine, please evaluate the factual accuracy of the response based on the\ngrading rubric below. [S804] An accurate answer should be factually correct and provide useful information\nto answer the user\u2019s question. [S805] \u2022 Accuracy = 0 (Missing). [S806] It covers the following situations. [S807] There is no response. [S808] There is a\nfailure to provide a response to the request (e.g. [S809] \u201cI\u2019m sorry . [S810] . [S811] . [S812] . [S813] . [S814] . [S815] , I can\u2019t do . [S816] . [S817] . [S818] \u201d) due to\ninability. [S819] E.g., Query: \u201cLatest news about the Nobel prize today.\u201d Response: \u201cI can\u2019t find\nspecific information regarding the Nobel prize . [S820] . [S821] . [S822] \u201d The response fails to answer and asks\nfollow-up questions. [S823] \u2022 Accuracy = 1 (Incorrect). [S824] It covers the following situations. [S825] The answer is unintelligible. [S826] The answer is poorly formed. [S827] The answer contains a major hallucination (e.g. [S828] wrong date,\n\n15\n\n\fTable 10: Accuracy and F1 for the ChatGPT and Llama 3 auto-evaluation models. [S829] Accuracy\n\nPrecision\n\nRecall\n\nF1 score\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nChatGPT\n\nLlama 3\n\nAccurate\nIncorrect\nMissing\n\nAverage\n\n94.1\n94.1\n100.0\n\n96.1\n\n98.6\n98.6\n100.0\n\n99.1\n\n98.8\n86.8\n100.0\n\n95.2\n\n98.5\n98.7\n100.0\n\n99.1\n\n92.2\n97.8\n100.0\n\n96.7\n\n99.3\n97.2\n100.0\n\n98.8\n\n92.0\n92.0\n100.0\n\n94.7\n\n98.9\n97.9\n100.0\n\n98.9\n\nwrong numbers, or other significant factual errors). [S830] The answer is irrelevant to the user\u2019s\nrequest. [S831] Answers in a category, location, or time window that is significantly different from\nthe user\u2019s request, if any. [S832] There is a significant structural/formatting error. [S833] The response is\notherwise a total structural/functional failure and does not contain sufficient well-formed\ncontent that can be used to determine accuracy",
    "[S834] \u2022 Accuracy = 2 (Acceptable). [S835] It covers the following situations. [S836] The answer is acceptably\ncorrect and relevant to the user\u2019s request, but may miss some information, i.e. [S837] accurate but\nnot complete, or mostly accurate with minor issues. [S838] The answer may contain some minor\nhallucination that doesn\u2019t significantly alter the overall meaning. [S839] \u201cMinor hallucination\u201d\nmeans the answer addressed the user\u2019s question but might be off on some additional details. [S840] The rule of thumb is to see if the answer serves the purpose of the user\u2019s question, and\nwhether the hallucination could mislead the users on what they were asking. [S841] \u2022 Accuracy = 3 (Accurate). [S842] The answer is factually correct, contains all the relevant informa-\ntion requested, responds appropriately to the query, and does not contain any hallucination. [S843] We requested two human graders to grade each question (agreement rate 94%), and when there is a\nconflict, a third more experienced grader will resolve it. [S844] A.2.2 Automatic evaluation\n\nIn auto-eval, we merge perfect and acceptable as accurate and consider only three scores: 1 for\naccurate, 0 for missing, and -1 for incorrect. [S845] The truthfulnessa is calculated by the average\ntruthfulness for all the examples in the evaluation set, and is effectively\n\naccuracy \u00b4 hallucination,\n\nwhere accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missing\nanswers in the test set. [S846] These score choices penalize incorrect answers, award correct, and assign a\nvalue of 0 to missing answers. [S847] Auto-evaluators. [S848] We computed the accuracy and F1 for the two auto-evaluators for the accurate,\nincorrect, and missing examples in the public test set, respectively. [S849] Here, we considered human-eval\nlabels as the ground truth. [S850] Table 10 shows both models attain reasonable accuracy and F1 scores as\nan evaluator compared to human evaluation. [S851] If the model prediction matches any provided answers from the Ground Truth Answer list,\n\nAuto-eval prompt. [S852] The prompt we used in the auto-eval is similar to the following. [S853] We did not\nrelease the exact prompt used in the challenge to avoid prompt attack. [S854] PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truth\nanswers, judge whether the model Prediction matches any answer from the list of Ground Truth\nanswers. [S855] Follow the instructions step by step to make a judgement. [S856] 1. [S857] \"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\". [S858] 2. [S859] If the model prediction says that it couldn\u2019t answer the question or it doesn\u2019t have enough\ninformation, \"Accuracy\" should always be \"False\". [S860] 3. [S861] If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction is\nexactly \"invalid question\". [S862] # Output:\nRespond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\". [S863] # Examples:\nQuestion: how many seconds is 3 minutes 15 seconds? [S864] Ground truth: [\"195 seconds\"]\n\n16\n\n\fPrediction: 3 minutes 15 seconds is 195 seconds. [S865] Accuracy: True\nQuestion: Who authored The Taming of the Shrew (published in 2002)? [S866] Ground truth: [\"William Shakespeare\", \"Roma Gill\"]\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare. [S867] Accuracy: False\nQuestion: Who played Sheldon in Big Bang Theory? [S868] Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]\nPrediction: I am sorry I don\u2019t know. [S869] Accuracy: False\n\"\"\"\n\nA.2.3 KDD Cup 2024 Meta CRAG challenge",
    "[S870] The KDD Cup 2024 Meta CRAG challenge has two stages. [S871] Stage 1 is designed for the participants to\ndevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2\ndetermines the final winners. [S872] We split our benchmark data into three sets with similar distributions:\nvalidation, public test, and private test at 30%, 30%, and 40%, respectively. [S873] We shared the validation\nand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select the\nfinal winners in Stage 2. [S874] We used auto-eval for Stage 1, and selected top teams with auto-eval in\nStage 2 to conduct manual evaluation. [S875] A.3 Evaluating straightforward RAG solutions\n\nWe send each CRAG question in a prompt shown below. [S876] This prompt is designed to include the\noriginal Query Time for the question and ask the LLM to answer the question based on the query\ntime and the retrieved result. [S877] Note that the retrieved result was also from the same query time and\nwas provided in CRAG. [S878] Moreover, this prompt encourages brief answers and \u201cI don\u2019t know\u201d answers\nwhen confidence is low. [S879] A.3.1 Prompts used in straightforward RAG solutions\n\nVanilla LLM. [S880] PROMPT = \"\"\" You are given a Question and the time when it was asked in the\nPacific Time Zone (PT), referred to as \"Query Time\". [S881] The query time is formatted as \"mm/dd/yyyy,\nhh:mm:ss PT\". [S882] Your task is to answer the question in as few words as possible. [S883] Please follow these guidelines when formulating your answer:\n1. [S884] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S885] 2. [S886] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S887] ### Question\n{query}\n### Query Time\n{query_time}\n### Answer\n\"\"\"\n\nRAG with web search results (Task 1). [S888] PROMPT = \"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". [S889] The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S890] The references may or may not help answer the\nquestion. [S891] Your task is to answer the question in as few words as possible. [S892] Please follow these guidelines when formulating your answer:\n1. [S893] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S894] 2. [S895] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S896] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n\n17\n\n\f### Answer\n\"\"\"\n\nRAG with KG and web search results (Tasks 2 and 3). [S897] PROMPT = \"\"\" You are given a Question,\nReferences and the time when it was asked in the Pacific Time Zone (PT), referred to as \"Query\nTime\". [S898] The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". [S899] The references may or may not\nhelp answer the question. [S900] Your task is to answer the question in as few words as possible. [S901] Please follow these guidelines when formulating your answer:\n1. [S902] If the question contains a false premise or assumption, answer \u201cinvalid question\u201d. [S903] 2. [S904] If you are uncertain or don\u2019t know the answer, respond with \u201cI don\u2019t know\u201d. [S905] ### Question\n{query}\n### Query Time\n{query_time}\n### References\n# web\n{web_results}\n# knowledge graph\n{kg_response}\n### Answer\n\"\"\"\n\nQuery entity extraction. [S906] PROMPT = \"\"\" You are an agent that only outputs JSON. [S907] You are given a\nQuery and Query Time. [S908] Do the following:\n\n1) Determine the domain the query is about. [S909] The domain should be one of the following:\n\"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". [S910] If none of the domains apply, use \"other\". [S911] Use\n\"domain\" as the key in the result json. [S912] 2) Extract structured information from the query. [S913] depending on the domains, and put them DIRECTLY in the result json. [S914] Here are the rules:\n\nInclude different keys into the result json\n\nFor \u2018encyclopedia\u2019 and \u2018other\u2019 queries, these are possible keys:\n- \u2018main_entity\u2019: extract the main entity of the query.",
    "[S915] For \u2018finance\u2019 queries, these are possible keys:\n- \u2018market_identifier\u2019: stock identifiers including individual company names, stock symbols. [S916] - \u2018metric\u2019: financial metrics that the query is asking about. [S917] This must be one of the following: \u2018price\u2019,\n\u2018dividend\u2019, \u2018P/E ratio\u2019, \u2018EPS\u2019, \u2018marketCap\u2019, and \u2018other\u2019. [S918] - \u2018datetime\u2019: time frame that the query asks about. [S919] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S920] For \u2018movie\u2019 queries, these are possible keys:\n- \u2018movie_name\u2019: name of the movie\n- \u2018movie_aspect\u2019: if the query is about a movie, which movie aspect the query asks. [S921] This must be one\nof the following: \u2018budget\u2019, \u2018genres\u2019, \u2018original_language\u2019, \u2018original_title\u2019, \u2018release_date\u2019, \u2018revenue\u2019,\n\u2018title\u2019, \u2018cast\u2019, \u2018crew\u2019, \u2018rating\u2019, \u2018length\u2019. [S922] - \u2018person\u2019: person name related to moves\n- \u2018person_aspect\u2019: if the query is about a person, which person aspect the query asks. [S923] This must be\none of the following: \u2018acted_movies\u2019, \u2018directed_movies\u2019, \u2018oscar_awards\u2019, \u2018birthday\u2019. [S924] - \u2018year\u2019: if the query is about movies released in a specific year, extract the year\n\nFor \u2018music\u2019 queries, these are possible keys:\n- \u2018artist_name\u2019: name of the artist\n- \u2018artist_aspect\u2019: if the query is about an artist, extract the aspect of the artist. [S925] This must be one of the\nfollowing: \u2018member\u2019, \u2018birth place\u2019, \u2018birth date\u2019, \u2018lifespan\u2019, \u2018artist work\u2019, \u2018grammy award count\u2019,\n\u2018grammy award date\u2019. [S926] - \u2018song_name\u2019: name of the song\n- \u2018song_aspect\u2019: if the query is about a song, extract the aspect of the song. [S927] This must be one of the\n\n18\n\n\ffollowing: \u2018author\u2019, \u2018grammy award count\u2019, \u2018release country\u2019, \u2018release date\u2019. [S928] For \u2018sports\u2019 queries, these are possible keys:\n- \u2018sport_type\u2019: one of \u2018basketball\u2018, \u2018soccer\u2018, \u2018other\u2018\n- \u2018tournament\u2019: NBA, World Cup, Olympic. [S929] - \u2018team\u2019: teams that users are interested in. [S930] - \u2018datetime\u2019: time frame that the user is interested in. [S931] When datetime is not explicitly mentioned, use\n\u2018Query Time\u2019 as default. [S932] Return the results in a FLAT json. [S933] *NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON!!!*\n\"\"\"\n\nA.3.2 Performance of straightforward RAG solutions\n\nTable 11 summarizes the results of straightforward RAG solutions. [S934] Table 11: Performance of straightforward RAG solutions on CRAG. [S935] Model\n\nAccuracy (%)\n\nHallucination (%) Missing (%)\n\nTruthfulness (%)\n\nLLM only\n\nTask 1\n\nTask 2\n\nTask 3\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\nLlama 2 7B Chat\nLlama 2 70B Chat\nLlama 3 8B Instruct\nLlama 3 70B Instruct\nFalcon 40B\nFLAN-T5-XXL 11B\nMixtral-8x7B-Instruct-v0.1\nGPT-4 Turbo\n\n14.8\n22.3\n23.7\n32.3\n10.8\n9.4\n20.8\n33.5\n\n16.4\n29.3\n28.5\n35.6\n21.9\n27.5\n33.6\n35.9\n\n16.4\n29.1\n28.6\n37.5\n21.9\n27.4\n33.4\n41.3\n\n16.0\n31.9\n32.1\n40.6\n22.0\n27.8\n33.5\n43.6\n\n78.4\n28.7\n33.8\n28.9\n41.9\n8.7\n27.0\n13.5\n\n83.1\n61.0\n45.6\n31.1\n55.5\n36.5\n44.4\n28.2\n\n83.1\n61.1\n45.5\n29.2\n55.4\n36.6\n44.6\n25.1\n\n83.6\n65.7\n56.3\n31.6\n56.6\n37.1\n44.1\n30.1\n\n6.7\n49.0\n42.6\n38.8\n47.3\n81.9\n52.1\n53.0\n\n0.5\n9.7\n25.9\n33.3\n22.5\n36.0\n22.0\n35.9\n\n0.5\n9.7\n25.9\n33.3\n22.7\n36.0\n22.0\n33.6\n\n0.4\n2.4\n11.6\n27.8\n21.3\n35.1\n22.4\n26.3\n\n-63.6\n-6.4\n-10.1\n3.4\n-31.1\n0.7\n-6.2\n20.0\n\n-66.7\n-31.7\n-17.1\n4.5\n-33.6\n-9.0\n-10.8\n7.7\n\n-66.7\n-32.0\n-16.9\n8.3\n-33.5\n-9.2\n-11.2\n16.2\n\n-67.6\n-33.7\n-24.1\n9.1\n-34.6\n-9.3\n-10.6\n13.4\n\nA.4 Evaluating state-of-the-art industry solutions\n\nA.4.1 Quality",
    "[S936] We send the CRAG public test set question as input to each of the SOTA RAG systems and collect\nthe responses for human grading. [S937] Note that the original Query Time and the provided retrieval results\nin CRAG are not used in this setting. [S938] We simply test the questions and ask human graders to grade\nthe responses based on when the query was made to the SOTA system. [S939] We called Copilot Pro,\nGemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API. [S940] Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such as\nAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabled\nby default in other systems. [S941] To ensure a fair comparison, we excluded these on-device components,\n\n19\n\n\fensuring that answer quality was not affected. [S942] We called each system on the following dates in\nPacific Time: 05/12/2024\u201e05/16/2024 (Copilot Pro), 05/20/2024\u201e05/28/2024 (Gemini Advanced),\n05/27/2024\u201e06/02/2024 (ChatGPT Plus), 05/15/2024\u201e05/16/2024 (Perplexity.ai), and 07/02/2024\n(Meta SG). [S943] We set the conversation style to \u201cPrecise\u201d when calling Copilot Pro and the temperature\nto 0 when calling Perplexity.ai. [S944] We select GPT-4o and llama-3-sonar-large-32k-online as the\nbase LLM when calling ChatGPT Plus and Perplexity.ai, respectively. [S945] A.4.2 Latency\n\nWe quantified the latency by calculating the time difference between the timestamp of the query\nsubmission to the system and the timestamp when the complete response was received. [S946] The latency of Perplexity.ai measured via API call is 2,455ms. [S947] Since latency measured by API call\nand web interface interactions are not directly comparable, we further called Perplexity.ai through\nits web interface and reported the latency under this setting in Table 6. [S948] Note that this latency may\nnot correspond to the accuracy numbers from the API calls. [S949] For Meta SG, we estimated a latency\ncomparable to other web interface interactions by excluding on-device components such as ASR and\nTTS from the overall end-to-end latency measurement. [S950] A.5 Limitations\n\nThe three tasks in our benchmark do not directly evaluate the construction of a first-stage retrieval\ncandidate pool, a demanding retrieval task in its own right. [S951] This design decision ensures that the\ncompetition remains both challenging and achievable within the KDD Cup\u2019s required three-month\ntimeframe. [S952] Despite the limitation, users of our dataset have the option to use the union of all 220K\nweb pages as a corpus to build a retriever. [S953] While this corpus does not match the entire web, it allows\nfor fair comparisons and manageable costs. [S954] 20"
  ]
}